import os
import pandas as pd
from typing import Union, Optional, List, Set, Dict, Tuple
import pathlib     # as pathlib
from datetime import datetime, timedelta

# --- åƒæ•¸è¨­å®š ---

def _load_and_filter_single_day(
    file_path: str, 
    top_n: int,
    volume_column: str, 
    code_column: str
) -> Optional[pd.DataFrame]:
    """
    è®€å–ã€æ¸…ç†å–®ä¸€ CSV æª”æ¡ˆï¼Œä¸¦ç¯©é¸å‡ºä¸‰å¤§æ³•äººè²·è¶…è‚¡æ•¸æœ€å¤§çš„ Top N è‚¡ç¥¨ã€‚

    Args:
        file_path (str): å–®æ—¥ä¸‰å¤§æ³•äººè²·è³£è¶…æ•¸æ“šæª”æ¡ˆè·¯å¾‘ã€‚
        top_n (int): è¦ç¯©é¸å‡ºçš„å‰ N åæ•¸é‡ã€‚
        volume_column (str): è²·è³£è¶…è‚¡æ•¸æ¬„ä½åç¨±ã€‚
        code_column (str): è­‰åˆ¸ä»£è™Ÿæ¬„ä½åç¨±ã€‚

    Returns:
        Optional[pd.DataFrame]: åŒ…å« 'ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸' çš„ Top N DataFrameï¼Œè‹¥å¤±æ•—å‰‡ç‚º Noneã€‚
    """
    try:
        # è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, encoding='big5')
            
        required_cols = [volume_column, code_column, 'è­‰åˆ¸åç¨±']
        if not all(col in df.columns for col in required_cols):
            return None

        # æ•¸æ“šæ¸…ç†èˆ‡è½‰æ›
        df[volume_column] = (
            df[volume_column].astype(str).str.replace(r'[",\s]', '', regex=True)
        )
        df[volume_column] = pd.to_numeric(df[volume_column], errors='coerce')
        df[code_column] = df[code_column].astype(str).str.strip()
        
        df.dropna(subset=[volume_column], inplace=True)
        
        # ç¯©é¸æ¢ä»¶ï¼š1. ä»£è™Ÿç‚º 4 ä½æ•¸å­— | 2. è²·è³£è¶…è‚¡æ•¸ > 0 (è²·è¶…)
        df_filtered = df[
            (df[code_column].str.match(r'^\d{4}$')) & 
            (df[volume_column] > 1000)  # å‡è¨­ 1000 è‚¡ (1 å¼µ) ç‚ºæœ€å°è²·è¶…é‡ï¼Œç”¨æ–¼åŸºç¤ç¯©é¸
        ].copy()

        # æ’åºä¸¦å–å‡º Top N
        df_sorted = df_filtered.sort_values(
            by=volume_column, 
            ascending=False
        )
        
        top_n_data = df_sorted.head(top_n).rename(
            columns={code_column: 'ä»£è™Ÿ', 'è­‰åˆ¸åç¨±': 'åç¨±', volume_column: 'è‚¡æ•¸'}
        )
        
        return top_n_data[['ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸']]
        
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æª”æ¡ˆ -> {os.path.basename(file_path)}")
        return None
    except Exception as e:
        print(f"âŒ æ•¸æ“šè™•ç†å¤±æ•— ({os.path.basename(file_path)})ï¼š{e}")
        return None


def analyze_top_stocks_trend(
    file_paths: List[str],
    top_n: int = 30,  #å¾€å‰20ç­†
    n_days_lookback: int = 5,  #å¾€å‰è¿½æœ”5å¤©
    volume_column: str = "ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸", 
    code_column: str = "è­‰åˆ¸ä»£è™Ÿ"
) -> Optional[str]:
    """
    åˆ†ææœ€æ–°ä¸€æ—¥ä¸‰å¤§æ³•äººè²·è¶… Top N è‚¡ç¥¨åœ¨éå» N å¤©çš„å›æº¯è¶¨å‹¢ã€‚

    Args:
        file_paths (List[str]): ä¾åºç‚º [æœ€æ–°æ—¥, å‰ä¸€æ—¥, ..., å‰ç¬¬ N æ—¥] çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ã€‚
                                (è‡³å°‘éœ€è¦ n_days_lookback + 1 å€‹æª”æ¡ˆè·¯å¾‘)
        top_n (int): åŸºæº–æ—¥è¦ç¯©é¸å‡ºçš„å‰ N åæ•¸é‡ (é è¨­ 30)ã€‚
        n_days_lookback (int): è¦å›æº¯çš„äº¤æ˜“æ—¥å¤©æ•¸ (é è¨­ 5)ã€‚
        volume_column (str): è²·è³£è¶…è‚¡æ•¸æ¬„ä½åç¨±ã€‚
        code_column (str): è­‰åˆ¸ä»£è™Ÿæ¬„ä½åç¨±ã€‚

    Returns:
        Optional[str]: æ ¼å¼åŒ–è¼¸å‡ºè¶¨å‹¢çµæœï¼Œæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
    """
    
    # ç¢ºä¿æœ‰è¶³å¤ çš„æª”æ¡ˆé€²è¡ŒåŸºæº–æ—¥ + å›æº¯æ—¥åˆ†æ
    required_files = n_days_lookback + 1
    if len(file_paths) < required_files:
        print(f"âš ï¸ éŒ¯èª¤ï¼šè‡³å°‘éœ€è¦ {required_files} å€‹æª”æ¡ˆ (åŸºæº–æ—¥ + {n_days_lookback} å€‹å›æº¯æ—¥)ã€‚ç›®å‰åªæœ‰ {len(file_paths)} å€‹ã€‚")
        return None

    # --- 1. è™•ç†æ‰€æœ‰äº¤æ˜“æ—¥æ•¸æ“š ---
    all_day_data: Dict[int, Set[str]] = {}
    day_labels: List[str] = []
    
    print(f"ğŸ” é–‹å§‹è™•ç† {required_files} å¤©æ•¸æ“š...")
    
    for i, path in enumerate(file_paths[:required_files]):
        df_day = _load_and_filter_single_day(path, top_n, volume_column, code_column)
        
        # æå–æª”æ¡ˆåç¨±ä¸­çš„æ—¥æœŸä½œç‚ºæ¨™ç±¤
        try:
             # å‡è¨­æª”æ¡ˆåç¨±åŒ…å«æ—¥æœŸï¼Œä¾‹å¦‚ "20251110_institutional.csv"
            date_label = os.path.basename(path).split('_')[0][:8]
        except:
            date_label = f"Day-{i}"
            
        if df_day is None or df_day.empty:
            print(f"âš ï¸ {date_label} æ•¸æ“šè¼‰å…¥æˆ–ç¯©é¸å¤±æ•—ï¼Œå°‡ç•¥éæ­¤æ—¥ã€‚")
            all_day_data[i] = set()
        else:
            # å°‡è©²æ—¥æœŸçš„ Top N è‚¡ç¥¨ä»£è™Ÿå­˜å„²ç‚ºé›†åˆ (Set)
            all_day_data[i] = set(df_day['ä»£è™Ÿ'].tolist())
            print(f"âœ… {date_label} æˆåŠŸç¯©é¸å‡º {len(all_day_data[i])} æª”è‚¡ç¥¨ã€‚")

        day_labels.append(date_label)
        
        # åŸºæº–æ—¥ (æœ€æ–°æ—¥) çš„æ•¸æ“šéœ€è¦å–®ç¨ä¿å­˜ï¼Œä»¥ä¾¿ç²å– Top N åç¨±å’Œè‚¡æ•¸
        if i == 0:
            df_base_day = df_day
    
    if df_base_day is None or df_base_day.empty:
        print("âŒ åŸºæº–æ—¥ (æœ€æ–°æ—¥) æ•¸æ“šç„¡æ•ˆæˆ–ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œè¶¨å‹¢åˆ†æã€‚")
        return None
        
    # --- 2. ç²å–åŸºæº–æ—¥ Top N è‚¡ç¥¨ä»£è™Ÿ (æ­¤ç‚ºåˆ†æçš„ä¸»é«”) ---
    base_stocks = df_base_day[['ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸']].head(top_n).copy()
    
    # --- 3. åŸ·è¡Œå›æº¯è¶¨å‹¢åˆ†æ ---
    
    # å»ºç«‹ä¸€å€‹æ–°æ¬„ä½ç”¨æ–¼å­˜æ”¾è¶¨å‹¢æ¨™è¨˜ (å¾æœ€æ–°æ—¥å¾€å‰ 5 å¤©)
    base_stocks['è¶¨å‹¢'] = "" 
    
    # æ ¼å¼åŒ–è¼¸å‡ºæ¨™é¡Œ
    trend_header = ""
    
    for i in range(1, n_days_lookback + 1):
        # å›æº¯æ—¥æœŸçš„æ¨™ç±¤
        day_tag = day_labels[i].replace("/", "")[-4:] # å–æ—¥æœŸå¾Œå››ç¢¼ (å¦‚ 1110)
        trend_header += f" | {day_tag} "
        
        # å–å¾—è©²å›æº¯æ—¥æœŸçš„ Top N è‚¡ç¥¨ä»£è™Ÿé›†åˆ
        past_top_n_codes = all_day_data.get(i, set())
        
        # å°åŸºæº–æ—¥çš„æ¯ä¸€æª”è‚¡ç¥¨é€²è¡Œæª¢æŸ¥
        presence_markers = []
        for code in base_stocks['ä»£è™Ÿ']:
            # æª¢æŸ¥ä»£è™Ÿæ˜¯å¦å‡ºç¾åœ¨éå» Top N åˆ—è¡¨ä¸­
            if code in past_top_n_codes:
                presence_markers.append("ğŸ”´") # å­˜åœ¨
            else:
                presence_markers.append("âšªï¸") # ä¸å­˜åœ¨
        
        # å°‡æ¨™è¨˜ä¸²æ¥èµ·ä¾†
        base_stocks['è¶¨å‹¢'] += pd.Series(presence_markers, index=base_stocks.index)
    
    # --- 4. æ ¼å¼åŒ–è¼¸å‡ºçµæœ ---
    
    # å°‡è‚¡æ•¸è½‰æ›ç‚ºå¼µæ•¸ä¸¦æ ¼å¼åŒ–
    volume_col_display_name = 'è²·è¶…å¼µæ•¸'
    base_stocks[volume_col_display_name] = base_stocks['è‚¡æ•¸'].apply(lambda x: f"{int(x / 1000):,}")
    
    # å®šç¾©æœ€çµ‚è¼¸å‡ºæ¬„ä½
    display_cols = ['ä»£è™Ÿ', 'åç¨±', volume_col_display_name, 'è¶¨å‹¢']
    
    # å»ºç«‹è¡¨æ ¼æ¨™é ­
    output_lines = [
        f"\nğŸ“ˆ ä¸‰å¤§æ³•äººè²·è¶…å‰{top_n}è‚¡(åŸºæº–æ—¥:{day_labels[0]})-éå»{n_days_lookback}æ—¥è¶¨å‹¢",
        "=" * 50,
        f"{'åæ¬¡'.center(3)}|{'ä»£è™Ÿ'.center(4)}|{'è­‰åˆ¸åç¨±'.center(8)}|{'è²·è¶…å¼µæ•¸'.center(8)}| å›æº¯è¶¨å‹¢ ({trend_header.strip()}) ",
        "-" * 50
    ]
    
    # å»ºç«‹è¡¨æ ¼å…§å®¹
    for rank, row in base_stocks.iterrows():
        rank_str = str(rank + 1).zfill(2)
        code_str = str(row['ä»£è™Ÿ']).center(6)
        name_str = row['åç¨±'].ljust(8, 'ã€€') # ä¸­æ–‡ä½”ç”¨å¯¬åº¦è™•ç†
        volume_str = row[volume_col_display_name].ljust(10)
        trend_str = row['è¶¨å‹¢'] # ğŸ”´âšªï¸ æ¨™è¨˜å·²åŒ…å«é–“éš”

        output_lines.append(f"{rank_str} |{code_str}| {name_str}|{volume_str} | {trend_str}")
        
    output_lines.append("=" * 50)
    output_lines.append(f"ğŸ”´: è©²æ—¥å‡ºç¾åœ¨ Top {top_n} åå–®ä¸­ | âšªï¸: è©²æ—¥æœªå‡ºç¾åœ¨ Top {top_n} åå–®ä¸­")
    
    return "\n".join(output_lines)

# æ ¹æ“šæŒ‡å®šæ—¥æœŸèˆ‡æ™‚é–“ï¼ˆ21:00æˆªæ­¢ï¼‰æä¾›å¾€å‰6å€‹äº¤æ˜“æ—¥ï¼Œå‰ä¸€å€‹äº¤æ˜“æ—¥å‰‡ç‚ºdf[-1]
def get_previous_n_trading_days(
    file_path: str,
    datetime_to_check: str,
    n_days: int = 6,        # å¾€å‰æå‡º6å€‹äº¤æ˜“æ—¥
    CUTOFF_HOUR: int = 21,  # è¨­å®šæˆªæ­¢æ™‚é–“ç‚º 21:00
    date_column_name: str = 'æ—¥æœŸ') -> Union[List[str], None]:
    """
    æ ¹æ“šæŒ‡å®šæ—¥æœŸèˆ‡æ™‚é–“ï¼ˆ21:00æˆªæ­¢ï¼‰ç¢ºå®šä¸€å€‹æœ‰æ•ˆæŸ¥è©¢æ—¥æœŸï¼Œ
    ä¸¦å¾è©²æ—¥æœŸï¼ˆå«ï¼‰é–‹å§‹å‘å‰è¿½æº¯ N å€‹æœ€è¿‘çš„äº¤æ˜“æ—¥ã€‚
    Args:
        file_path (str): åŒ…å«äº¤æ˜“æ—¥æ¸…å–®çš„ CSV æª”æ¡ˆå®Œæ•´è·¯å¾‘ (å‡è¨­æª”æ¡ˆä¸­åˆ—å‡ºçš„æ˜¯äº¤æ˜“æ—¥)ã€‚
        datetime_to_check (str): è¦æª¢æŸ¥çš„æ—¥æœŸå’Œæ™‚é–“å­—ä¸²ï¼Œä¾‹å¦‚ '2025/10/10 15:30:00'ã€‚
        n_days (int): è¦ç²å–çš„ä¸Šä¸€å€‹äº¤æ˜“æ—¥çš„æ•¸é‡ (é è¨­ç‚º 6)ã€‚
        date_column_name (str): æª”æ¡ˆä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æ—¥æœŸ'ã€‚
    Returns:
        Union[List[str], None]: åŒ…å« N å€‹äº¤æ˜“æ—¥å­—ä¸²ï¼ˆ'YYYY/MM/DD' æ ¼å¼ï¼‰çš„åˆ—è¡¨ï¼Œ
                                 æˆ–ç™¼ç”ŸéŒ¯èª¤æ™‚å›å‚³ Noneã€‚
    """
    
    # æª¢æŸ¥æª”æ¡ˆè·¯å¾‘
    if not os.path.exists(file_path):
        print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢º: {file_path}")
        return None
        
    try:
        # è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        if date_column_name not in df.columns:
            print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ—¥æœŸæ¬„ä½: '{date_column_name}'ã€‚æ¬„ä½æœ‰: {df.columns.tolist()}")
            return None
            
        # ç¢ºä¿æ—¥æœŸæ¬„ä½æ˜¯å­—ä¸²ï¼Œä»¥é¿å…æ ¼å¼ä¸ä¸€è‡´çš„å•é¡Œ
        df[date_column_name] = df[date_column_name].astype(str)

        # è¨­å®šæ—¥æœŸæ ¼å¼
        input_dt_format = '%Y/%m/%d %H:%M:%S'
        input_date_format = '%Y/%m/%d'
        
        # 1. è§£æè¼¸å…¥çš„æ—¥æœŸæ™‚é–“
        try:
            input_dt = datetime.strptime(datetime_to_check, input_dt_format)
        except ValueError:
            print(f"ã€éŒ¯èª¤ã€‘è¼¸å…¥æ—¥æœŸæ™‚é–“æ ¼å¼ä¸æ­£ç¢ºã€‚æ‡‰ç‚º '{input_dt_format}'ã€‚æ‚¨è¼¸å…¥çš„æ˜¯: {datetime_to_check}")
            return None
        
        input_date = input_dt.date()
        input_time = input_dt.time()
        
        # 2. æ ¹æ“šæ™‚é–“åˆ¤æ–·ã€Œæœ‰æ•ˆæŸ¥è©¢æ—¥æœŸã€
        # å¦‚æœæ™‚é–“åœ¨ 21:00 (å«) ä¹‹å¾Œï¼Œæœ‰æ•ˆæ—¥æœŸç‚ºä»Šå¤©ï¼›å¦å‰‡ç‚ºå‰ä¸€å¤©ã€‚
        cutoff_time = input_dt.replace(hour=CUTOFF_HOUR, minute=0, second=0, microsecond=0).time()
        
        effective_check_date = input_date
        
        if input_time < cutoff_time:
            # å¦‚æœåœ¨ 21:00 ä¹‹å‰ï¼Œè¦–ç‚ºå‰ä¸€å¤©çš„äº¤æ˜“
            effective_check_date = input_date - timedelta(days=1)
        
        # 3. è¿´åœˆå‘å‰å°‹æ‰¾æœ€è¿‘çš„ N å€‹äº¤æ˜“æ—¥
        current_check_date = effective_check_date
        trading_days_found: List[str] = []
        
        print(f"è¼¸å…¥æ—¥æœŸæ™‚é–“: {datetime_to_check}")
        print(f"èµ·å§‹æŸ¥è©¢æ—¥æœŸ (æ ¹æ“š {CUTOFF_HOUR}:00 æˆªæ­¢ç·šåˆ¤æ–·): {current_check_date.strftime(input_date_format)}")
        print(f"ç›®æ¨™ï¼šå‘å‰è¿½æº¯ {n_days} å€‹äº¤æ˜“æ—¥...")

        max_lookback_days = n_days * 3  # è¨­å®šæœ€å¤§è¿½æº¯å¤©æ•¸ï¼Œé¿å…ç„¡é™è¿´åœˆ
        days_passed = 0

        while len(trading_days_found) < n_days:
            
            # å®‰å…¨æ©Ÿåˆ¶æª¢æŸ¥
            if days_passed > max_lookback_days:
                print(f"ã€è­¦å‘Šã€‘å·²å‘å‰è¿½æº¯è¶…é {max_lookback_days} å¤© ({current_check_date.strftime(input_date_format)})ï¼Œå¯èƒ½è³‡æ–™æ¸…å–®ä¸å®Œæ•´ã€‚åœæ­¢å°‹æ‰¾ã€‚")
                break

            date_str = current_check_date.strftime(input_date_format)
            
            # ä½¿ç”¨ isin æª¢æŸ¥æ—¥æœŸæ˜¯å¦å­˜åœ¨æ–¼äº¤æ˜“æ—¥æ¸…å–®ä¸­
            is_trading_day = df[date_column_name].isin([date_str]).any()
            
            if is_trading_day:
                # æ‰¾åˆ°äº¤æ˜“æ—¥ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                trading_days_found.append(date_str)
                print(f"âœ… æ‰¾åˆ°ç¬¬ {len(trading_days_found)} å€‹äº¤æ˜“æ—¥: {date_str}")
            
            # ç„¡è«–æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼Œéƒ½å¾€å‰æ¨ä¸€å¤©ï¼Œç›´åˆ°æ‰¾åˆ°è¶³å¤ çš„æ•¸é‡
            current_check_date -= timedelta(days=1)
            days_passed += 1

        # å°‡åˆ—è¡¨åè½‰ï¼Œä½¿å…¶æŒ‰æ™‚é–“é †åºæ’åˆ— (å¦‚æœéœ€è¦çš„è©±ï¼Œé€šå¸¸æ˜¯å¾æœ€æ—©åˆ°æœ€è¿‘)
        # å¦‚æœå¸Œæœ›å¾æœ€è¿‘åˆ°æœ€èˆŠï¼Œå‰‡ä¸éœ€è¦åè½‰
        #trading_days_found.reverse() 

        # 4. åˆ¤æ–·ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ä¸¦å›å‚³çµæœ
        current_day_is_trading = df[date_column_name].isin([input_date.strftime(input_date_format)]).any()
        
        if current_day_is_trading:
             print(f"\nä»Šå¤©æ—¥æœŸ ({input_date.strftime(input_date_format)}) ç‚ºäº¤æ˜“æ—¥ã€‚")
        else:
             print(f"\nä»Šå¤©æ—¥æœŸ ({input_date.strftime(input_date_format)}) ç‚ºä¼‘å¸‚æ—¥ã€‚")
        
        
        # ç¢ºä¿åˆ—è¡¨æ˜¯å¾æœ€èˆŠåˆ°æœ€æ–°æ’åˆ—
        #trading_days_found.reverse()

        if len(trading_days_found) == n_days:
            # å®Œæ•´æ”¶é›†åˆ° N å¤©
            print(f"âœ… æˆåŠŸæ”¶é›†åˆ° {n_days} å€‹äº¤æ˜“æ—¥ã€‚")
            return trading_days_found
        else:
            # æœªæ”¶é›†åˆ° N å¤© (é€šå¸¸æ˜¯æ•¸æ“šä¸è¶³)
            print(f"âš ï¸ åƒ…æ‰¾åˆ° {len(trading_days_found)} å€‹äº¤æ˜“æ—¥ï¼Œæ•¸é‡ä¸è¶³ {n_days} å€‹ã€‚")
            return trading_days_found # å³ä½¿ä¸è¶³ä¹Ÿå›å‚³æ‰¾åˆ°çš„çµæœ

    except pd.errors.EmptyDataError:
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚")
        return None
    except Exception as e:
        print(f"ã€éŒ¯èª¤ã€‘è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

HOLIDAYS_FILE = r'D:\Python_repo\python\Jason_Stock_Analyzer\datas\processed\get_holidays\trading_day_2021-2025.csv'
Now_time_hour = datetime.now().strftime("%H")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„ã€Œå¹¾é»é˜ã€
Now_day_time = datetime.now().strftime("%Y/%m/%d %H:%M:%S")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„æ—¥æœŸåŠæ™‚é–“ã€Œä¾‹å¦‚ 2025-11-12 11:12ã€
    
print(f"--- æ¡ˆä¾‹ 4: ä»Šå¤©{datetime.now().strftime("%Y/%m/%d") }ï¼Œç¾åœ¨æ™‚é–“{datetime.now().strftime("%H:%M:%S")} ---")
result = get_previous_n_trading_days(
    file_path=HOLIDAYS_FILE, 
    datetime_to_check=datetime.now().strftime("%Y/%m/%d %H:%M:%S")
)

T86_folder_base = pathlib.Path(__file__).resolve().parent / "datas" / "raw" / "11_T86"
mock_file_paths = []
for day_roll1 in result[0:-1]:
    day_roll1 = day_roll1.replace('/', '')
    file_path =  T86_folder_base / f"{day_roll1}_T86_InstitutionalTrades.csv"
    mock_file_paths.append(file_path)
    
analysis_result = analyze_top_stocks_trend(
    file_paths=mock_file_paths,
    top_n = 30, #å®šç¾©è¦æŠ“å–çš„å‰20ç­†
    n_days_lookback= 4, #å®šç¾©å›æº¯å¤©æ•¸
)

# 3. è¼¸å‡ºçµæœ
if analysis_result:
    print(analysis_result)
