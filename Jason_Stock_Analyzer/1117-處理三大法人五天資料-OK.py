import os
import pandas as pd
from typing import Union, Optional, List, Set, Dict, Tuple
import pathlib
from datetime import datetime, timedelta
import sys

# --- åƒæ•¸è¨­å®š ---

def _load_and_filter_single_day(
    file_path: str, 
    top_n: int,
    volume_column: str, 
    code_column: str
) -> Optional[pd.DataFrame]:
    """
    è®€å–ã€æ¸…ç†å–®ä¸€ CSV æª”æ¡ˆï¼Œä¸¦ç¯©é¸å‡ºä¸‰å¤§æ³•äººè²·è¶…è‚¡æ•¸æœ€å¤§çš„ Top N è‚¡ç¥¨ã€‚

    Args:
        file_path (str): å–®æ—¥ä¸‰å¤§æ³•äººè²·è³£è¶…æ•¸æ“šæª”æ¡ˆè·¯å¾‘ã€‚
        top_n (int): è¦ç¯©é¸å‡ºçš„å‰ N åæ•¸é‡ã€‚
        volume_column (str): è²·è³£è¶…è‚¡æ•¸æ¬„ä½åç¨±ã€‚
        code_column (str): è­‰åˆ¸ä»£è™Ÿæ¬„ä½åç¨±ã€‚

    Returns:
        Optional[pd.DataFrame]: åŒ…å« 'ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸' çš„ Top N DataFrameï¼Œè‹¥å¤±æ•—å‰‡ç‚º Noneã€‚
    """
    try:
        # è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, encoding='big5')
            
        required_cols = [volume_column, code_column, 'è­‰åˆ¸åç¨±']
        if not all(col in df.columns for col in required_cols):
            return None

        # æ•¸æ“šæ¸…ç†èˆ‡è½‰æ›
        df[volume_column] = (
            df[volume_column].astype(str).str.replace(r'[",\s]', '', regex=True)
        )
        df[volume_column] = pd.to_numeric(df[volume_column], errors='coerce')
        df[code_column] = df[code_column].astype(str).str.strip()
        
        df.dropna(subset=[volume_column], inplace=True)
        
        # ç¯©é¸æ¢ä»¶ï¼š1. ä»£è™Ÿç‚º 4 ä½æ•¸å­— | 2. è²·è³£è¶…è‚¡æ•¸ > 1000 è‚¡ (è²·è¶…)
        df_filtered = df[
            (df[code_column].str.match(r'^\d{4}$')) & 
            (df[volume_column] > 1000) 
        ].copy()

        # æ’åºä¸¦å–å‡º Top N
        df_sorted = df_filtered.sort_values(
            by=volume_column, 
            ascending=False
        )
        
        top_n_data = df_sorted.head(top_n).rename(
            columns={code_column: 'ä»£è™Ÿ', 'è­‰åˆ¸åç¨±': 'åç¨±', volume_column: 'è‚¡æ•¸'}
        )
        
        return top_n_data[['ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸']]
        
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æª”æ¡ˆ -> {os.path.basename(file_path)}")
        return None
    except Exception as e:
        print(f"âŒ æ•¸æ“šè™•ç†å¤±æ•— ({os.path.basename(file_path)})ï¼š{e}")
        return None


def analyze_top_stocks_trend(
    file_paths: List[str],
    top_n: int = 30, 
    n_days_lookback: int = 5, 
    volume_column: str = "ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸", 
    code_column: str = "è­‰åˆ¸ä»£è™Ÿ"
) -> Optional[str]:
    """
    åˆ†ææœ€æ–°ä¸€æ—¥ä¸‰å¤§æ³•äººè²·è¶… Top N è‚¡ç¥¨åœ¨éå» N å¤©çš„å›æº¯è¶¨å‹¢ã€‚
    è¼¸å‡ºçµæœä¸åŒ…å«åæ¬¡ï¼Œä¸¦ä¾ ä»£è™Ÿ, è­‰åˆ¸åç¨±, å›æº¯è¶¨å‹¢, è²·è¶…å¼µæ•¸ æ’åºã€‚
    å›æº¯è¶¨å‹¢çš„æ—¥æœŸæ¨™ç±¤å’Œæ¨™è¨˜çš†ä»¥ã€Œæœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥ã€çš„é †åºæ’åˆ—ã€‚

    Args:
        file_paths (List[str]): ä¾åºç‚º [æœ€æ–°æ—¥, å‰ä¸€æ—¥, ..., å‰ç¬¬ N æ—¥] çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ã€‚
        top_n (int): åŸºæº–æ—¥è¦ç¯©é¸å‡ºçš„å‰ N åæ•¸é‡ (é è¨­ 30)ã€‚
        n_days_lookback (int): è¦å›æº¯çš„äº¤æ˜“æ—¥å¤©æ•¸ (é è¨­ 5)ã€‚
        volume_column (str): è²·è³£è¶…è‚¡æ•¸æ¬„ä½åç¨±ã€‚
        code_column (str): è­‰åˆ¸ä»£è™Ÿæ¬„ä½åç¨±ã€‚

    Returns:
        Optional[str]: æ ¼å¼åŒ–è¼¸å‡ºè¶¨å‹¢çµæœï¼Œæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
    """
    
    # ç¢ºä¿æœ‰è¶³å¤ çš„æª”æ¡ˆé€²è¡ŒåŸºæº–æ—¥ + å›æº¯æ—¥åˆ†æ
    required_files = n_days_lookback + 1
    if len(file_paths) < required_files:
        print(f"âš ï¸ éŒ¯èª¤ï¼šè‡³å°‘éœ€è¦ {required_files} å€‹æª”æ¡ˆ (åŸºæº–æ—¥ + {n_days_lookback} å€‹å›æº¯æ—¥)ã€‚ç›®å‰åªæœ‰ {len(file_paths)} å€‹ã€‚")
        return None

    # --- 1. è™•ç†æ‰€æœ‰äº¤æ˜“æ—¥æ•¸æ“š ---
    all_day_data: Dict[int, Set[str]] = {}
    day_labels: List[str] = []
    df_base_day: Optional[pd.DataFrame] = None 

    print(f"ğŸ” é–‹å§‹è™•ç† {required_files} å¤©æ•¸æ“š...")
    
    for i, path in enumerate(file_paths[:required_files]):
        df_day = _load_and_filter_single_day(path, top_n, volume_column, code_column)
        
        # æå–æª”æ¡ˆåç¨±ä¸­çš„æ—¥æœŸä½œç‚ºæ¨™ç±¤
        try:
            # å‡è¨­æª”æ¡ˆåç¨±åŒ…å«æ—¥æœŸï¼Œä¾‹å¦‚ "20251110_institutional.csv"
            date_label = os.path.basename(path).split('_')[0][:8]
        except:
            date_label = f"Day-{i}"
            
        if df_day is None or df_day.empty:
            print(f"âš ï¸ {date_label} æ•¸æ“šè¼‰å…¥æˆ–ç¯©é¸å¤±æ•—ï¼Œå°‡ç•¥éæ­¤æ—¥ã€‚")
            all_day_data[i] = set()
        else:
            # å°‡è©²æ—¥æœŸçš„ Top N è‚¡ç¥¨ä»£è™Ÿå­˜å„²ç‚ºé›†åˆ (Set)
            all_day_data[i] = set(df_day['ä»£è™Ÿ'].tolist())
            print(f"âœ… {date_label} æˆåŠŸç¯©é¸å‡º {len(all_day_data[i])} æª”è‚¡ç¥¨ã€‚")

        day_labels.append(date_label)
        
        # åŸºæº–æ—¥ (æœ€æ–°æ—¥) çš„æ•¸æ“šéœ€è¦å–®ç¨ä¿å­˜
        if i == 0:
            df_base_day = df_day
    
    if df_base_day is None or df_base_day.empty:
        print("âŒ åŸºæº–æ—¥ (æœ€æ–°æ—¥) æ•¸æ“šç„¡æ•ˆæˆ–ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œè¶¨å‹¢åˆ†æã€‚")
        return None
        
    # --- 2. ç²å–åŸºæº–æ—¥ Top N è‚¡ç¥¨ä»£è™Ÿ (æ­¤ç‚ºåˆ†æçš„ä¸»é«”) ---
    base_stocks = df_base_day[['ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸']].head(top_n).copy()
    
    # --- 3. åŸ·è¡Œå›æº¯è¶¨å‹¢åˆ†æ ---
    
    # å»ºç«‹åˆ—è¡¨ç”¨æ–¼å­˜æ”¾æ¯å¤©çš„å›æº¯æ¨™è¨˜ (Series) å’Œæ—¥æœŸæ¨™ç±¤
    # åˆå§‹é †åºç‚º i=1 åˆ° n_days_lookback (æœ€æ–°å›æº¯æ—¥åˆ°æœ€èˆŠå›æº¯æ—¥)
    trend_header_parts: List[str] = []
    trend_marker_series: List[pd.Series] = []
    
    for i in range(1, n_days_lookback + 1):
        # å›æº¯æ—¥æœŸçš„æ¨™ç±¤
        day_tag = day_labels[i].replace("/", "")[-4:] # å–æ—¥æœŸå¾Œå››ç¢¼ (å¦‚ 1110)
        trend_header_parts.append(day_tag)
        
        # å–å¾—è©²å›æº¯æ—¥æœŸçš„ Top N è‚¡ç¥¨ä»£è™Ÿé›†åˆ
        past_top_n_codes = all_day_data.get(i, set())
        
        # å°åŸºæº–æ—¥çš„æ¯ä¸€æª”è‚¡ç¥¨é€²è¡Œæª¢æŸ¥
        presence_markers = []
        for code in base_stocks['ä»£è™Ÿ']:
            # æª¢æŸ¥ä»£è™Ÿæ˜¯å¦å‡ºç¾åœ¨éå» Top N åˆ—è¡¨ä¸­
            if code in past_top_n_codes:
                presence_markers.append("ğŸ”´") # å­˜åœ¨ (å¤šåŠ ä¸€å€‹ç©ºæ ¼ä»¥ç¢ºä¿é–“éš”ä¸€è‡´)
            else:
                presence_markers.append("âšªï¸") # ä¸å­˜åœ¨
        
        # å°‡æ¨™è¨˜ Series å­˜å…¥åˆ—è¡¨
        trend_marker_series.append(pd.Series(presence_markers, index=base_stocks.index))
        
    # *** é—œéµä¿®æ”¹ï¼šå°‡åˆ—è¡¨åè½‰ï¼Œä½¿é †åºè®Šç‚ºã€Œæœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥ã€ ***
    trend_header_parts.reverse()
    trend_marker_series.reverse()
    
    # å»ºç«‹æœ€çµ‚çš„å›æº¯è¶¨å‹¢æ¨™é ­ (æœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥)
    trend_header = " ".join(trend_header_parts)
    
    # ä¸²æ¥æ‰€æœ‰çš„è¶¨å‹¢æ¨™è¨˜ Series (æœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥)
    if trend_marker_series:
        # ä½¿ç”¨ copy() ç¢ºä¿ç¨ç«‹æ€§
        base_stocks['è¶¨å‹¢'] = trend_marker_series[0].copy() 
        for series in trend_marker_series[1:]:
            base_stocks['è¶¨å‹¢'] += series
    else:
        base_stocks['è¶¨å‹¢'] = pd.Series("", index=base_stocks.index) # é¿å…ç©ºåˆ—è¡¨éŒ¯èª¤

    # ç§»é™¤è¶¨å‹¢æ¬„ä½å°¾éƒ¨å¤šé¤˜çš„ç©ºæ ¼
    base_stocks['è¶¨å‹¢'] = base_stocks['è¶¨å‹¢'].str.strip()
    
    # --- 4. æ ¼å¼åŒ–è¼¸å‡ºçµæœ ---
    
    # å°‡è‚¡æ•¸è½‰æ›ç‚ºå¼µæ•¸ä¸¦æ ¼å¼åŒ–
    volume_col_display_name = 'è²·è¶…å¼µæ•¸'
    base_stocks[volume_col_display_name] = base_stocks['è‚¡æ•¸'].apply(lambda x: f"{int(x / 1000):,}")
    
    # ç¸½å¯¬åº¦èª¿æ•´ (é…åˆæ–°é †åºèˆ‡æ¬„ä½)
    TOTAL_WIDTH = 25
    
    # å»ºç«‹è¡¨æ ¼æ¨™é ­ - ç§»é™¤åæ¬¡ï¼Œèª¿æ•´é †åº: ä»£è™Ÿ | è­‰åˆ¸åç¨± | å›æº¯è¶¨å‹¢ | è²·è¶…å¼µæ•¸
    output_lines = [
        f"\n    ğŸ“ˆ ä¸‰å¤§æ³•äººè²·è¶…Top{top_n}\nåŸºæº–æ—¥:{day_labels[0]}-éå»{n_days_lookback}æ—¥è¶¨å‹¢",
        "=" * TOTAL_WIDTH,
    #    f"{'ä»£è™Ÿ'.center(6)} | {'è­‰åˆ¸åç¨±'.center(6)} | å›æº¯è¶¨å‹¢ > > > > {day_labels[0][-4:]}  | {'è²·è¶…å¼µæ•¸'.center(8)}", 
    #    "-" * TOTAL_WIDTH
    ]
    
    # å»ºç«‹è¡¨æ ¼å…§å®¹
    for index, row in base_stocks.iterrows():
        #code_str = str(row['ä»£è™Ÿ']).center(6)
        name_str = row['åç¨±'].ljust(4, 'ã€€') # ä¸­æ–‡ä½”ç”¨å¯¬åº¦è™•ç†
        volume_str = row[volume_col_display_name].rjust(6) # å³å°é½Šæ•¸å­—
        trend_str = row['è¶¨å‹¢'] 

        show_width_in_stock_name = 4
        if len(name_str.replace(' ', '')) < show_width_in_stock_name :
            padding_width = show_width_in_stock_name - len(name_str.replace(' ', ''))
            name_str = name_str.replace(' ', '') + '  ' * padding_width
        else:
            name_str = name_str.replace(' ', '')

        # è¼¸å‡ºé †åº: ä»£è™Ÿ | è­‰åˆ¸åç¨± | å›æº¯è¶¨å‹¢ | è²·è¶…å¼µæ•¸
        #output_lines.append(f"{code_str} | {name_str} | {trend_str} | {volume_str}")
        output_lines.append(f"{name_str}{trend_str}({volume_str}å¼µ)")
        
        #print(f"âœ… {name_str.replace('  ', '')}")
        #sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ
    output_lines.append("=" * TOTAL_WIDTH)
    output_lines.append(f"ğŸ”´: è©²æ—¥å‡ºç¾åœ¨ Top {top_n} åå–®ä¸­ \nâšªï¸: è©²æ—¥æœªå‡ºç¾åœ¨ Top {top_n} åå–®ä¸­")
    
    return "\n".join(output_lines)

# æ ¹æ“šæŒ‡å®šæ—¥æœŸèˆ‡æ™‚é–“ï¼ˆ21:00æˆªæ­¢ï¼‰æä¾›å¾€å‰6å€‹äº¤æ˜“æ—¥ï¼Œå‰ä¸€å€‹äº¤æ˜“æ—¥å‰‡ç‚ºdf[-1]
def get_previous_n_trading_days(
    file_path: str,
    datetime_to_check: str,
    n_days: int = 6, 
    CUTOFF_HOUR: int = 21, 
    date_column_name: str = 'æ—¥æœŸ') -> Union[List[str], None]:
    """
    æ ¹æ“šæŒ‡å®šæ—¥æœŸèˆ‡æ™‚é–“ï¼ˆ21:00æˆªæ­¢ï¼‰ç¢ºå®šä¸€å€‹æœ‰æ•ˆæŸ¥è©¢æ—¥æœŸï¼Œ
    ä¸¦å¾è©²æ—¥æœŸï¼ˆå«ï¼‰é–‹å§‹å‘å‰è¿½æº¯ N å€‹æœ€è¿‘çš„äº¤æ˜“æ—¥ã€‚
    Args:
        file_path (str): åŒ…å«äº¤æ˜“æ—¥æ¸…å–®çš„ CSV æª”æ¡ˆå®Œæ•´è·¯å¾‘ (å‡è¨­æª”æ¡ˆä¸­åˆ—å‡ºçš„æ˜¯äº¤æ˜“æ—¥)ã€‚
        datetime_to_check (str): è¦æª¢æŸ¥çš„æ—¥æœŸå’Œæ™‚é–“å­—ä¸²ï¼Œä¾‹å¦‚ '2025/10/10 15:30:00'ã€‚
        n_days (int): è¦ç²å–çš„ä¸Šä¸€å€‹äº¤æ˜“æ—¥çš„æ•¸é‡ (é è¨­ç‚º 6)ã€‚
        date_column_name (str): æª”æ¡ˆä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æ—¥æœŸ'ã€‚
    Returns:
        Union[List[str], None]: åŒ…å« N å€‹äº¤æ˜“æ—¥å­—ä¸²ï¼ˆ'YYYY/MM/DD' æ ¼å¼ï¼‰çš„åˆ—è¡¨ï¼Œ
                                 æˆ–ç™¼ç”ŸéŒ¯èª¤æ™‚å›å‚³ Noneã€‚
    """
    
    # æª¢æŸ¥æª”æ¡ˆè·¯å¾‘
    if not os.path.exists(file_path):
        print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢º: {file_path}")
        return None
        
    try:
        # è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        if date_column_name not in df.columns:
            print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ—¥æœŸæ¬„ä½: '{date_column_name}'ã€‚æ¬„ä½æœ‰: {df.columns.tolist()}")
            return None
            
        # ç¢ºä¿æ—¥æœŸæ¬„ä½æ˜¯å­—ä¸²ï¼Œä»¥é¿å…æ ¼å¼ä¸ä¸€è‡´çš„å•é¡Œ
        df[date_column_name] = df[date_column_name].astype(str)

        # è¨­å®šæ—¥æœŸæ ¼å¼
        input_dt_format = '%Y/%m/%d %H:%M:%S'
        input_date_format = '%Y/%m/%d'
        
        # 1. è§£æè¼¸å…¥çš„æ—¥æœŸæ™‚é–“
        try:
            input_dt = datetime.strptime(datetime_to_check, input_dt_format)
        except ValueError:
            print(f"ã€éŒ¯èª¤ã€‘è¼¸å…¥æ—¥æœŸæ™‚é–“æ ¼å¼ä¸æ­£ç¢ºã€‚æ‡‰ç‚º '{input_dt_format}'ã€‚æ‚¨è¼¸å…¥çš„æ˜¯: {datetime_to_check}")
            return None
        
        input_date = input_dt.date()
        input_time = input_dt.time()
        
        # 2. æ ¹æ“šæ™‚é–“åˆ¤æ–·ã€Œæœ‰æ•ˆæŸ¥è©¢æ—¥æœŸã€
        # å¦‚æœæ™‚é–“åœ¨ 21:00 (å«) ä¹‹å¾Œï¼Œæœ‰æ•ˆæ—¥æœŸç‚ºä»Šå¤©ï¼›å¦å‰‡ç‚ºå‰ä¸€å¤©ã€‚
        cutoff_time = input_dt.replace(hour=CUTOFF_HOUR, minute=0, second=0, microsecond=0).time()
        
        effective_check_date = input_date
        
        if input_time < cutoff_time:
            # å¦‚æœåœ¨ 21:00 ä¹‹å‰ï¼Œè¦–ç‚ºå‰ä¸€å¤©çš„äº¤æ˜“
            effective_check_date = input_date - timedelta(days=1)
        
        # 3. è¿´åœˆå‘å‰å°‹æ‰¾æœ€è¿‘çš„ N å€‹äº¤æ˜“æ—¥
        current_check_date = effective_check_date
        trading_days_found: List[str] = []
        
        print(f"è¼¸å…¥æ—¥æœŸæ™‚é–“: {datetime_to_check}")
        print(f"èµ·å§‹æŸ¥è©¢æ—¥æœŸ (æ ¹æ“š {CUTOFF_HOUR}:00 æˆªæ­¢ç·šåˆ¤æ–·): {current_check_date.strftime(input_date_format)}")
        print(f"ç›®æ¨™ï¼šå‘å‰è¿½æº¯ {n_days} å€‹äº¤æ˜“æ—¥...")

        max_lookback_days = n_days * 3  # è¨­å®šæœ€å¤§è¿½æº¯å¤©æ•¸ï¼Œé¿å…ç„¡é™è¿´åœˆ
        days_passed = 0

        while len(trading_days_found) < n_days:
            
            # å®‰å…¨æ©Ÿåˆ¶æª¢æŸ¥
            if days_passed > max_lookback_days:
                print(f"ã€è­¦å‘Šã€‘å·²å‘å‰è¿½æº¯è¶…é {max_lookback_days} å¤© ({current_check_date.strftime(input_date_format)})ï¼Œå¯èƒ½è³‡æ–™æ¸…å–®ä¸å®Œæ•´ã€‚åœæ­¢å°‹æ‰¾ã€‚")
                break

            date_str = current_check_date.strftime(input_date_format)
            
            # ä½¿ç”¨ isin æª¢æŸ¥æ—¥æœŸæ˜¯å¦å­˜åœ¨æ–¼äº¤æ˜“æ—¥æ¸…å–®ä¸­
            is_trading_day = df[date_column_name].isin([date_str]).any()
            
            if is_trading_day:
                # æ‰¾åˆ°äº¤æ˜“æ—¥ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                trading_days_found.append(date_str)
                print(f"âœ… æ‰¾åˆ°ç¬¬ {len(trading_days_found)} å€‹äº¤æ˜“æ—¥: {date_str}")
            
            # ç„¡è«–æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼Œéƒ½å¾€å‰æ¨ä¸€å¤©ï¼Œç›´åˆ°æ‰¾åˆ°è¶³å¤ çš„æ•¸é‡
            current_check_date -= timedelta(days=1)
            days_passed += 1

        # 4. åˆ¤æ–·ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ä¸¦å›å‚³çµæœ
        current_day_is_trading = df[date_column_name].isin([input_date.strftime(input_date_format)]).any()
        
        if current_day_is_trading:
            print(f"\nä»Šå¤©æ—¥æœŸ ({input_date.strftime(input_date_format)}) ç‚ºäº¤æ˜“æ—¥ã€‚")
        else:
            print(f"\nä»Šå¤©æ—¥æœŸ ({input_date.strftime(input_date_format)}) ç‚ºä¼‘å¸‚æ—¥ã€‚")
        
        
        # åˆ—è¡¨é †åº: [æœ€æ–°æ—¥, å‰ä¸€æ—¥, ..., æœ€èˆŠæ—¥]
        if len(trading_days_found) == n_days:
            # å®Œæ•´æ”¶é›†åˆ° N å¤©
            print(f"âœ… æˆåŠŸæ”¶é›†åˆ° {n_days} å€‹äº¤æ˜“æ—¥ã€‚")
            return trading_days_found
        else:
            # æœªæ”¶é›†åˆ° N å¤© (é€šå¸¸æ˜¯æ•¸æ“šä¸è¶³)
            print(f"âš ï¸ åƒ…æ‰¾åˆ° {len(trading_days_found)} å€‹äº¤æ˜“æ—¥ï¼Œæ•¸é‡ä¸è¶³ {n_days} å€‹ã€‚")
            return trading_days_found # å³ä½¿ä¸è¶³ä¹Ÿå›å‚³æ‰¾åˆ°çš„çµæœ

    except pd.errors.EmptyDataError:
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚")
        return None
    except Exception as e:
        print(f"ã€éŒ¯èª¤ã€‘è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None



# === åŸ·è¡Œç¨‹å¼ ===

# åƒæ•¸å®šç¾©
HOLIDAYS_FILE = r'D:\Python_repo\python\Jason_Stock_Analyzer\datas\processed\get_holidays\trading_day_2021-2025.csv'
Now_day_time = datetime.now().strftime("%Y/%m/%d %H:%M:%S") 
    
result = get_previous_n_trading_days(
    file_path=HOLIDAYS_FILE, 
    datetime_to_check=datetime.now().strftime("%Y/%m/%d %H:%M:%S")
)

# æ¨¡æ“¬æª”æ¡ˆè·¯å¾‘
T86_folder_base = pathlib.Path(__file__).resolve().parent / "datas" / "raw" / "11_T86"
mock_file_paths = []

print(result)
sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ

if result:
    for day_str in result:
        day_str = day_str.replace('/', '')
        file_path = T86_folder_base / f"{day_str}_T86_InstitutionalTrades.csv"
        # å°‡ Path è½‰å› str å‚³å…¥
        mock_file_paths.append(str(file_path)) 
    
    if len(mock_file_paths) > 0:
        actual_lookback_days = len(mock_file_paths) - 1
        analysis_result = analyze_top_stocks_trend(
            file_paths=mock_file_paths,
            top_n = 30, # å®šç¾©è¦æŠ“å–çš„å‰30ç­†
            n_days_lookback=actual_lookback_days, # ä¾æ“šå¯¦éš›æ‰¾åˆ°çš„å¤©æ•¸è¨­å®šå›æº¯å¤©æ•¸
        )

        # 3. è¼¸å‡ºçµæœ
        if analysis_result:
            print(analysis_result)
    else:
        print("âŒ ç”±æ–¼æœªèƒ½å–å¾—è¶³å¤ çš„äº¤æ˜“æ—¥è·¯å¾‘ï¼Œç„¡æ³•é€²è¡Œåˆ†æã€‚")