import pandas as pd
from dotenv import load_dotenv # âŠ åŒ¯å…¥Lineæ©Ÿå™¨äººå‡½å¼åº«
import os
from datetime import date
from pathlib import Path
from typing import Union
import schedule
from datetime import datetime
import time
import keyboard  # æ–°å¢: ç”¨æ–¼åµæ¸¬éµç›¤è¼¸å…¥
import requests
from io import StringIO
import urllib3
import re
from typing import Optional, Tuple, List
import utils.jason_utils as jutils
from google import genai
from google.genai.errors import APIError

# æª”æ¡ˆè·¯å¾‘

BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__)))
OUTPUT_DIR = BASE_DIR / "datas" / "raw" / "4_MI_INDEX20"

#TARGET_DATE = date.today().strftime("%Y%m%d") 
TARGET_DATE = "20251007"  # æ¸¬è©¦ç”¨ç‰¹å®šæ—¥æœŸ

file_path = OUTPUT_DIR / f"{TARGET_DATE}_MI_INDEX20_Market.csv"

# --------  Lineæ©Ÿå™¨äººè¨Šæ¯ -------
# â‹ è¼‰å…¥ line_API.env æª”æ¡ˆä¸­çš„è®Šæ•¸
# æ³¨æ„ï¼šå¦‚æœæ‚¨ä½¿ç”¨ .env ä»¥å¤–çš„æª”å (å¦‚ line_token.env)ï¼Œéœ€è¦æŒ‡å®šæª”å
line_token_file = BASE_DIR / "line_API.env"
load_dotenv(line_token_file) 
#load_dotenv(r"D:\Python_repo\python\Jason_Stock_Analyzer\line_API.env") 


# âŒ å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– Token å’Œ User ID
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_USER_ID = os.getenv("LINE_USER_ID")


# ä¿®æ­£ LineBotApiError çš„åŒ¯å…¥è·¯å¾‘ï¼ˆæ ¹æ“šæ‚¨ä¸Šä¸€å€‹å•é¡Œçš„è§£ç­”ï¼‰
from linebot.v3.messaging import Configuration, ApiClient, MessagingApi
from linebot.v3.messaging import TextMessage, PushMessageRequest

# google-gemini AIè©¢å•
def get_gemini_response(prompt_text):
    # æ­¤è™•ä¸å†éœ€è¦ os.getenv()ï¼Œå› ç‚º load_dotenv() å·²ç¶“å°‡é‡‘é‘°è¼‰å…¥åˆ°ç³»çµ±ç’°å¢ƒè®Šæ•¸ä¸­
    try:
        # âŒ Client() æœƒè‡ªå‹•åµæ¸¬ä¸¦ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„ GEMINI_API_KEY
        client = genai.Client() 
        
        # ... å¾ŒçºŒç¨‹å¼ç¢¼èˆ‡ä¹‹å‰ç¯„ä¾‹ç›¸åŒ ...
        model_name = 'gemini-2.5-flash'
        print(f"\n--- æ­£åœ¨ä½¿ç”¨ {model_name} è©¢å• Gemini ---")
        
        response = client.models.generate_content(
            model=model_name,
            contents=prompt_text
        )
        return response.text

    except APIError as e:
        return f"Gemini API éŒ¯èª¤: {e}"
    except Exception as e:
        return f"ç™¼ç”ŸéŒ¯èª¤: {e}"

# Lineæ©Ÿå™¨äºº
def send_stock_notification(user_id, message_text):
    try:
        push_message_request = PushMessageRequest(
            to=user_id,
            messages=[TextMessage(text=message_text)]
        )
        # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨å…¨åŸŸè®Šæ•¸ messaging_apiï¼Œå¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œé€™è£¡æœƒå ±éŒ¯
        messaging_api.push_message(push_message_request) 
        print(f"è¨Šæ¯å·²æˆåŠŸç™¼é€çµ¦ {user_id}")
    except Exception as e:
        print(f"å…¶ä»–éŒ¯èª¤: {e}")


# æª¢æŸ¥æ˜¯å¦ç‚ºä¼‘å¸‚æ—¥
def check_date_in_csv(file_path: str, date_to_check: str, date_column_name: str = 'æ—¥æœŸ') -> Union[bool, pd.Series]:
    """
    æª¢æŸ¥æŒ‡å®šæ—¥æœŸå­—ä¸²æ˜¯å¦å‡ºç¾åœ¨ CSV æª”æ¡ˆçš„ç‰¹å®šæ¬„ä½ä¸­ã€‚
    
    Args:
        file_path (str): holidays_all.csv æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        date_to_check (str): è¦æª¢æŸ¥çš„æ—¥æœŸå­—ä¸²ï¼Œä¾‹å¦‚ '2025/10/10'ã€‚
        date_column_name (str): æª”æ¡ˆä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æ—¥æœŸ'ã€‚
        
    Returns:
        Union[bool, pd.Series]: å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›åŒ…å«åŒ¹é…è¡Œçš„ Series (å¸ƒæ—å€¼)ï¼Œ
                                 å¦‚æœæœªæ‰¾åˆ°æˆ–æª”æ¡ˆä¸å­˜åœ¨ï¼Œè¿”å› Falseã€‚
    """
    print(f"ğŸ” æ­£åœ¨æª¢æŸ¥æª”æ¡ˆ: {os.path.basename(file_path)}")
    print(f"ç›®æ¨™æ—¥æœŸ: {date_to_check}")

    if not os.path.exists(file_path):
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
        return False
        
    try:
        # ç”±æ–¼ holidays_all.csv æ˜¯ç”±æ‚¨ç¨‹å¼ç¢¼æœ€å¾Œå„²å­˜çš„ï¼Œ
        # ä¸”æ‚¨å„²å­˜æ™‚ä½¿ç”¨ encoding='utf-8-sig'ï¼Œé€™è£¡ä¹Ÿä½¿ç”¨ç›¸åŒçš„ç·¨ç¢¼è®€å–
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        if date_column_name not in df.columns:
            print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ—¥æœŸæ¬„ä½: '{date_column_name}'ã€‚")
            print(f"æª”æ¡ˆä¸­çš„æ¬„ä½æœ‰: {df.columns.tolist()}")
            return False

        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œ (isin) æª¢æŸ¥æ¬„ä½ä¸­æ˜¯å¦åŒ…å«ç›®æ¨™æ—¥æœŸ
        # å³ä½¿æ¬„ä½é¡å‹æ˜¯ object (å­—ä¸²)ï¼Œä¹Ÿèƒ½æ­£ç¢ºæª¢æŸ¥
        is_present = df[date_column_name].isin([date_to_check])
        
        if is_present.any():
            # æ‰¾åˆ°åŒ¹é…çš„è¡Œ
            matched_rows = df[is_present]
            print(f"âœ… æ—¥æœŸ '{date_to_check}' å·²åœ¨æª”æ¡ˆä¸­æ‰¾åˆ°ï¼")
            print("--- åŒ¹é…çš„è³‡æ–™åˆ— ---")
            print(matched_rows)
            return True
        else:
            print(f"âŒ æ—¥æœŸ '{date_to_check}' æœªåœ¨æª”æ¡ˆçš„ '{date_column_name}' æ¬„ä½ä¸­æ‰¾åˆ°ã€‚")
            return False

    except pd.errors.EmptyDataError:
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚")
        return False
    except Exception as e:
        print(f"ã€éŒ¯èª¤ã€‘è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

# æŠ“å–è³‡æ–™ --------------
def _read_twse_csv(response_text: str, header_row: int) -> Optional[pd.DataFrame]:
    """
    Args:
        response_text: HTTP è«‹æ±‚å›å‚³çš„æ–‡å­—å…§å®¹ (Big5 ç·¨ç¢¼)ã€‚
        header_row: CSV æª”æ¡ˆä¸­è³‡æ–™è¡¨é ­æ‰€åœ¨çš„è¡Œæ•¸ (0-indexed)ã€‚
    Returns:
        Optional[pd.DataFrame]: è™•ç†å¾Œçš„ DataFrameã€‚
    """
    try:
        csv_data = StringIO(response_text)
        # å˜—è©¦è®€å– CSV
        df = pd.read_csv(
            csv_data, 
            header=header_row,          # è³‡æ–™è¡¨é ­æ‰€åœ¨çš„è¡Œæ•¸
            skipinitialspace=True,      # è·³éåˆ†éš”ç¬¦å¾Œçš„ç©ºæ ¼
            on_bad_lines='skip',        # è·³éæ ¼å¼ä¸æ­£ç¢ºçš„è¡Œ
            encoding='Big5'             # ä½¿ç”¨ Big5 ç·¨ç¢¼è®€å–
        )
        # TWSE çš„ CSV æ¬„ä½åç¨±å¸¸æœ‰éš±è—ç©ºæ ¼ï¼Œå°è‡´ df.columns ç„¡æ³•æ­£ç¢ºåŒ¹é…ã€‚
        if not df.empty:
            df.columns = df.columns.str.strip()
        # ç§»é™¤æ‰€æœ‰æ¬„ä½çš†ç‚ºç©ºçš„è¡Œ
        df = df.dropna(how='all')
        # ç§»é™¤è³‡æ–™å°¾éƒ¨å¯èƒ½å‡ºç¾çš„å½™ç¸½æˆ–å‚™è¨»è¡Œ
        if not df.empty and df.iloc[-1].astype(str).str.contains('åˆè¨ˆ|ç¸½è¨ˆ|å‚™è¨»', na=False).any():
            df = df.iloc[:-1]
        return df
    except Exception as e:
        print(f"åœ¨è®€å–æˆ–æ¸…ç† CSV æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return None

# å…±ç”¨çš„è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼ç™¼é€ HTTP è«‹æ±‚ä¸¦æª¢æŸ¥ç‹€æ…‹ã€‚
def _fetch_twse_data(url: str) -> Optional[str]:
    """
    Args:
        url: å®Œæ•´çš„ TWSE è³‡æ–™ URLã€‚
    Returns:
        Optional[str]: æˆåŠŸç²å–å¾Œï¼Œä»¥ Big5 è§£ç¢¼çš„æ–‡å­—å…§å®¹ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status() 
        response.encoding = 'Big5'
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f"âŒ HTTP éŒ¯èª¤ï¼š{errh} (è©²æ—¥å¯èƒ½ç„¡äº¤æ˜“è³‡æ–™)")
    except requests.exceptions.RequestException as err:
        print(f"âŒ é€£ç·šæˆ– Requests éŒ¯èª¤: {err}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")

    return None
# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”ç¢ºå¯¦æ˜¯ä¸€å€‹æª”æ¡ˆ (éè³‡æ–™å¤¾)
def check_folder_and_create(folder_path: str):
    """
    åƒæ•¸:
        file_path (str): è¦æª¢æŸ¥çš„æª”æ¡ˆè·¯å¾‘ã€‚
    å›å‚³:
        bool: æª”æ¡ˆå­˜åœ¨æ™‚å›å‚³ Trueï¼›å¦å‰‡å›å‚³ Falseã€‚
    """
    OUTPUT_DIR, filename_new = jutils.get_path_to_folder_file(folder_path)
    jutils.check_and_create_folder(OUTPUT_DIR)
    jutils.check_file_exists(filename_new)
    return True

def fetch_twse_mi_index20(target_date: str) -> Optional[pd.DataFrame]:
    """
    (4/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ MI_INDEX20 å ±å‘Š (æ”¶ç›¤æŒ‡æ•¸åŠæˆäº¤é‡å€¼è³‡è¨Š)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX20
    
    **ä¿®æ­£:** è¡¨é ­ç´¢å¼•æ”¹ç‚º 2 (åŸç‚º 1)ã€‚
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX20"
    url = f"{base_url}?date={target_date}&response=csv"
        
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "4_MI_INDEX20")
    filename = OUTPUT_DIR + f"\\{target_date}_MI_INDEX20_Market.csv"
    
    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (4/10) MI_INDEX20 è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None
    
    # MI_INDEX20 å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 2
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (4/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def main_run():

    # å…ˆæŠ“å–è³‡æ–™
    fetch_twse_mi_index20(TARGET_DATE)

    # ----------------- æª¢æŸ¥ Token æ˜¯å¦å­˜åœ¨ -----------------

    # æ‚¨æŒ‡å®šçš„æª”æ¡ˆè·¯å¾‘

    FILE_PATH = BASE_DIR / "datas" / "processed" / "get_holidays" / "holidays_all.csv"
    DATE_TO_CHECK = date.today().strftime("%Y/%m/%d") 
    #DATE_TO_CHECK = '2025/10/07' 
    DATE_COLUMN = 'æ—¥æœŸ' # æ ¹æ“šæ‚¨å‰é¢çš„ç¨‹å¼ç¢¼ï¼Œåˆä½µå¾Œçš„æ¬„ä½åç¨±æ‡‰ç‚º 'æ—¥æœŸ'

    # åŸ·è¡Œæª¢æŸ¥
    result_found = check_date_in_csv(FILE_PATH, DATE_TO_CHECK, DATE_COLUMN)

    # if not LINE_CHANNEL_ACCESS_TOKEN:
    #     print("éŒ¯èª¤ï¼šLINE_CHANNEL_ACCESS_TOKEN æœªåœ¨ line_API.env ä¸­è¨­ç½®æˆ–è®€å–å¤±æ•—ã€‚ç¨‹å¼ä¸­æ­¢ã€‚")
    #     exit()

    # try:
    #     # åˆå§‹åŒ– Configuration å’Œ MessagingApi
    #     configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
    #     api_client = ApiClient(configuration)
    #     messaging_api = MessagingApi(api_client)
    #     print("Line Bot API åˆå§‹åŒ–æˆåŠŸã€‚")
    # except Exception as e:
    #     print(f"Line Bot API åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Tokenï¼š{e}")
    #     return

    # ... æ¥ä¸‹ä¾†çš„ç¨‹å¼ç¢¼ä¿æŒä¸è®Š ...
    # é€™æ˜¯æ¥æ”¶è¨Šæ¯çš„ç”¨æˆ¶ ID æˆ–ç¾¤çµ„ ID
    # LINE_USER_ID ç¾åœ¨å·²ç¶“å¾ .env æª”æ¡ˆä¸­è®€å–

    # Lineæ©Ÿå™¨äººç¯„ä¾‹åŸ·è¡Œ
    #analysis_report = "å°ç©é›» (2330) è¿‘æœŸèµ°å‹¢å¼·å‹ï¼ŒRSI ä½æ–¼ 65ï¼Œé æœŸçŸ­æœŸå…§ä»æœ‰ä¸Šæ¼²å‹•èƒ½ã€‚"
    #send_stock_notification(LINE_USER_ID, analysis_report)
    #--------------------------------

    try:
        # è®€å– CSV æª”æ¡ˆ
        # å‡è¨­æª”æ¡ˆæ˜¯ Big5 ç·¨ç¢¼ (å°ç£å¸¸è¦‹)ï¼Œè‹¥é‡åˆ°ç·¨ç¢¼éŒ¯èª¤å¯å˜—è©¦ 'utf-8' æˆ– 'cp950'
        df = pd.read_csv(file_path, encoding='utf-8')

        # ç¢ºä¿ã€è­‰åˆ¸ä»£è™Ÿã€‘æ˜¯ä»¥å­—ä¸²å½¢å¼è™•ç†ï¼Œä»¥ä¾¿æ–¼é•·åº¦ç¯©é¸
        df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip()
        
        # --- ç¯©é¸æ¢ä»¶ ---
        
        # 1. å–å¾—ã€è­‰åˆ¸ä»£è™Ÿã€‘ç‚º4ç¢¼æ•¸å­—çš„å» å•†
        # ä½¿ç”¨ .str.match() é…åˆæ­£å‰‡è¡¨é”å¼ä¾†æª¢æŸ¥æ˜¯å¦ç‚ºå››ä½æ•¸å­—
        df_filtered = df[df['è­‰åˆ¸ä»£è™Ÿ'].str.match(r'^\d{4}$', na=False)]
        
        # 2. å–å¾—ã€æ¼²è·Œ(+/-)ã€‘æ¬„ä½ç‚ºã€+ã€‘
        # ä½¿ç”¨ .str.strip() ä¾†ç§»é™¤å¯èƒ½çš„ç©ºç™½å­—å…ƒï¼Œç¢ºä¿ç²¾ç¢ºåŒ¹é…
        df_filtered = df_filtered[df_filtered['æ¼²è·Œ(+/-)'].astype(str).str.strip() == '+']
        
        # é¸æ“‡éœ€è¦çš„æ¬„ä½ï¼šã€è­‰åˆ¸ä»£è™Ÿã€‘ã€ã€è­‰åˆ¸åç¨±ã€‘
        result = df_filtered[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±']]
        
        for i in result:
            print(i)

        # é¡¯ç¤ºçµæœ
        # if not result.empty:
        #     print("ç¬¦åˆæ¢ä»¶çš„å» å•†è³‡æ–™å¦‚ä¸‹ï¼š")
        #     print("--------------------")
        #     print(result.to_string(index=False))
        #     print(f"\nç¸½å…±æ‰¾åˆ° {len(result)} ç­†è³‡æ–™ã€‚")

        #     analysis_report = f"""{DATE_TO_CHECK}äº¤æ˜“å‰20å®¶å» å•†è³‡æ–™å¦‚ä¸‹ï¼š
        #         \n{result.to_string(index=False)}
        #         \nç¸½å…± {len(result)} å®¶ã€‚
        # """
        # else:
        #     print("æœªæ‰¾åˆ°ç¬¦åˆæ‰€æœ‰ç¯©é¸æ¢ä»¶çš„è³‡æ–™ã€‚")

    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆè·¯å¾‘ {file_path}")
    except UnicodeDecodeError as e:
        print(f"éŒ¯èª¤ï¼šæª”æ¡ˆç·¨ç¢¼å•é¡Œã€‚è«‹å˜—è©¦ä¿®æ”¹ read_csv ä¸­çš„ encoding åƒæ•¸ (ä¾‹å¦‚ 'utf-8' æˆ– 'cp950')ã€‚è©³ç´°éŒ¯èª¤ï¼š{e}")
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šCSV æª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ã€‚è«‹æª¢æŸ¥æ¬„ä½åç¨±æ˜¯å¦æ­£ç¢ºã€‚è©³ç´°éŒ¯èª¤ï¼š{e}")
    except Exception as e:
        print(f"ç™¼ç”Ÿäº†ä¸€å€‹æœªé æœŸçš„éŒ¯èª¤ï¼š{e}")

    # AIè©¢å•
    if not os.getenv("GEMINI_API_KEY"):
         print("éŒ¯èª¤ï¼šæœªèƒ½å¾ gemini_API.env è¼‰å…¥ GEMINI_API_KEYã€‚è«‹æª¢æŸ¥æª”æ¡ˆå…§å®¹å’Œåç¨±ã€‚")
         exit()
    #user_input = f"ä»Šå¤©æ˜¯{DATE_TO_CHECK}ï¼Œè«‹å¹«æˆ‘è’é›†è‚¡ç¥¨åŠ›ç©é›»(6770)çš„ç›¸é—œæ–°èï¼ä¸¦ç°¡çŸ­ä½¿ç”¨300å­—åšèªªæ˜"
    user_input = f"ä»Šå¤©æ˜¯{DATE_TO_CHECK}ï¼Œè«‹å¹«æˆ‘æä¾›ç¾åœ‹è‚¡å¸‚æ¼²è·Œ"
    # user_input = input("è«‹è¼¸å…¥æ‚¨çš„è©¢å•æ–‡å­—: \n")
    if user_input.strip():
        gemini_reply = get_gemini_response(user_input)
        print("\n================== Gemini å›è¦† ==================")
        print(gemini_reply)
        print("=================================================")
    else:
        print("è¼¸å…¥ä¸èƒ½ç‚ºç©ºã€‚")
    # -----------------------

    if result_found:
        analysis_report = f"ä»Šå¤©æ˜¯2025/10/26ï¼Œè«‹å¹«æˆ‘æä¾›ç¾åœ‹è‚¡å¸‚æ¼²è·Œ"
        #send_stock_notification(LINE_USER_ID, analysis_report)
    else:
        send_stock_notification(LINE_USER_ID, analysis_report)

#---------------

if not LINE_CHANNEL_ACCESS_TOKEN:
        print("éŒ¯èª¤ï¼šLINE_CHANNEL_ACCESS_TOKEN æœªåœ¨ line_API.env ä¸­è¨­ç½®æˆ–è®€å–å¤±æ•—ã€‚ç¨‹å¼ä¸­æ­¢ã€‚")
        exit()

try:
    # åˆå§‹åŒ– Configuration å’Œ MessagingApi
    configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
    api_client = ApiClient(configuration)
    messaging_api = MessagingApi(api_client)
    print("Line Bot API åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    print(f"Line Bot API åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Tokenï¼š{e}")



# å…ˆé‹è¡Œ schedule.clear() å°‡æ’ç¨‹æ¸…é™¤ï¼Œé¿å…ç¿’æ…£ä½¿ç”¨ jupyter notebook æ•´åˆé–‹ç™¼ç’°å¢ƒçš„è®€è€…ï¼Œ
# æœ‰æ®˜å­˜çš„æ’ç¨‹ï¼Œé€ æˆé‹è¡Œçµæœä¸å¦‚é æœŸ
#schedule.clear()

# æŒ‡å®šæ¯ 15 ç§’é‹è¡Œä¸€æ¬¡ say_hi å‡½æ•¸
schedule.every(5).seconds.do(main_run)

# æ¯å¤© 15:00 é‹è¡Œä¸€æ¬¡ get_price å‡½æ•¸
#schedule.every().day.at('17:00').do(main_run)

# å°‡ schedule.run_pending() æ”¾åœ¨ while ç„¡çª®è¿´åœˆå…§
while True:
    schedule.run_pending()
    time.sleep(1) # å¢åŠ  time.sleep(1) æ¸›å°‘ CPU è² è¼‰

