#æ¨™æº–å‡½å¼åº«
import os
import sys
import re
import shutil
from io import StringIO
import pathlib     # as pathlib
from datetime import date, datetime, timedelta, time as time_TimeClass

#ç¬¬ä¸‰æ–¹å‡½å¼åº«
import numpy as np # ç”¨æ–¼æ•¸å€¼æ“ä½œ
import pandas as pd # ç”¨æ–¼è³‡æ–™è™•ç†èˆ‡åˆ†æ
import requests
import schedule
import keyboard  # ç”¨æ–¼ç›£è½éµç›¤äº‹ä»¶
from dotenv import load_dotenv # âŠ åŒ¯å…¥å‡½å¼åº«
from typing import Optional, Tuple, List, Union
import time as time_module # ç”¨æ–¼ sleep() æˆ– time()

#æœ¬åœ°æ¨¡çµ„
import get_stocks_company_all 
from utils import jason_utils as jutils

# æŠ‘åˆ¶ç•¶ verify=False æ™‚å½ˆå‡ºçš„ InsecureRequestWarning è­¦å‘Š
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# ==========================================================
# åƒæ•¸è¨­å®š  --- é…ç½® (Configuration) ---
# ==========================================================
SUMMARY_LOG_FILENAME_PREFIX = "fetch_summary" # å®šç¾©æ‘˜è¦æ—¥èªŒæª”æ¡ˆå‰ç¶´

# è¨­å®šéµç›¤ç›£æ§ -- 1. åˆå§‹åŒ–é‹è¡Œç‹€æ…‹ (ç¢ºä¿æ˜¯å…¨å±€è®Šæ•¸)
running = True
# """æŒ‰éµ [Q] åœæ­¢ç¨‹å¼"""
def stop_program():
    print("\n\nğŸ‘‹ åµæ¸¬åˆ° 'Q' éµï¼Œç¨‹å¼å³å°‡å®‰å…¨é€€å‡º...")
    global running
    running = False
    
# è®€å–é—œæ³¨çš„è‚¡ç¥¨
def get_stock_names_from_excel(file_path: str, sheet_name: str, column_name: str) -> Optional[pd.Series]:
    """
    è®€å– Excel æª”æ¡ˆä¸­æŒ‡å®šå·¥ä½œè¡¨çš„æŒ‡å®šæ¬„ä½æ•¸æ“šã€‚

    Args:
        file_path (str): Excel æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        sheet_name (str): å·¥ä½œè¡¨çš„æ¨™ç±¤åç¨± (e.g., 'ã€é—œæ³¨çš„è‚¡ç¥¨ã€‘')ã€‚
        column_name (str): è¦æŠ“å–çš„æ¬„ä½åç¨± (e.g., 'è­‰åˆ¸åç¨±')ã€‚

    Returns:
        pd.Series or None: åŒ…å«è­‰åˆ¸åç¨±çš„ Seriesï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨å˜—è©¦è®€å– Excel æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ é–å®šå·¥ä½œè¡¨ï¼šã€{sheet_name}ã€‘")

    try:
        # è®€å– Excel æª”æ¡ˆä¸­æŒ‡å®šçš„å·¥ä½œè¡¨
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        
        # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
        if column_name in df.columns:
            # æŠ“å–ä¸¦è¿”å› 'è­‰åˆ¸åç¨±' æ¬„ä½çš„è³‡æ–™
            stock_names = df[column_name]
            
            print(f"âœ… æˆåŠŸæŠ“å–å·¥ä½œè¡¨ '{sheet_name}' ä¸­ '{column_name}' æ¬„ä½çš„æ•¸æ“šã€‚")
            
            # è¼¸å‡ºåˆ—è¡¨å…§å®¹
            print("-" * 50)
            print("ã€è­‰åˆ¸åç¨±ã€‘åˆ—è¡¨ï¼š")
            print(stock_names.to_string(index=False)) # è¼¸å‡ºä¹¾æ·¨çš„åˆ—è¡¨
            print("-" * 50)
            
            return stock_names
        else:
            print(f"âŒ éŒ¯èª¤ï¼šå·¥ä½œè¡¨ '{sheet_name}' ä¸­æ‰¾ä¸åˆ°æ¬„ä½ '{column_name}'ã€‚")
            print(f"å¯¦éš›æ¬„ä½åç¨±ï¼š{list(df.columns)}")
            return None

    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„ Excel æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except ValueError as e:
        if "Worksheet named" in str(e):
            print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åç‚º '{sheet_name}' çš„å·¥ä½œè¡¨ã€‚è«‹æª¢æŸ¥æ¨™ç±¤åç¨±æ˜¯å¦æ­£ç¢ºã€‚")
        else:
            print(f"âŒ è®€å– Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")
        return None
    
# è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„æ•¸æ“šï¼Œä¸¦è¿”å›å…¶æŒ‡æ¨™è³‡æ–™ã€‚
def get_stock_indicators(file_path: str, target_name: str) -> Optional[pd.DataFrame]:
    """
    è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„æ•¸æ“šï¼Œä¸¦è¿”å›å…¶æŒ‡æ¨™è³‡æ–™ã€‚
    
    Args:
        file_path (str): CSV æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        target_name (str): è¦ç¯©é¸çš„è­‰åˆ¸åç¨± (e.g., 'å°ç»')ã€‚
        
    Returns:
        pd.DataFrame or None: åŒ…å«ç›®æ¨™è­‰åˆ¸æŒ‡æ¨™æ•¸æ“šçš„ DataFrameï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ æœå°‹ç›®æ¨™è­‰åˆ¸ï¼šã€{target_name}ã€‘çš„æŒ‡æ¨™æ•¸æ“š")

    # 1. è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except:
                df = pd.read_csv(file_path, encoding='big5')
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤æˆ–ç·¨ç¢¼å•é¡Œï¼š{e}")
        return None

    # 2. æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
    # æ ¹æ“š TWSE/BWIBBU æª”æ¡ˆå¸¸è¦‹çµæ§‹ï¼Œæ¬„ä½åç¨±å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œé€™è£¡æ²¿ç”¨ä¸Šä¸€æ¬¡çš„æ¬„ä½åç¨±ï¼Œ
    # ä½†è«‹æ ¹æ“šå¯¦éš›æª”æ¡ˆæƒ…æ³èª¿æ•´ï¼Œä¾‹å¦‚ï¼š'æœ¬ç›Šæ¯”' å¯èƒ½ç‚º 'PE Ratio'
    required_cols = ['è­‰åˆ¸åç¨±', 'æ®–åˆ©ç‡(%)', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']
    
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{missing_cols}ã€‚")
        print(f"æª”æ¡ˆå¯¦éš›æ¬„ä½åç¨±ï¼š{list(df.columns)}")
        # ç”±æ–¼ BWIBBU æª”æ¡ˆæ ¼å¼å¯èƒ½è¼ƒç‚ºè¤‡é›œï¼Œé€™è£¡å…ˆå‡è¨­æ¬„ä½åç¨±æ˜¯æ­£ç¢ºçš„ã€‚
        # å¦‚æœåŸ·è¡Œæ™‚å ±éŒ¯ï¼Œè«‹æª¢æŸ¥å¯¦éš› CSV æª”æ¡ˆä¸­çš„æ¬„ä½åç¨±ã€‚
        return None

    # 3. æ•¸æ“šæ¸…ç†èˆ‡ç¯©é¸
    # æ¸…ç† 'è­‰åˆ¸åç¨±' å…©å´ç©ºç™½ï¼Œç¢ºä¿ç²¾ç¢ºåŒ¹é…
    df['è­‰åˆ¸åç¨±'] = df['è­‰åˆ¸åç¨±'].astype(str).str.strip()

    # ç¯©é¸å‡ºç›®æ¨™è­‰åˆ¸åç¨±çš„æ•¸æ“š
    target_data = df[df['è­‰åˆ¸åç¨±'] == target_name]

    if target_data.empty:
        print(f"\nâ„¹ï¸ æç¤ºï¼šåœ¨æª”æ¡ˆä¸­æ‰¾ä¸åˆ°è­‰åˆ¸åç¨±ç‚º ã€{target_name}ã€‘ çš„æ•¸æ“šã€‚")
        return pd.DataFrame()
    
    # 4. æå–ç›®æ¨™æ¬„ä½æ•¸æ“š
    indicator_cols = ['è­‰åˆ¸åç¨±', 'æ®–åˆ©ç‡(%)', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']
    result_df = target_data[indicator_cols].copy() 

    # 5. è¼¸å‡ºçµæœ
    print(f"\nâœ… æˆåŠŸæ‰¾åˆ° ã€{target_name}ã€‘ çš„æŒ‡æ¨™æ•¸æ“šï¼š")
    print("=" * 40)
    
    # ä½¿ç”¨ to_string é€²è¡Œæ ¼å¼åŒ–è¼¸å‡º
    print(
        result_df.to_string(
            index=False,
            justify='left' # è®“æ–‡å­—é å·¦å°é½Š
        )
    )
    print("=" * 40)
    
    return result_df

# ä¸‰å¤§æ³•äººè²·è¶…å‰20
def get_top_20_institutional_trades_filtered(
    file_path: str, 
    volume_column: str = "ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸", 
    code_column: str = "è­‰åˆ¸ä»£è™Ÿ"
) -> Optional[pd.DataFrame]:
    """
    è®€å– CSV æª”æ¡ˆï¼Œé€²è¡Œä»¥ä¸‹ç¯©é¸ï¼š
    1. è­‰åˆ¸ä»£è™Ÿå¿…é ˆç‚º 4 ä½æ•¸å­—ã€‚
    2. ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸å¿…é ˆç‚ºæ­£æ•¸ (è²·è¶…)ã€‚
    3. è¿”å›è²·è³£è¶…è‚¡æ•¸æœ€å¤§çš„å‰ 10 åæ•¸æ“šï¼Œä¸¦è¼¸å‡ºç‚ºæ ¼å¼åŒ–è¡¨æ ¼ã€‚
    """
    print(f"\nğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ ç¯©é¸æ¢ä»¶ï¼š1. ä»£è™Ÿç‚º 4 ä½æ•¸å­— | 2. è²·è³£è¶…è‚¡æ•¸ > 0")

    # 1. è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except:
                df = pd.read_csv(file_path, encoding='big5')
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤æˆ–ç·¨ç¢¼å•é¡Œï¼š{e}")
        return None
    
    # 2. æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
    required_cols = [volume_column, code_column, 'è­‰åˆ¸åç¨±']
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{missing_cols}ã€‚")
        return None

    # 3. æ•¸æ“šæ¸…ç†èˆ‡æ•¸å€¼è½‰æ›
    try:
        # æ¸…ç†è²·è³£è¶…è‚¡æ•¸æ¬„ä½ï¼šç§»é™¤å¼•è™Ÿå’Œé€—è™Ÿ
        df[volume_column] = (
            df[volume_column].astype(str).str.replace('"', '', regex=False)
            .str.replace(',', '', regex=False).str.strip()
        )
        # è½‰æ›ç‚ºæ•¸å€¼é¡å‹ï¼Œç„¡æ³•è½‰æ›çš„å€¼è¨­ç‚º NaN
        df[volume_column] = pd.to_numeric(df[volume_column], errors='coerce')
        
        # æ¸…ç†è­‰åˆ¸ä»£è™Ÿæ¬„ä½
        df[code_column] = df[code_column].astype(str).str.strip()
        
        # ç§»é™¤ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼çš„è¡Œ
        df.dropna(subset=[volume_column], inplace=True)
        
    except Exception as e:
        print(f"âŒ æ•¸æ“šæ¸…ç†æˆ–æ•¸å€¼è½‰æ›å¤±æ•—ï¼š{e}")
        return None

    # 4. åŸ·è¡Œç¯©é¸æ¢ä»¶ 1ï¼šè­‰åˆ¸ä»£è™Ÿç‚º 4 ä½æ•¸å­—
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ç¯©é¸å‡ºå®Œå…¨ç¬¦åˆå››ä½æ•¸å­—çš„ä»£è™Ÿ
    df_filtered_code = df[df[code_column].str.match(r'^\d{4}$')]
    
    if df_filtered_code.empty:
        print("â„¹ï¸ æç¤ºï¼šç¯©é¸å¾Œï¼Œæ²’æœ‰æ‰¾åˆ°è­‰åˆ¸ä»£è™Ÿç‚º 4 ä½æ•¸å­—çš„æ•¸æ“šã€‚")
        return pd.DataFrame()

    # 5. åŸ·è¡Œç¯©é¸æ¢ä»¶ 2ï¼šè²·è³£è¶…è‚¡æ•¸ç‚ºæ­£æ•¸ (è²·è¶…)
    df_filtered_positive = df_filtered_code[df_filtered_code[volume_column] > 0]

    if df_filtered_positive.empty:
        print("â„¹ï¸ æç¤ºï¼šç¯©é¸å¾Œï¼Œæ²’æœ‰æ‰¾åˆ°ä¸‰å¤§æ³•äººè²·è¶… (æ­£æ•¸) çš„æ•¸æ“šã€‚")
        return pd.DataFrame()

    # 6. æ’åºä¸¦å–å‡ºå‰ 20 å
    df_sorted = df_filtered_positive.sort_values(
        by=volume_column, 
        ascending=False # è²·è¶…æ•¸æœ€å¤§çš„æ’åœ¨æœ€å‰é¢
    )
    
    # å–å‡ºå‰ 20 ç­†æ•¸æ“š
    top_20_trades = df_sorted.head(20)

    # 7. è¼¸å‡ºçµæœ (å›ºå®šæ¬„ä½å¯¬åº¦èˆ‡ç½®ä¸­å°é½Š)
    
    print(f"\nâœ… ç¯©é¸å¾Œçš„ä¸‰å¤§æ³•äººè²·è¶…å‰ {len(top_20_trades)} åï¼š")
    print("=" * 40)
    
    # æ ¼å¼åŒ–è¼¸å‡ºï¼šå°‡è‚¡æ•¸è½‰æ›ç‚ºæ•´æ•¸æ ¼å¼ï¼Œä¸¦åŠ ä¸Šåƒåˆ†ä½é€—è™Ÿ
    top_20_trades_display = top_20_trades.copy()
    
    # é‡æ–°å‘½åæ¬„ä½ä»¥ç°¡åŒ–æ¨™é¡Œ
    volume_col_display_name = 'è²·è¶…å¼µæ•¸'
    top_20_trades_display = top_20_trades_display.rename(
        columns={'è­‰åˆ¸ä»£è™Ÿ': 'ä»£è™Ÿ', volume_column: volume_col_display_name}
    )

    # æ ¼å¼åŒ–æ•¸å­— (åŠ ä¸Šåƒåˆ†ä½é€—è™Ÿ)
    top_20_trades_display[volume_col_display_name] = top_20_trades_display[volume_col_display_name].apply(lambda x: f"{int(x/1000):,}")

    # å®šç¾©è¼¸å‡ºçš„æ¬„ä½é †åº
    #actual_display_cols = ['ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', volume_col_display_name]
    actual_display_cols = ['è­‰åˆ¸åç¨±', volume_col_display_name]

    # è¨­å®šæ¯å€‹æ¬„ä½çš„æœ€å°å¯¬åº¦ï¼Œä»¥åˆ©ç½®ä¸­ (ä¸­æ–‡å­—ä½” 2 å¯¬åº¦)
    col_space_width = 8 

    # ä½¿ç”¨ to_string é…åˆ col_space å’Œ justify='center'
    print(
        top_20_trades_display[actual_display_cols].to_string(
            index=False,
            col_space=col_space_width, 
            justify='left' # å˜—è©¦ç½®ä¸­å°é½Š
        )
    )
    print("=" * 40)
    
    top_20_trades = ""
    index_no = 1
    target_length = 6
    for index, rol in top_20_trades_display.iterrows():
        index_no_str = str(index_no).zfill(2)
        
        current_length = len(rol['è­‰åˆ¸åç¨±'].strip())
        
        current_volume_column = rol['è­‰åˆ¸åç¨±'].strip()
        if current_length <= target_length:
        # è¨ˆç®—éœ€è¦å¡«å……çš„é•·åº¦
            padding_needed = target_length - current_length 
            new_current_volume_column = current_volume_column.ljust(padding_needed, ' ')
        
        top_20_trades += f"{index_no_str}." + f" {new_current_volume_column} " + f" ({rol[volume_col_display_name]}å¼µ)\n"
        index_no += 1
        
    return top_20_trades

# è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„è³‡æ–™ï¼Œä¸¦åªè¿”å›ã€Œè²·è³£è¶…è‚¡æ•¸ã€æ•¸æ“šã€‚
def get_stock_net_volume(file_path, target_name, target_column="ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸"):
    """
    è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„è³‡æ–™ï¼Œä¸¦åªè¿”å›ã€Œè²·è³£è¶…è‚¡æ•¸ã€æ•¸æ“šã€‚
    Args:
        file_path (str): CSVæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        target_name (str): è¦ç¯©é¸çš„è­‰åˆ¸åç¨±ã€‚
        target_column (str): è¦å–å‡ºçš„æ¬„ä½åç¨± (é è¨­ç‚º 'è²·è³£è¶…è‚¡æ•¸')ã€‚
    Returns:
        pd.Series or None: åŒ…å«ç›®æ¨™è²·è³£è¶…è‚¡æ•¸çš„ Seriesï¼Œå¦‚æœè®€å–æˆ–ç¯©é¸å¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ æœå°‹ç›®æ¨™ï¼šã€{target_name}ã€‘ï¼Œä¸¦å–å‡ºã€{target_column}ã€‘æ•¸æ“š")

    # 1. è®€å–CSVæª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦ï¼Œç¢ºä¿è¼¸å…¥æ­£ç¢º)
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'utf-8-sig' ç·¨ç¢¼è®€å–ã€‚")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                print("â„¹ï¸ ä½¿ç”¨ 'utf-8' ç·¨ç¢¼è®€å–ã€‚")
            except:
                df = pd.read_csv(file_path, encoding='big5')
                print("â„¹ï¸ ä½¿ç”¨ 'big5' ç·¨ç¢¼è®€å–ã€‚")
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤æˆ–ç·¨ç¢¼å•é¡Œï¼š{e}")
        return None
    
    # 2. æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
    required_cols = ['è­‰åˆ¸åç¨±', target_column]
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{missing_cols}ã€‚")
        print(f"æª”æ¡ˆå¯¦éš›æ¬„ä½åç¨±ï¼š{list(df.columns)}")
        return None

    # 3. æ•¸æ“šæ¸…ç†èˆ‡ç¯©é¸
    # æ¸…ç† 'è­‰åˆ¸åç¨±' å…©å´ç©ºç™½ï¼Œç¢ºä¿ç²¾ç¢ºåŒ¹é…
    df['è­‰åˆ¸åç¨±'] = df['è­‰åˆ¸åç¨±'].astype(str).str.strip()

    # â­ æ ¸å¿ƒä¿®æ”¹é» A: æ¸…ç† 'è²·è³£è¶…è‚¡æ•¸' æ¬„ä½ï¼Œç§»é™¤å¼•è™Ÿä¸¦æ¸…ç†ç©ºç™½ï¼Œç‚ºæ•¸å€¼è½‰æ›åšæº–å‚™
    try:
        df[target_column] = df[target_column].astype(str).str.replace('"', '', regex=False).str.strip()
        # print(f"âœ… æˆåŠŸç§»é™¤ {target_column} æ¬„ä½ä¸­çš„é›™å¼•è™Ÿã€‚")
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Šï¼šå˜—è©¦æ¸…ç† {target_column} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    target_data = df[df['è­‰åˆ¸åç¨±'] == target_name]

    # 4. å–å‡ºç›®æ¨™æ¬„ä½æ•¸æ“š
    if target_data.empty:
        print(f"\nâ„¹ï¸ æç¤ºï¼šåœ¨æª”æ¡ˆä¸­æ‰¾ä¸åˆ°è­‰åˆ¸åç¨±ç‚º ã€{target_name}ã€‘ çš„æ•¸æ“šã€‚")
        return pd.Series(dtype='object')
    else:
        # å–å‡º 'è²·è³£è¶…è‚¡æ•¸' æ¬„ä½ï¼Œé€™æ˜¯ä¸€å€‹ pandas.Series å°è±¡
        net_volume_series = target_data[target_column]
        
        print(f"\nâœ… æˆåŠŸæ‰¾åˆ° ã€{target_name}ã€‘ çš„ {len(net_volume_series)} ç­†ã€{target_column}ã€‘æ•¸æ“šã€‚")
        print("-" * 60)
        # é€™è£¡ä¸é¡¯ç¤º Series åŸå§‹å…§å®¹ï¼Œè®“æœ€çµ‚è¼¸å‡ºæ›´èšç„¦
        
    return net_volume_series

# è®€å–æŒ‡å®šçš„CSVæª”æ¡ˆï¼Œé¸å–ç¬¬2æ¬„åˆ°ç¬¬6æ¬„çš„æ•¸æ“š
def select_and_save_columns_fix_encoding(input_file_path, output_directory, output_file_name="selected_data.csv"):
    """
    è®€å–æŒ‡å®šçš„CSVæª”æ¡ˆï¼Œé¸å–ç¬¬2æ¬„åˆ°ç¬¬6æ¬„çš„æ•¸æ“šï¼Œ
    ä¸¦å°‡çµæœå¦å­˜ç‚ºæ–°çš„CSVæª”æ¡ˆï¼Œä½¿ç”¨ 'utf-8-sig' ç·¨ç¢¼è§£æ±ºè¼¸å‡ºä¸­æ–‡äº‚ç¢¼å•é¡Œã€‚

    Args:
        input_file_path (str): ä¾†æº CSV æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        output_directory (str): å„²å­˜æ–° CSV æª”æ¡ˆçš„ç›®éŒ„è·¯å¾‘ã€‚
        output_file_name (str): å„²å­˜æ–° CSV æª”æ¡ˆçš„åç¨±ã€‚

    Returns:
        bool: æˆåŠŸå„²å­˜å‰‡è¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
    """
    # çµ„åˆå®Œæ•´çš„è¼¸å‡ºè·¯å¾‘
    output_file_path = os.path.join(output_directory, output_file_name)
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(output_directory, exist_ok=True)
    
    print(f"ğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{input_file_path}")

    try:
        # 1. è®€å–CSVæª”æ¡ˆ (ä¿ç•™å¤šç·¨ç¢¼å˜—è©¦ï¼Œç¢ºä¿è¼¸å…¥æ­£ç¢º)
        try:
            df = pd.read_csv(input_file_path, encoding='big5')
            print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'big5' ç·¨ç¢¼è®€å–æª”æ¡ˆã€‚")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(input_file_path, encoding='utf-8')
                print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'utf-8' ç·¨ç¢¼è®€å–æª”æ¡ˆã€‚")
            except Exception:
                df = pd.read_csv(input_file_path, encoding='cp950')
                print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'cp950' ç·¨ç¢¼è®€å–æª”æ¡ˆã€‚")
                
        # 2. åˆ¤æ–·æ¬„ä½æ•¸é‡æ˜¯å¦è¶³å¤ ï¼Œä¸¦é¸å–ç¬¬ 2 æ¬„åˆ°ç¬¬ 6 æ¬„ (ç´¢å¼• 1 åˆ° 5)
        start_index = 1 
        end_index = 6
        num_columns = len(df.columns)
        
        if num_columns < end_index:
            print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆæ¬„ä½ç¸½æ•¸ä¸è¶³ {end_index} æ¬„ (åªæœ‰ {num_columns} æ¬„)ã€‚ç„¡æ³•é¸å–ç¬¬ 2 åˆ° 6 æ¬„ã€‚")
            return False

        # é¸å–æ•¸æ“š
        df_selected = df.iloc[:, start_index:end_index]
        
        print(f"âœ… æˆåŠŸé¸å–æ¬„ä½ï¼š{list(df_selected.columns)}")

        # --- æ ¸å¿ƒä¿®æ”¹é» ---
        # 3. å¦å­˜ç‚ºæ–°çš„ CSV æª”æ¡ˆï¼Œä½¿ç”¨ 'utf-8-sig' ç·¨ç¢¼
        # 'utf-8-sig' åŒ…å« BOM (Byte Order Mark)ï¼Œæœ‰åŠ©æ–¼ Excel ç­‰è»Ÿé«”æ­£ç¢ºè­˜åˆ¥ä¸­æ–‡æª”é ­ã€‚
        df_selected.to_csv(output_file_path, index=False, encoding='utf-8-sig') 
        # -------------------

        print("-" * 40)
        print(f"âœ¨ æ•¸æ“šå·²æˆåŠŸå„²å­˜åˆ°ï¼š{output_file_path}")
        print("-" * 40)
        return True

    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {input_file_path}")
        return False
    except pd.errors.EmptyDataError:
        print(f"âŒ éŒ¯èª¤ï¼šæª”æ¡ˆæ˜¯ç©ºçš„æˆ–ç„¡æ•ˆçš„æ•¸æ“šæ ¼å¼ -> {input_file_path}")
        return False
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤ï¼š{e}")
        return False

# å°‡æŒ‡å®šæª”æ¡ˆè¤‡è£½åˆ°ç›®æ¨™ç›®éŒ„ã€‚
def copy_file_to_directory(source: str, destination: str):
    """
    å°‡æŒ‡å®šæª”æ¡ˆè¤‡è£½åˆ°ç›®æ¨™ç›®éŒ„ã€‚
    """
    source_path = pathlib.Path(source)
    destination_dir_path = pathlib.Path(destination)
    
    print(f"âœ… æ­£åœ¨æº–å‚™è¤‡è£½æª”æ¡ˆ...")
    print(f"ä¾†æº: {source_path}")
    print(f"ç›®æ¨™ç›®éŒ„: {destination_dir_path}")

    # 1. æª¢æŸ¥ä¾†æºæª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not source_path.exists() or not source_path.is_file():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆæˆ–ä¾†æºä¸æ˜¯ä¸€å€‹æª”æ¡ˆ: {source}")
        return

    # 2. æª¢æŸ¥ç›®æ¨™ç›®éŒ„æ˜¯å¦å­˜åœ¨ (å¦‚æœä¸å­˜åœ¨ï¼Œshutil.copy æœƒè‡ªå‹•å‰µå»ºï¼Œä½†æˆ‘å€‘æœ€å¥½å…ˆæª¢æŸ¥ä¸¦åˆ—å°è¨Šæ¯)
    if not destination_dir_path.exists():
        print(f"âš ï¸ è­¦å‘Šï¼šç›®æ¨™ç›®éŒ„ '{destination}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å˜—è©¦å»ºç«‹...")
        try:
            destination_dir_path.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•å»ºç«‹ç›®æ¨™ç›®éŒ„: {e}")
            return
    
    try:
        # 3. åŸ·è¡Œè¤‡è£½æ“ä½œ
        # shutil.copy æœƒå°‡æª”æ¡ˆè¤‡è£½åˆ°ç›®éŒ„ä¸­ï¼Œä¸¦ä¿ç•™åŸå§‹æª”æ¡ˆå
        shutil.copy(source, destination)
        
        # å»ºç«‹å®Œæ•´çš„ç›®æ¨™è·¯å¾‘ç”¨æ–¼è¼¸å‡ºè¨Šæ¯
        destination_file_path = destination_dir_path / source_path.name
        
        print("\n" + "="*50)
        print("ğŸ‰ æª”æ¡ˆè¤‡è£½æˆåŠŸï¼")
        print(f"æ–°æª”æ¡ˆä½ç½®: {destination_file_path}")
        print("="*50)

    except Exception as e:
        print(f"âŒ è¤‡è£½æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# å¾æŒ‡å®šçš„ CSV æª”æ¡ˆä¸­æŸ¥è©¢ç‰¹å®šè­‰åˆ¸çš„æ”¶ç›¤åƒ¹ã€‚
def lookup_stock_price(file_path: str, stock_name: str, name_col: str, price_col: str):
    """
    å¾æŒ‡å®šçš„ CSV æª”æ¡ˆä¸­æŸ¥è©¢ç‰¹å®šè­‰åˆ¸çš„æ”¶ç›¤åƒ¹ã€‚
    """
    file = pathlib.Path(file_path)
    
    #print(f"âœ… æ­£åœ¨å˜—è©¦è®€å–æª”æ¡ˆ: {file.name}")
    #print(f"ğŸ” æŸ¥è©¢ç›®æ¨™: {stock_name}")
    
    if not file.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆåœ¨è·¯å¾‘ï¼š{file_path}")
        return

    try:
        # è®€å– CSV æª”æ¡ˆï¼Œä½¿ç”¨ Big5 ç·¨ç¢¼ (è‡ºç£é‡‘èæ•¸æ“šå¸¸ç”¨)ï¼Œä¸¦æ¸…ç†æ¬„ä½åç¨±çš„ç©ºç™½
        df = pd.read_csv(file_path, encoding='utf-8', skipinitialspace=True)
        df.columns = df.columns.str.strip()
        
        # æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
        if name_col not in df.columns or price_col not in df.columns:
            print(f"âŒ éŒ¯èª¤ï¼šCSV æª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ ('{name_col}' æˆ– '{price_col}')ã€‚")
            return

        # ç¢ºä¿æ¯”å°æ¬„ä½æ˜¯å­—ä¸²ä¸”æ¸…ç†ç©ºç™½
        df[name_col] = df[name_col].astype(str).str.strip()

        # åŸ·è¡Œç¯©é¸
        result = df[df[name_col] == stock_name]

        if result.empty:
            print(f"\nâš ï¸ è­¦å‘Šï¼šåœ¨æª”æ¡ˆä¸­æ‰¾ä¸åˆ° '{stock_name}' çš„æ”¶ç›¤åƒ¹è³‡æ–™ã€‚")
            return

        # å–å¾—æ”¶ç›¤åƒ¹ï¼Œåªå–ç¬¬ä¸€å€‹çµæœï¼ˆå› ç‚ºå¯èƒ½æœ‰å¤šè¡Œç›¸åŒåç¨±ï¼Œä½†é€šå¸¸åªå–ç¬¬ä¸€ç­†ï¼‰
        price = result.iloc[0][price_col]
        
        # print("\n" + "="*50)
        # print(f"ğŸ‰ æŸ¥è©¢çµæœ ({file.name})")
        # print(f"è­‰åˆ¸åç¨±: {stock_name}")
        # print(f"æ”¶ç›¤åƒ¹ ({price_col}): **{price}**")
        # print("="*50)
        return price    
    except Exception as e:
        print(f"âŒ è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

# å¾äº¤æ˜“æ—¥æª”æ¡ˆä¸­ï¼Œæ‰¾å‡ºä»Šå¤©å¾€å‰æ•¸ N å€‹äº¤æ˜“æ—¥ï¼Œä¸¦æ ¹æ“šç•¶å‰æ™‚é–“ (15:00) åˆ¤æ–·æ˜¯å¦ç´å…¥ä»Šå¤©ã€‚
def find_last_n_trading_days_with_time_check(file_path, n=6):
    """
    å¾äº¤æ˜“æ—¥æª”æ¡ˆä¸­ï¼Œæ‰¾å‡ºä»Šå¤©å¾€å‰æ•¸ N å€‹äº¤æ˜“æ—¥ï¼Œä¸¦æ ¹æ“šç•¶å‰æ™‚é–“ (15:00) åˆ¤æ–·æ˜¯å¦ç´å…¥ä»Šå¤©ã€‚

    :param file_path: è‚¡ç¥¨äº¤æ˜“æ—¥ CSV æª”æ¡ˆè·¯å¾‘
    :param n: å¾€å‰æ‰¾çš„äº¤æ˜“æ—¥æ•¸é‡ (é è¨­ç‚º 6)
    --å–6å€‹ä½†æœ€å¾Œä¸€å€‹ä¸é¡¯ç¤ºï¼Œä½œç‚ºæ•¸æ“šè¨ˆç®—ç”¨
    :return: åŒ…å«æœ€è¿‘ N å€‹äº¤æ˜“æ—¥çš„ DataFrame (æˆ– None if failed)
    """
    
    # 1. å®šç¾©ç•¶å‰æ™‚é–“å’Œåˆ¤æ–·æ¨™æº–
    now = datetime.now()
    today_date = now.date()
    cutoff_time = time_TimeClass(15, 0, 0) # ä¸‹åˆ 15:00:00
    is_after_cutoff = now.time() >= cutoff_time

    print(f"ç•¶å‰æ—¥æœŸ: {today_date.strftime('%Y/%m/%d')}, ç•¶å‰æ™‚é–“æ˜¯å¦åœ¨ 15:00 ä¹‹å¾Œ: {is_after_cutoff}")
    
    # 2. è®€å–äº¤æ˜“æ—¥æª”æ¡ˆ
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}")
        return None
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘å’Œç·¨ç¢¼: {e}")
        return None

    # å‡è¨­æ—¥æœŸæ¬„ä½ç‚º 'æ—¥æœŸ'
    date_column = 'æ—¥æœŸ' 
    if date_column not in df.columns:
        # å˜—è©¦ä½¿ç”¨å¸¸è¦‹çš„è‹±æ–‡æ¬„ä½å
        if 'Date' in df.columns:
            date_column = 'Date'
        else:
            print(f"éŒ¯èª¤ï¼šç„¡æ³•è­˜åˆ¥äº¤æ˜“æ—¥æœŸçš„æ¬„ä½åç¨±ã€‚è«‹æª¢æŸ¥æ‚¨çš„ CSV æª”æ¡ˆã€‚")
            return None
        
    # 3. æ¸…ç†å’Œè½‰æ›æ—¥æœŸæ ¼å¼
    df[date_column] = pd.to_datetime(df[date_column], errors='coerce').dt.normalize()
    df.dropna(subset=[date_column], inplace=True)
    
    # å»ºç«‹æ‰€æœ‰äº¤æ˜“æ—¥çš„é›†åˆï¼Œç”¨æ–¼å¿«é€Ÿåˆ¤æ–·ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥
    all_trading_dates = set(df[date_column].dt.date)
    is_today_trading_day = today_date in all_trading_dates
    
    print(f"ä»Šå¤© ({today_date.strftime('%Y/%m/%d')}) æ˜¯å¦ç‚ºäº¤æ˜“æ—¥: {is_today_trading_day}")

    # 4. æ ¹æ“šæ™‚é–“åˆ¤æ–·æ±ºå®šè³‡æ–™ç¯©é¸çš„æˆªæ­¢æ—¥æœŸ
    
    # é è¨­ï¼šå¦‚æœä¸æ»¿è¶³ç´å…¥ä»Šå¤©çš„æ¢ä»¶ï¼Œå‰‡æˆªæ­¢æ—¥æœŸç‚ºæ˜¨å¤©
    inclusion_date = today_date - timedelta(days=1)
    
    # åˆ¤æ–·æ˜¯å¦æ‡‰è©²ç´å…¥ä»Šå¤©
    if is_today_trading_day and is_after_cutoff:
        # æ¢ä»¶ 1: ä»Šå¤©æ˜¯äº¤æ˜“æ—¥
        # æ¢ä»¶ 2: ä¸”æ™‚é–“åœ¨ 15:00 ä¹‹å¾Œ (è¦–ç‚ºä»Šå¤©äº¤æ˜“å·²å®Œæˆ)
        # -> ç´å…¥ä»Šå¤©
        inclusion_date = today_date
        print("-> åˆ¤æ–·ï¼šç´å…¥ä»Šå¤©çš„äº¤æ˜“æ—¥ã€‚")
    else:
        # å…¶ä»–æƒ…æ³ (éäº¤æ˜“æ—¥ã€æˆ–äº¤æ˜“æ—¥ä½†æœªæ»¿ 15:00)
        # -> æ’é™¤ä»Šå¤©ï¼Œåªå–æ˜¨å¤©åŠæ›´æ—©çš„äº¤æ˜“æ—¥
        inclusion_date = today_date - timedelta(days=1)
        print("-> åˆ¤æ–·ï¼šæ’é™¤ä»Šå¤©çš„äº¤æ˜“æ—¥ï¼Œåªå–æ˜¨å¤©åŠæ›´æ—©çš„æ—¥æœŸã€‚")

    # 5. ç¯©é¸ã€æ’åºä¸¦é¸å–æœ€è¿‘ N å€‹äº¤æ˜“æ—¥
    
    # ç¯©é¸å‡ºæ—¥æœŸå°æ–¼æˆ–ç­‰æ–¼æ±ºå®šæˆªæ­¢æ—¥æœŸçš„äº¤æ˜“æ—¥
    df_past = df[df[date_column].dt.date <= inclusion_date]
    
    # ç¢ºä¿æ—¥æœŸç”±è¿‘åˆ°é æ’åº
    df_past = df_past.sort_values(by=date_column, ascending=False)

    # é¸å–æœ€è¿‘çš„ N å€‹äº¤æ˜“æ—¥
    last_n_days = df_past.head(n)

    if last_n_days.empty:
        print(f"è­¦å‘Šï¼šäº¤æ˜“æ—¥è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•æ‰¾åˆ°å¾€å‰ {n} å€‹äº¤æ˜“æ—¥ã€‚")
        return None

    # å°‡çµæœç”±èˆŠåˆ°æ–°æ’åºä¸¦æ ¼å¼åŒ–è¼¸å‡º
    last_n_days = last_n_days.sort_values(by=date_column, ascending=True)
    last_n_days[date_column] = last_n_days[date_column].dt.strftime('%Y/%m/%d')
    
    print(f"\nâœ… æˆåŠŸæ‰¾åˆ°ä»Šå¤©å¾€å‰ {n} å€‹äº¤æ˜“æ—¥ã€‚")
    return last_n_days

# å¾ Excel æª”æ¡ˆä¸­è®€å–è‚¡ç¥¨åº«å­˜ï¼Œå°‡å…¶å¦å­˜ç‚º CSV æª”æ¡ˆã€‚
def extract_excel_sheet_filter_and_save(excel_file_path: str, sheet_name: str, filter_column: str, filter_value: any, output_dir: str = None) -> pathlib.Path:
    """
    å¾æŒ‡å®šçš„ Excel æª”æ¡ˆä¸­è®€å–ç‰¹å®šå·¥ä½œè¡¨ï¼Œè·³éç¬¬äºŒè¡Œï¼Œç¯©é¸è³‡æ–™å¾Œï¼Œå°‡å…¶å¦å­˜ç‚º CSV æª”æ¡ˆã€‚

    Args:
        excel_file_path (str): åŸå§‹ Excel æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        sheet_name (str): è¦è®€å–çš„å·¥ä½œè¡¨åç¨± (ä¾‹å¦‚: 'è‚¡ç¥¨åº«å­˜çµ±è¨ˆ')ã€‚
        filter_column (str): è¦é€²è¡Œç¯©é¸çš„æ¬„ä½åç¨± (ä¾‹å¦‚: 'ç›®å‰è‚¡æ•¸åº«å­˜çµ±è¨ˆ')ã€‚
        filter_value (any): è¦ç¯©é™¤çš„å€¼ã€‚
        output_dir (str, optional): CSV æª”æ¡ˆçš„å„²å­˜ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡å„²å­˜åœ¨åŸå§‹æª”æ¡ˆçš„ç›®éŒ„ã€‚

    Returns:
        Path: å„²å­˜æˆåŠŸçš„ CSV æª”æ¡ˆè·¯å¾‘ã€‚
    """
    
    original_path = pathlib.Path(excel_file_path)
    
    if not original_path.exists():
        raise FileNotFoundError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° Excel æª”æ¡ˆåœ¨è·¯å¾‘ï¼š{excel_file_path}")

    print(f"âœ… æ­£åœ¨è®€å– Excel æª”æ¡ˆï¼š{original_path.name}")
    print(f"ğŸ¯ ç›®æ¨™å·¥ä½œè¡¨åç¨±ï¼š{sheet_name}")

    try:
        # 1. è®€å– Excel ä¸­æŒ‡å®šå·¥ä½œè¡¨çš„è³‡æ–™
        # header=0: æŒ‡å®š Excel çš„ç¬¬ä¸€è¡Œï¼ˆç´¢å¼• 0ï¼‰ä½œç‚ºæ¬„ä½åç¨±
        # skiprows=[1]: è·³éç´¢å¼•ç‚º 1 çš„è¡Œï¼Œå³ Excel ä¸­çš„ç¬¬äºŒè¡Œ
        df = pd.read_excel(
            original_path, 
            sheet_name=sheet_name, 
            header=0,
            skiprows=[1]  # <--- â— é€™è£¡åŠ å…¥è·³é Excel ç¬¬äºŒè¡Œï¼ˆç´¢å¼• 1ï¼‰çš„è¨­å®š
        )
        
        if df.empty:
            print(f"è­¦å‘Šï¼šå·¥ä½œè¡¨ '{sheet_name}' è®€å–åˆ°çš„æ•¸æ“šç‚ºç©ºã€‚")
            return None

    except ValueError as e:
        raise ValueError(f"éŒ¯èª¤ï¼šåœ¨ Excel æª”æ¡ˆä¸­æ‰¾ä¸åˆ°åç‚º '{sheet_name}' çš„å·¥ä½œè¡¨ã€‚è«‹æª¢æŸ¥åç¨±æ˜¯å¦æ­£ç¢ºã€‚è©³ç´°éŒ¯èª¤: {e}")
    except Exception as e:
        raise Exception(f"è®€å– Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
    # 2. **ã€é—œéµç¯©é¸æ­¥é©Ÿã€‘**
    if filter_column not in df.columns:
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°ç¯©é¸æ¬„ä½ '{filter_column}'ã€‚è·³éç¯©é¸æ­¥é©Ÿã€‚")
    else:
        initial_rows = len(df)
        print(f"\nğŸ” é–‹å§‹ç¯©é¸ï¼šç§»é™¤ '{filter_column}' å€¼ç‚º '{filter_value}' çš„è³‡æ–™...")
        
        # å˜—è©¦å°‡ç¯©é¸æ¬„ä½è½‰æ›ç‚ºæ•¸å€¼é¡å‹ï¼Œcoerce æœƒå°‡éæ•¸å€¼è½‰æ›ç‚º NaN
        df[filter_column] = pd.to_numeric(df[filter_column], errors='coerce')
        
        # ç¯©é¸é‚è¼¯ï¼šä¿ç•™ 'ç›®å‰è‚¡æ•¸åº«å­˜çµ±è¨ˆ' ä¸ç­‰æ–¼ 0 çš„è¡Œ
        df_filtered = df[df[filter_column] != float(filter_value)]
        
        removed_rows = initial_rows - len(df_filtered)
        print(f"  -> åŸå§‹ç­†æ•¸ (å·²è·³éç¬¬äºŒè¡Œ): {initial_rows} ç­†")
        print(f"  -> ç§»é™¤ç­†æ•¸: {removed_rows} ç­†")
        print(f"  -> å‰©é¤˜ç­†æ•¸: {len(df_filtered)} ç­†")
        
        df = df_filtered
        
        if df.empty:
            print("è­¦å‘Šï¼šç¯©é¸å¾Œæ•¸æ“šç‚ºç©ºã€‚")
            return None


    # 3. æº–å‚™è¼¸å‡º CSV æª”æ¡ˆçš„è·¯å¾‘
    
    if output_dir is None:
        output_dir = original_path.parent
    else:
        output_dir = pathlib.Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("_%Y%m%d_%H%M%S")
    csv_file_name = f"{sheet_name}_filtered{timestamp}.csv"
    output_csv_path = output_dir / csv_file_name
    
    # 4. å„²å­˜ç‚º CSV æª”æ¡ˆ
    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')

    return output_csv_path

# è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºåœ¨ä»Šå¤©æˆ–ä»Šå¤©ä¹‹å‰çš„æ‰€æœ‰æ—¥æœŸï¼Œä¸¦ä»¥ YYYYMMDD å­—ä¸²æ ¼å¼è¿”å›ã€‚
def get_past_dates_in_yyyymmdd(file_path, date_column_name='æ—¥æœŸ'):
    """
    è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºåœ¨ä»Šå¤©æˆ–ä»Šå¤©ä¹‹å‰çš„æ‰€æœ‰æ—¥æœŸï¼Œä¸¦ä»¥ YYYYMMDD å­—ä¸²æ ¼å¼è¿”å›ã€‚

    Args:
        file_path (str): CSV æª”æ¡ˆçš„è·¯å¾‘ã€‚
        date_column_name (str): CSV ä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ã€‚é è¨­ç‚º 'Date'ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰éå»æ—¥æœŸçš„ YYYYMMDD æ ¼å¼å­—ä¸²åˆ—è¡¨ï¼Œå¦‚æœå‡ºéŒ¯å‰‡è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    try:
        # 1. è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(file_path)

        # 2. ç¢ºä¿æ—¥æœŸæ¬„ä½æ˜¯ datetime æ ¼å¼
        # errors='coerce' æœƒå°‡ç„¡æ³•è§£æçš„å€¼è¨­ç‚º NaT
        df[date_column_name] = pd.to_datetime(df[date_column_name], errors='coerce')

        # 3. ç²å–ä»Šå¤©çš„æ—¥æœŸ (åªå–å¹´æœˆæ—¥éƒ¨åˆ†)
        # ä»Šå¤©çš„æ—¥æœŸç‚º 2025-11-01
        today = pd.to_datetime(datetime.now().date()) 
        
        # 4. ç¯©é¸å‡ºä»Šå¤©ä¹‹å‰ (å³ <= ä»Šå¤©) çš„æ—¥æœŸè³‡æ–™
        # ç¯©é¸æ¢ä»¶æ˜¯ï¼šæ—¥æœŸæ¬„ä½å€¼ <= ä»Šå¤©çš„æ—¥æœŸ
        past_dates_df = df[df[date_column_name] <= today]

        # 5. ç§»é™¤æ—¥æœŸç‚º NaT çš„åˆ—
        past_dates_df = past_dates_df.dropna(subset=[date_column_name])
        
        # 6. æ’åº (å¯é¸ï¼Œé€šå¸¸æ—¥æœŸè³‡æ–™æŒ‰æ™‚é–“é †åºæ’åˆ—è¼ƒå¥½)
        past_dates_df = past_dates_df.sort_values(by=date_column_name)
        
        # 7. **ã€é—œéµã€‘æ ¼å¼åŒ–ä¸¦è¿”å›æ—¥æœŸåˆ—è¡¨**
        # ä½¿ç”¨ .dt.strftime('%Y%m%d') å°‡ datetime ç‰©ä»¶è½‰æ›ç‚º YYYYMMDD æ ¼å¼çš„å­—ä¸²
        yyyymmdd_list = past_dates_df[date_column_name].dt.strftime('%Y%m%d').tolist()
        
        return yyyymmdd_list

    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆåœ¨è·¯å¾‘ï¼š{file_path}")
        return []
    except KeyError:
        print(f"éŒ¯èª¤ï¼šCSV æª”æ¡ˆä¸­æ‰¾ä¸åˆ°åç‚º '{date_column_name}' çš„æ—¥æœŸæ¬„ä½ã€‚")
        return []
    except Exception as e:
        print(f"ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤ï¼š{e}")
        return []

# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”ç¢ºå¯¦æ˜¯ä¸€å€‹æª”æ¡ˆ (éè³‡æ–™å¤¾)
def check_folder_and_create(folder_path: str):
    """
    åƒæ•¸:
        file_path (str): è¦æª¢æŸ¥çš„æª”æ¡ˆè·¯å¾‘ã€‚
    å›å‚³:
        bool: æª”æ¡ˆå­˜åœ¨æ™‚å›å‚³ Trueï¼›å¦å‰‡å›å‚³ Falseã€‚
    """
    OUTPUT_DIR, filename_new = jutils.get_path_to_folder_file(folder_path)
    jutils.check_and_create_folder(OUTPUT_DIR)
    jutils.check_file_exists(filename_new)
    return True

# å…±ç”¨çš„è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼è™•ç† TWSE çš„ Big5 ç·¨ç¢¼å’Œ Pandas è®€å–é‚è¼¯ã€‚
def _read_twse_csv(response_text: str, header_row: int) -> Optional[pd.DataFrame]:
    """
    Args:
        response_text: HTTP è«‹æ±‚å›å‚³çš„æ–‡å­—å…§å®¹ (Big5 ç·¨ç¢¼)ã€‚
        header_row: CSV æª”æ¡ˆä¸­è³‡æ–™è¡¨é ­æ‰€åœ¨çš„è¡Œæ•¸ (0-indexed)ã€‚
    Returns:
        Optional[pd.DataFrame]: è™•ç†å¾Œçš„ DataFrameã€‚
    """
    try:
        csv_data = StringIO(response_text)
        # å˜—è©¦è®€å– CSV
        df = pd.read_csv(
            csv_data, 
            header=header_row,          # è³‡æ–™è¡¨é ­æ‰€åœ¨çš„è¡Œæ•¸
            skipinitialspace=True,      # è·³éåˆ†éš”ç¬¦å¾Œçš„ç©ºæ ¼
            on_bad_lines='skip',        # è·³éæ ¼å¼ä¸æ­£ç¢ºçš„è¡Œ
            encoding='Big5'             # ä½¿ç”¨ Big5 ç·¨ç¢¼è®€å–
        )
        # TWSE çš„ CSV æ¬„ä½åç¨±å¸¸æœ‰éš±è—ç©ºæ ¼ï¼Œå°è‡´ df.columns ç„¡æ³•æ­£ç¢ºåŒ¹é…ã€‚
        if not df.empty:
            df.columns = df.columns.str.strip()
        # ç§»é™¤æ‰€æœ‰æ¬„ä½çš†ç‚ºç©ºçš„è¡Œ
        df = df.dropna(how='all')
        # ç§»é™¤è³‡æ–™å°¾éƒ¨å¯èƒ½å‡ºç¾çš„å½™ç¸½æˆ–å‚™è¨»è¡Œ
        if not df.empty and df.iloc[-1].astype(str).str.contains('åˆè¨ˆ|ç¸½è¨ˆ|å‚™è¨»', na=False).any():
            df = df.iloc[:-1]
        return df
    except Exception as e:
        print(f"åœ¨è®€å–æˆ–æ¸…ç† CSV æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return None

# å…±ç”¨çš„è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼ç™¼é€ HTTP è«‹æ±‚ä¸¦æª¢æŸ¥ç‹€æ…‹ã€‚
def _fetch_twse_data(url: str) -> Optional[str]:
    """
    Args:
        url: å®Œæ•´çš„ TWSE è³‡æ–™ URLã€‚
    Returns:
        Optional[str]: æˆåŠŸç²å–å¾Œï¼Œä»¥ Big5 è§£ç¢¼çš„æ–‡å­—å…§å®¹ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status() 
        response.encoding = 'Big5'
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f"âŒ HTTP éŒ¯èª¤ï¼š{errh} (è©²æ—¥å¯èƒ½ç„¡äº¤æ˜“è³‡æ–™)")
    except requests.exceptions.RequestException as err:
        print(f"âŒ é€£ç·šæˆ– Requests éŒ¯èª¤: {err}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")

    return None

# å°‡æ‰€æœ‰å ±å‘Šçš„æŠ“å–çµæœæ‘˜è¦å¯«å…¥æ—¥èªŒæª”æ¡ˆï¼Œä¸¦åŒæ™‚åˆ—å°åˆ°æ§åˆ¶å°ã€‚
def log_summary_results(results: List[Tuple[str, Optional[pd.DataFrame]]], fetch_date: str, summary_filename_prefix: str = SUMMARY_LOG_FILENAME_PREFIX):
    BASE_DIR = pathlib.Path(os.path.dirname(os.path.abspath(__file__)))
    OUTPUT_DIR = BASE_DIR / "datas" / "logs"

    # ç¢ºä¿æ—¥èªŒè³‡æ–™å¤¾å­˜åœ¨
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    log_file_name = f"{summary_filename_prefix}_{fetch_date}.log"
    filename_new = OUTPUT_DIR / log_file_name 
    
    # å»ºç«‹æ‘˜è¦å…§å®¹å­—ä¸²
    summary_lines = []
    
    header = "\n" + "="*50 + "\n"
    header += f"--- {fetch_date} å ±å‘ŠæŠ“å–çµæœæ‘˜è¦ ---\n"
    header += "="*50
    summary_lines.append(header)
    
    success_count = 0
    fail_count = 0

    for name, df in results:
        if df is not None:
            line = f"\n[ğŸŸ¢ {name} (æˆåŠŸ)] æ•¸æ“šç­†æ•¸: {len(df)}"
            success_count += 1
        else:
            line = f"[ğŸ”´ {name} (å¤±æ•—)] ç„¡æ•¸æ“šæˆ–æŠ“å–éŒ¯èª¤ã€‚"
            fail_count += 1
        summary_lines.append(line)

    footer = "\n" + "="*50
    footer += f"\nç¸½çµï¼šæˆåŠŸ {success_count} å€‹å ±å‘Š, å¤±æ•— {fail_count} å€‹å ±å‘Šã€‚"
    footer += "\næ‰€æœ‰æˆåŠŸæŠ“å–çš„ CSV æª”æ¡ˆå·²å„²å­˜è‡³å°æ‡‰çš„ 'datas/raw' å­è³‡æ–™å¤¾ä¸­ã€‚"
    footer += "\n--- æ—¥èªŒè¨˜éŒ„çµæŸ ---\n"
    
    summary_lines.append(footer)
    
    log_content = "\n".join(summary_lines)

    # å¯«å…¥æ—¥èªŒæª”æ¡ˆ
    try:
        with open(filename_new, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        # åŒæ™‚åˆ—å°åˆ°æ§åˆ¶å°
        print(log_content)
        print(f"[æ—¥èªŒ] æˆåŠŸå°‡æ‘˜è¦çµæœå¯«å…¥æª”æ¡ˆï¼š{filename_new}")
    except Exception as e:
        print(f"âŒ å¯«å…¥æ‘˜è¦æ—¥èªŒæª”æ¡ˆç™¼ç”ŸéŒ¯èª¤: {e}")

# --- 10 å¤§ TWSE å ±å‘ŠæŠ“å–å‡½å¼ (åˆ†é èˆ‡å€‹è‚¡) ---
def fetch_twse_stock_day(target_date: str, stock_no: str) -> Optional[pd.DataFrame]:
    """
    (1/10) æŠ“å–æŒ‡å®šæ—¥æœŸå’Œè‚¡ç¥¨ä»£è™Ÿçš„ STOCK_DAY å ±å‘Š (æ¯æ—¥æˆäº¤è³‡è¨Š)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY"
    url = f"{base_url}?date={target_date}&stockNo={stock_no}&response=csv"

    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "1_STOCK_DAY")
    filename = OUTPUT_DIR + f"\\{target_date}_{stock_no}_STOCK_DAY.csv"

    check_folder_and_create(filename)

    print(f"å˜—è©¦æŠ“å– (1/10) {stock_no} STOCK_DAY è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    
    if response_text is None: return None
    
    df = _read_twse_csv(response_text, header_row=1)
    if df is not None and 'æ—¥æœŸ' in df.columns:
        df = df[df['æ—¥æœŸ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (1/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_mi_index(target_date: str) -> Optional[pd.DataFrame]:
    """
    (2/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ MI_INDEX å ±å‘Š (æ‰€æœ‰é¡è‚¡æˆäº¤çµ±è¨ˆ)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX"
    url = f"{base_url}?date={target_date}&type=ALLBUT0999&response=csv"
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "2_MI_INDEX")
    filename = OUTPUT_DIR + f"\\{target_date}_MI_INDEX_Sector.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (2/10) MI_INDEX (é¡è‚¡çµ±è¨ˆ) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # MI_INDEX å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'æŒ‡æ•¸' in df.columns:
        df = df[df['æŒ‡æ•¸'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (2/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_bwibbu_d(target_date: str) -> Optional[pd.DataFrame]:
    """
    (3/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ BWIBBU_d å ±å‘Š (ç™¼è¡Œé‡åŠ æ¬Šè‚¡åƒ¹æŒ‡æ•¸é¡è‚¡æ—¥æˆäº¤é‡å€¼åŠå ±é…¬ç‡)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d"
    url = f"{base_url}?date={target_date}&selectType=ALL&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "3_BWIBBU_d")
    filename = OUTPUT_DIR + f"\\{target_date}_BWIBBU_d_IndexReturn.csv"

    check_folder_and_create(filename)
        
    print(f"å˜—è©¦æŠ“å– (3/10) BWIBBU_d (é¡è‚¡å ±é…¬ç‡) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # BWIBBU_d å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (3/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_mi_index20(target_date: str) -> Optional[pd.DataFrame]:
    """
    (4/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ MI_INDEX20 å ±å‘Š (æ”¶ç›¤æŒ‡æ•¸åŠæˆäº¤é‡å€¼è³‡è¨Š)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX20
    
    **ä¿®æ­£:** è¡¨é ­ç´¢å¼•æ”¹ç‚º 2 (åŸç‚º 1)ã€‚
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX20"
    url = f"{base_url}?date={target_date}&response=csv"
        
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "4_MI_INDEX20")
    filename = OUTPUT_DIR + f"\\{target_date}_MI_INDEX20_Market.csv"
    
    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (4/10) MI_INDEX20 è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None
    
    # MI_INDEX20 å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 2
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (4/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_twtasu(target_date: str) -> Optional[pd.DataFrame]:
    """
    (5/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ TWTASU å ±å‘Š (æ¯æ—¥ç¸½æˆäº¤é‡å€¼èˆ‡å¹³å‡è‚¡åƒ¹)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/TWTASU
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/TWTASU"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "5_TWTASU")
    filename = OUTPUT_DIR + f"\\{target_date}_TWTASU_VolumePrice.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (5/10) TWTASU (ç¸½é‡å€¼) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # TWTASU å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=2)
   
    if df is not None and 'è­‰åˆ¸åç¨±' in df.columns: 
        df = df[df['è­‰åˆ¸åç¨±'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (5/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_bfiamu(target_date: str) -> Optional[pd.DataFrame]:
    """
    (6/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ BFIAMU å ±å‘Š (è‡ªç‡Ÿå•†è²·è³£è¶…å½™ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/BFIAMU
    
    **ä¿®æ­£:** è¡¨é ­ç´¢å¼•æ”¹ç‚º 3 (åŸç‚º 2)ã€‚
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/BFIAMU"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "6_BFIAMU")
    filename = OUTPUT_DIR + f"\\{target_date}_BFIAMU_DealerTrade.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (6/10) BFIAMU (è‡ªç‡Ÿå•†è²·è³£è¶…) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # BFIAMU å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'åˆ†é¡æŒ‡æ•¸åç¨±' in df.columns:
        df = df.dropna(how='all') 
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (6/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_fmtqik(target_date: str) -> Optional[pd.DataFrame]:
    """
    (7/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ FMTQIK å ±å‘Š (æ¯æ—¥åˆ¸å•†æˆäº¤é‡å€¼ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/FMTQIK
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/FMTQIK"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "7_FMTQIK")
    filename = OUTPUT_DIR + f"\\{target_date}_FMTQIK_BrokerVolume.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (7/10) FMTQIK (åˆ¸å•†æˆäº¤ç¸½è¡¨) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # FMTQIK å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 2
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'æ—¥æœŸ' in df.columns:
        df = df[df['æ—¥æœŸ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (7/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_bfi82u(target_date: str) -> Optional[pd.DataFrame]:
    """
    (8/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ BFI82U å ±å‘Š (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨ - æ—¥)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/BFI82U
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/fund/BFI82U"
    # åªä½¿ç”¨ dayDate é€²è¡Œæ—¥æœŸåƒæ•¸æ¨¡çµ„åŒ–
    url = f"{base_url}?type=day&dayDate={target_date}&response=csv"
        
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "8_BFI82U")
    filename = OUTPUT_DIR + f"\\{target_date}_BFI82U_3IParty_Day.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (8/10) BFI82U (ä¸‰å¤§æ³•äººæ—¥å ±) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # BFI82U å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'å–®ä½åç¨±' in df.columns:
        df = df[df['å–®ä½åç¨±'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (8/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_twt43u(target_date: str) -> Optional[pd.DataFrame]:
    """
    (9/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ TWT43U å ±å‘Š (å¤–è³‡åŠé™¸è³‡è²·è³£è¶…å½™ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/TWT43U
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/fund/TWT43U"
    url = f"{base_url}?date={target_date}&response=csv"

    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "9_TWT43U")
    filename = OUTPUT_DIR + f"\\{target_date}_TWT43U_ForeignTrade.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (9/10) TWT43U (å¤–è³‡è²·è³£è¶…) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # TWT43U å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=2)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (9/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_twt44u(target_date: str) -> Optional[pd.DataFrame]:
    """
    (10/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ TWT44U å ±å‘Š (æŠ•ä¿¡è²·è³£è¶…å½™ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/TWT44U
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/fund/TWT44U"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "10_TWT44U")
    filename = OUTPUT_DIR + f"\\{target_date}_TWT44U_InvestmentTrust.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (10/10) TWT44U (æŠ•ä¿¡è²·è³£è¶…) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (10/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_t86(target_date: str) -> Optional[pd.DataFrame]:
    """
    æŠ“å–æŒ‡å®šæ—¥æœŸçš„ T86 å ±å‘Š (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨ - ä¾é¡åˆ¥)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/T86
    
    :param target_date: æŸ¥è©¢æ—¥æœŸï¼Œæ ¼å¼ç‚º YYYYMMDD (ä¾‹å¦‚: 20251031)
    :return: åŒ…å«ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™çš„ DataFrameï¼Œå¦‚æœå¤±æ•—å‰‡ç‚º None
    """
    
    if not re.fullmatch(r'\d{8}', target_date): 
        print("æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYYMMDD æ ¼å¼ã€‚")
        return None
        
    # å®šç¾© URL çµæ§‹
    base_url = "https://www.twse.com.tw/rwd/zh/fund/T86"
    url = f"{base_url}?date={target_date}&selectType=ALL&response=csv"
    
    # å®šç¾©æª”æ¡ˆå„²å­˜è·¯å¾‘
    # å‡è¨­ "datas/raw/10_T86" æ˜¯ç›¸å°æ–¼æ­¤è…³æœ¬çš„ä½ç½®
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "11_T86")
    filename = os.path.join(OUTPUT_DIR, f"{target_date}_T86_InstitutionalTrades.csv")

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– T86 (ä¸‰å¤§æ³•äººè²·è³£è¶… - ä¾é¡åˆ¥) è³‡æ–™ï¼Œæ—¥æœŸ: {target_date}...")
    
    # 1. æŠ“å–è³‡æ–™
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # 2. è§£æ CSV (header_row=1 è¡¨ç¤ºæ¬„ä½åç¨±åœ¨ç¬¬äºŒè¡Œ)
    # T86 è¡¨æ ¼çš„æ¬„ä½åç¨±é€šå¸¸åœ¨å›å‚³çš„ CSV å…§å®¹çš„ç¬¬äºŒè¡Œ
    df = _read_twse_csv(response_text, header_row=1)

    # 3. æ•¸æ“šæ¸…ç†èˆ‡å„²å­˜
    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        # æ¸…é™¤æ²’æœ‰è­‰åˆ¸ä»£è™Ÿçš„ç©ºè¡Œ
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        
        # æ¸…ç†å¤šé¤˜çš„æè¿°è¡Œï¼ˆä¾‹å¦‚åº•éƒ¨çš„åˆè¨ˆè¡Œï¼Œå…¶è­‰åˆ¸ä»£è™Ÿæ¬„ä½å¯èƒ½ç‚ºç©ºï¼‰
        if 'æŠ•ä¿¡è²·è³£è¶…' in df.columns:
            # ç¢ºä¿æ•¸å­—æ¬„ä½å¯ä»¥è¢«è½‰æ›
            df['æŠ•ä¿¡è²·è³£è¶…'] = pd.to_numeric(df['æŠ•ä¿¡è²·è³£è¶…'], errors='coerce')
        
        # åˆªé™¤æ‰€æœ‰æ•¸å­—æ¬„ä½çš†ç‚º NaN çš„è¡Œ (å¯èƒ½æ˜¯åˆè¨ˆæˆ–ç„¡ç”¨è¨Šæ¯)
        df.dropna(subset=df.columns[2:], how='all', inplace=True)
        
        # å„²å­˜ CSV
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    
    print(f"âŒ æ•¸æ“šè™•ç†å¤±æ•—ï¼Œå¯èƒ½è©²æ—¥æœŸ ({target_date}) ç‚ºéäº¤æ˜“æ—¥æˆ–ç¶²ç«™è³‡æ–™çµæ§‹æ”¹è®Šã€‚")
    return None

# æª¢æŸ¥æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼Œè‹¥æ˜¯å‰‡å›å‚³Trueï¼Œå¦å‰‡å›å‚³"False"ï¼Œä¸‹ä¸€å€‹äº¤æ˜“æ—¥
def check_next_date_in_csv(file_path: str, date_to_check: str, date_column_name: str = 'æ—¥æœŸ') -> Union[bool, pd.Series]:
    """
    æª¢æŸ¥æŒ‡å®šæ—¥æœŸå­—ä¸²æ˜¯å¦å‡ºç¾åœ¨ CSV æª”æ¡ˆçš„ç‰¹å®šæ¬„ä½ä¸­ã€‚
    Args:
        file_path (str): holidays_all.csv æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        date_to_check (str): è¦æª¢æŸ¥çš„æ—¥æœŸå­—ä¸²ï¼Œä¾‹å¦‚ '2025/10/10'ã€‚
        date_column_name (str): æª”æ¡ˆä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æ—¥æœŸ'ã€‚
    Returns:
        Union[bool, pd.Series]: å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›åŒ…å«åŒ¹é…è¡Œçš„ Series (å¸ƒæ—å€¼)ï¼Œ
                                å¦‚æœæœªæ‰¾åˆ°æˆ–æª”æ¡ˆä¸å­˜åœ¨ï¼Œè¿”å› Falseã€‚è¿”å›ä¸‹å€‹äº¤æ˜“æ—¥çš„ Series (å¸ƒæ—å€¼)ï¼Œ
    """
    #print(f"ğŸ” æ­£åœ¨æª¢æŸ¥æª”æ¡ˆ: {os.path.basename(file_path)}")
    #print(f"ç›®æ¨™æ—¥æœŸ: {date_to_check}")

    if not os.path.exists(file_path):
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
        return False, None
        
    try:
        # ä¸”æ‚¨å„²å­˜æ™‚ä½¿ç”¨ encoding='utf-8-sig'ï¼Œé€™è£¡ä¹Ÿä½¿ç”¨ç›¸åŒçš„ç·¨ç¢¼è®€å–
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        if date_column_name not in df.columns:
            print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ—¥æœŸæ¬„ä½: '{date_column_name}'ã€‚")
            print(f"æª”æ¡ˆä¸­çš„æ¬„ä½æœ‰: {df.columns.tolist()}")
            return False, None
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œ (isin) æª¢æŸ¥æ¬„ä½ä¸­æ˜¯å¦åŒ…å«ç›®æ¨™æ—¥æœŸ
        # å³ä½¿æ¬„ä½é¡å‹æ˜¯ object (å­—ä¸²)ï¼Œä¹Ÿèƒ½æ­£ç¢ºæª¢æŸ¥
        
        date_format = '%Y/%m/%d'
        current_date = datetime.strptime(date_to_check, date_format)
        one_day = timedelta(days=1)
        date_to_check_save = date_to_check
        check_next_day = True
        while check_next_day:
            
            is_present = df[date_column_name].isin([date_to_check])
            #print("æ¸¬è©¦1:date_column_name:", date_to_check)
            
            if is_present.any():
                # æ‰¾åˆ°åŒ¹é…çš„è¡Œ
                matched_rows = df[is_present]
                print(f"âœ… æ—¥æœŸ '{date_to_check}' ç‚ºäº¤æ˜“æ—¥ï¼")
                #print("--- åŒ¹é…çš„è³‡æ–™åˆ— ---")
                #print(matched_rows)
                check_next_day = False
            
            else:
                print(f"âœ… æ—¥æœŸ '{date_to_check}' ä¼‘å¸‚æ—¥ï¼")
                tomorrow_date = current_date + one_day
                tomorrow_date_str = tomorrow_date.strftime(date_format)
                #print("æ¸¬è©¦2:date_column_name:", tomorrow_date_str)    
                date_to_check = tomorrow_date_str
                current_date = tomorrow_date
                check_next_day = True
                
        if date_to_check_save == current_date.strftime(date_format):
            print(f"ä»Šå¤©æ—¥æœŸ: {date_to_check_save} ç‚ºäº¤æ˜“æ—¥")
            return True, date_to_check_save
        else:
            print(f"ä»Šå¤©æ—¥æœŸ: {date_to_check_save} ç‚ºä¼‘å¸‚æ—¥")
            print(f"ä¸‹ä¸€å€‹äº¤æ˜“æ—¥: {current_date.strftime(date_format)}")
            return False, current_date.strftime(date_format)
        
    except pd.errors.EmptyDataError:
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚")
        return False, None
    except Exception as e:
        print(f"ã€éŒ¯èª¤ã€‘è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False, None

# æª¢æŸ¥æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼Œè‹¥æ˜¯å‰‡å›å‚³Trueï¼Œå¦å‰‡å›å‚³"False"ï¼Œä¸Šä¸€å€‹äº¤æ˜“æ—¥
def check_pre_date_in_csv(file_path: str, date_to_check: str, date_column_name: str = 'æ—¥æœŸ') -> Union[bool, pd.Series]:
    """
    æª¢æŸ¥æŒ‡å®šæ—¥æœŸå­—ä¸²æ˜¯å¦å‡ºç¾åœ¨ CSV æª”æ¡ˆçš„ç‰¹å®šæ¬„ä½ä¸­ã€‚
    Args:
        file_path (str): holidays_all.csv æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        date_to_check (str): è¦æª¢æŸ¥çš„æ—¥æœŸå­—ä¸²ï¼Œä¾‹å¦‚ '2025/10/10'ã€‚
        date_column_name (str): æª”æ¡ˆä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æ—¥æœŸ'ã€‚
    Returns:
        Union[bool, pd.Series]: å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›åŒ…å«åŒ¹é…è¡Œçš„ Series (å¸ƒæ—å€¼)ï¼Œ
                                å¦‚æœæœªæ‰¾åˆ°æˆ–æª”æ¡ˆä¸å­˜åœ¨ï¼Œè¿”å› Falseã€‚è¿”å›ä¸‹å€‹äº¤æ˜“æ—¥çš„ Series (å¸ƒæ—å€¼)ï¼Œ
    """
    #print(f"ğŸ” æ­£åœ¨æª¢æŸ¥æª”æ¡ˆ: {os.path.basename(file_path)}")
    #print(f"ç›®æ¨™æ—¥æœŸ: {date_to_check}")

    if not os.path.exists(file_path):
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
        return False, None
        
    try:
        # ä¸”æ‚¨å„²å­˜æ™‚ä½¿ç”¨ encoding='utf-8-sig'ï¼Œé€™è£¡ä¹Ÿä½¿ç”¨ç›¸åŒçš„ç·¨ç¢¼è®€å–
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        if date_column_name not in df.columns:
            print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ—¥æœŸæ¬„ä½: '{date_column_name}'ã€‚")
            print(f"æª”æ¡ˆä¸­çš„æ¬„ä½æœ‰: {df.columns.tolist()}")
            return False, None
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œ (isin) æª¢æŸ¥æ¬„ä½ä¸­æ˜¯å¦åŒ…å«ç›®æ¨™æ—¥æœŸ
        # å³ä½¿æ¬„ä½é¡å‹æ˜¯ object (å­—ä¸²)ï¼Œä¹Ÿèƒ½æ­£ç¢ºæª¢æŸ¥
        
        date_format = '%Y/%m/%d'
        current_date = datetime.strptime(date_to_check, date_format)
        one_day = timedelta(days=1)
        date_to_check_save = date_to_check
        check_next_day = True
        while check_next_day:
            
            is_present = df[date_column_name].isin([date_to_check])
            #print("æ¸¬è©¦1:date_column_name:", date_to_check)
            
            if is_present.any():
                # æ‰¾åˆ°åŒ¹é…çš„è¡Œ
                matched_rows = df[is_present]
                print(f"âœ… æ—¥æœŸ '{date_to_check}' ç‚ºäº¤æ˜“æ—¥ï¼")
                #print("--- åŒ¹é…çš„è³‡æ–™åˆ— ---")
                #print(matched_rows)
                check_next_day = False
            
            else:
                print(f"âœ… æ—¥æœŸ '{date_to_check}' ä¼‘å¸‚æ—¥ï¼")
                tomorrow_date = current_date - one_day
                tomorrow_date_str = tomorrow_date.strftime(date_format)
                #print("æ¸¬è©¦2:date_column_name:", tomorrow_date_str)    
                date_to_check = tomorrow_date_str
                current_date = tomorrow_date
                check_next_day = True
                
        if date_to_check_save == current_date.strftime(date_format):
            print(f"ä»Šå¤©æ—¥æœŸ: {date_to_check_save} ç‚ºäº¤æ˜“æ—¥")
            return True, date_to_check_save
        else:
            print(f"ä»Šå¤©æ—¥æœŸ: {date_to_check_save} ç‚ºä¼‘å¸‚æ—¥")
            print(f"ä¸Šä¸€å€‹äº¤æ˜“æ—¥: {current_date.strftime(date_format)}")
            return False, current_date.strftime(date_format)
        
    except pd.errors.EmptyDataError:
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚")
        return False, None
    except Exception as e:
        print(f"ã€éŒ¯èª¤ã€‘è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False, None



# è¨­å®šæ‚¨æƒ³è¦æŠ“å–çš„ç›®æ¨™æ—¥æœŸ (åªéœ€ä¿®æ”¹æ­¤è™•å³å¯æŠ“å–æ‰€æœ‰å ±å‘Šçš„è³‡æ–™)
def main_run():
    #----------------
    global running # å¼•ç”¨å…¨å±€è®Šæ•¸
    
    if not running:
        # å¦‚æœåœ¨ç­‰å¾…åŸ·è¡Œçš„æ’ç¨‹éšŠåˆ—ä¸­ï¼Œæª¢æŸ¥åˆ°ä¸é‹è¡Œï¼Œå‰‡ç›´æ¥è·³é
        print("\n[å®šæ™‚ä»»å‹™]: åµæ¸¬åˆ°é€€å‡ºä¿¡è™Ÿï¼Œè·³éæœ¬æ¬¡åŸ·è¡Œã€‚")
        return

    #--------------
    TARGET_DATE = date.today().strftime("%Y%m%d") 
    Yesterday_day = (date.today() - timedelta(days=1)).strftime("%Y%m%d")
    Now_time_hour = datetime.now().strftime("%H")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„ã€Œå¹¾é»é˜ã€
    Now_day_time = datetime.now().strftime("%Y-%m-%d %H:%M")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„æ—¥æœŸåŠæ™‚é–“ã€Œä¾‹å¦‚ 2025-11-12 11:12ã€
    Now_time_year = datetime.now().strftime("%Y")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„ã€Œå¹´ã€
    Trading_day_file_path = pathlib.Path(__file__).resolve().parent / "datas" / "processed" / "get_holidays" / f"trading_day_2021-{Now_time_year}.csv"
    DATE_TO_CHECK = date.today().strftime("%Y/%m/%d")  
    
    # è™•ç†è¦æŠ“å–å“ªä¸€å¤©çš„è³‡æ–™é‚è¼¯
    result_found_next = check_next_date_in_csv(Trading_day_file_path, DATE_TO_CHECK)
    if result_found_next[0]:  # å¦‚æœä»Šå¤©æ˜¯äº¤æ˜“æ—¥
        TARGET_DATE = DATE_TO_CHECK  # æŠ“å–ä»Šå¤©çš„è³‡æ–™  
        print(f"\n[æ™‚é–“æª¢æŸ¥]: ä»Šå¤©æ—¥æœŸ ({DATE_TO_CHECK}) ç‚ºäº¤æ˜“æ—¥ã€‚")    
   
        if Now_time_hour > '21':  # å‡è¨­åœ¨æ™šä¸Š9é»å¾Œï¼ŒæŠ“å–ç•¶å¤©çš„è³‡æ–™
            print(f"\n[æ™‚é–“æª¢æŸ¥]: ç¾åœ¨æ™‚é–“ç‚º {Now_day_time}ï¼ŒæŠ“å– ({TARGET_DATE})ç•¶å¤©è³‡æ–™ã€‚")
            print("\n" + "="*50)
            print("--- ç¨‹å¼é–‹å§‹åŸ·è¡Œï¼šTWSE å ±å‘Šè³‡æ–™æŠ“å– ---")
            print("="*50 + "\n")
        else:
            result_found_pre = check_pre_date_in_csv(Trading_day_file_path, DATE_TO_CHECK)
            TARGET_DATE = result_found_pre[1]  # æŠ“å–ä¸Šä¸€å€‹äº¤æ˜“æ—¥çš„è³‡æ–™
            print(f"\n[æ™‚é–“æª¢æŸ¥]: ç¾åœ¨æ™‚é–“ç‚º {Now_day_time}ï¼Œç•¶å¤©è³‡æ–™å°šæœªæ›´æ–°ï¼Œå°‡æä¾›å‰ä¸€å€‹äº¤æ˜“æ—¥ ({TARGET_DATE}) çš„è³‡æ–™ã€‚")
   
    else:  # å¦‚æœä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥
        result_found_pre = check_pre_date_in_csv(Trading_day_file_path, DATE_TO_CHECK)
        TARGET_DATE = result_found_pre[1]  # æŠ“å–ä¸Šä¸€å€‹äº¤æ˜“æ—¥çš„è³‡æ–™
        print(f"\n[æ™‚é–“æª¢æŸ¥]: ä»Šå¤©æ—¥æœŸ ({DATE_TO_CHECK}) ç‚ºä¼‘å¸‚æ—¥ï¼Œå°‡æä¾›å‰ä¸€å€‹äº¤æ˜“æ—¥ ({TARGET_DATE}) çš„è³‡æ–™ã€‚")
        
    # è¨­ç½®ä¸€å€‹åˆ—è¡¨ä¾†å„²å­˜çµæœ(æŠ“å–çš„ç¶²è·¯è³‡æ–™)ï¼Œä¾¿æ–¼æœ€çµ‚é è¦½
    results = []

    # è½‰æ›TARGET_DATEç‚ºYYYYMMDDæ ¼å¼
    TARGET_DATE = TARGET_DATE.replace("/", "")
    # 1. STOCK_DAY (å€‹è‚¡æ—¥æˆäº¤è³‡è¨Š)
    # æ”¹ä»¥å–®ç¨çš„ç¨‹å¼æŠ“å–è³‡æ–™
    #results.append(("STOCK_DAY", fetch_twse_stock_day(TARGET_DATE, TARGET_STOCK)))

    # 2. MI_INDEX (æ‰€æœ‰é¡è‚¡æˆäº¤çµ±è¨ˆ)
    results.append(("MI_INDEX", fetch_twse_mi_index(TARGET_DATE))) 

    # 3. BWIBBU_d (é¡è‚¡æ—¥æˆäº¤é‡å€¼åŠå ±é…¬ç‡)
    results.append(("BWIBBU_d", fetch_twse_bwibbu_d(TARGET_DATE))) 
    
    # 4. MI_INDEX20 (æ”¶ç›¤æŒ‡æ•¸åŠæˆäº¤é‡å€¼è³‡è¨Š)
    results.append(("MI_INDEX20", fetch_twse_mi_index20(TARGET_DATE)))

    # 5. TWTASU (æ¯æ—¥ç¸½æˆäº¤é‡å€¼èˆ‡å¹³å‡è‚¡åƒ¹)
    results.append(("TWTASU", fetch_twse_twtasu(TARGET_DATE))) 

    # 6. BFIAMU (è‡ªç‡Ÿå•†è²·è³£è¶…å½™ç¸½è¡¨)
    results.append(("BFIAMU", fetch_twse_bfiamu(TARGET_DATE))) 

    # 7. FMTQIK (æ¯æ—¥åˆ¸å•†æˆäº¤é‡å€¼ç¸½è¡¨)
    results.append(("FMTQIK", fetch_twse_fmtqik(TARGET_DATE)) )

    # 8. BFI82U (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨ - æ—¥)
    results.append(("BFI82U", fetch_twse_bfi82u(TARGET_DATE)))

    # 9. TWT43U (å¤–è³‡åŠé™¸è³‡è²·è³£è¶…å½™ç¸½è¡¨)
    results.append(("TWT43U", fetch_twse_twt43u(TARGET_DATE)))

    # 10. TWT44U (æŠ•ä¿¡è²·è³£è¶…å½™ç¸½è¡¨)
    results.append(("TWT44U", fetch_twse_twt44u(TARGET_DATE)))
    
    # --- è™•ç†ä¸¦copyæª”æ¡ˆåˆ°outputè³‡æ–™å¤¾ ---
    input_path = pathlib.Path(__file__).resolve().parent / "datas" / "raw" / "10_TWT44U" / f"{TARGET_DATE}_TWT44U_InvestmentTrust.csv"
    output_dir = pathlib.Path(__file__).resolve().parent / "datas" / "output"
    output_file = f"{TARGET_DATE}_TWT44U_SelectedColumns_Fixed.csv" # æ›´æ”¹æª”åä»¥é¿å…è¦†è“‹èˆŠæª”æ¡ˆ
    select_and_save_columns_fix_encoding(input_path, output_dir, output_file)

    # 11. T86 (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨)
    results.append(("T86", fetch_twse_t86(TARGET_DATE)))
    
    # 12. MI_MARGN (èè³‡èåˆ¸å½™ç¸½ (å…¨éƒ¨))
    #results.append(("TWT93U", fetch_twse_mi_margn(TARGET_DATE)))

    # --- æœ€çµ‚çµæœé è¦½ ---
    print("\n" + "="*50)
    print("--- 10 å€‹å ±å‘ŠæŠ“å–çµæœæ‘˜è¦ ---")
    print("="*50)

    
    for name, df in results:
        if df is not None:
            print(f"\n[ğŸŸ¢ {name} (æˆåŠŸ)] æ•¸æ“šç­†æ•¸: {len(df)}")
            # print(df.head().to_markdown(index=False)) # è¨»é‡‹æ‰é¿å…è¼¸å‡ºéå¤š
        else:
            print(f"[ğŸ”´ {name} (å¤±æ•—)] ç„¡æ•¸æ“šæˆ–æŠ“å–éŒ¯èª¤ã€‚")

    time_module.sleep(5) 

    # å¢åŠ æ—¥èªŒå„²å­˜ï¼šè¨˜éŒ„æœ¬æ¬¡å˜—è©¦æŠ“å–çš„æ—¥æœŸ
    log_summary_results(results, TARGET_DATE)

    print("\næ‰€æœ‰ CSV æª”æ¡ˆå·²å„²å­˜è‡³ç¨‹å¼åŸ·è¡Œç›®éŒ„ä¸‹ã€‚")
    print("--- ç¨‹å¼åŸ·è¡ŒçµæŸ ---")
    
    # å–å¾—åº«å­˜è‚¡ç¥¨æ¸…å–®åŠè¿‘5æ—¥æ”¶ç›¤åƒ¹
    # ==========================================================
    # --- åƒæ•¸è¨­å®š ---
    # ==========================================================

    BASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)))
    EXCEL_PATH = BASE_DIR + "/datas/è‚¡ç¥¨åˆ†æ.xlsx"
    SHEET_NAME = "è‚¡ç¥¨åº«å­˜çµ±è¨ˆ"
    FILTER_COLUMN = "ç›®å‰è‚¡æ•¸åº«å­˜çµ±è¨ˆ"
    FILTER_VALUE = "0"
    OUTPUT_DIRECTORY = None 

    # ä¾†æºæª”æ¡ˆè·¯å¾‘
    SOURCE_FILE = r"Y:\æ”¶æ”¯è¨˜éŒ„\è‚¡ç¥¨åˆ†æ\è‚¡ç¥¨åˆ†æ.xlsx"
    # ç›®æ¨™ç›®éŒ„è·¯å¾‘
    DESTINATION_DIR = EXCEL_PATH # ä¹Ÿå°±æ˜¯ D æ§½çš„æ ¹ç›®éŒ„

    # --- ä¸»è¦åŸ·è¡Œå€å¡Š ---

    # åŸ·è¡Œè¤‡è£½åŠŸèƒ½
    copy_file_to_directory(SOURCE_FILE, DESTINATION_DIR)

    try:
        final_csv_path = extract_excel_sheet_filter_and_save(
            excel_file_path=EXCEL_PATH,
            sheet_name=SHEET_NAME,
            filter_column=FILTER_COLUMN,
            filter_value=FILTER_VALUE,
            output_dir=OUTPUT_DIRECTORY
        )
        
        if final_csv_path:
            print("\n" + "="*50)
            print("ğŸ‰ ä»»å‹™æˆåŠŸå®Œæˆï¼")
            print(f"CSV æª”æ¡ˆå·²å„²å­˜è‡³ï¼š\n {final_csv_path}")
            print("="*50)

    except (FileNotFoundError, ValueError, Exception) as e:
        print("\n" + "="*50)
        print("âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼")
        print(e)
        print("="*50)

    #-- å–å¾—è­‰åˆ¸åç¨±æ¸…å–® ---
    print("\n--- å–å¾—è­‰åˆ¸åç¨±æ¸…å–® ---")    
    df = pd.read_csv(final_csv_path, encoding='utf-8', skipinitialspace=True)
    df.columns = df.columns.str.strip()

    #print(df["è­‰åˆ¸åç¨±"])
    TARGET_STOCK_NAMES = []
    for col in df["è­‰åˆ¸åç¨±"]:
        TARGET_STOCK_NAMES.append(col)

    focused_sheet_name = "é—œæ³¨çš„è‚¡ç¥¨"
    focused_column_name = "è­‰åˆ¸åç¨±"
    focused_stock_names = get_stock_names_from_excel(DESTINATION_DIR, focused_sheet_name, focused_column_name)

    #-- å–å¾—å¾€å‰6å€‹äº¤æ˜“æ—¥ ---
    N_DAYS = 6 # å¾€å‰æ‰¾çš„äº¤æ˜“æ—¥æ•¸é‡

    recent_trading_days_df = find_last_n_trading_days_with_time_check(Trading_day_file_path, n=N_DAYS)
    #recent_trading_days_df.sort_values(by="æ—¥æœŸ", ascending=False, inplace=True)
    Send_message_ALL = ""
    for TARGET_STOCK_NAME in TARGET_STOCK_NAMES:
    #    print(f"\n--- {TARGET_STOCK_NAME} æœ€è¿‘ 5 å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ ---")
        Send_message = ""
        #-- å–å¾—äº”å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ä¸¦åˆä½µ ---
        CSV_NAME_COLUMN = "è­‰åˆ¸åç¨±" # å‡è¨­ CSV ä¸­ç”¨æ–¼åç¨±æ¯”å°çš„æ¬„ä½
        CSV_PRICE_COLUMN = "æ”¶ç›¤åƒ¹"  # å‡è¨­ CSV ä¸­æ”¶ç›¤åƒ¹çš„æ¬„ä½

        day_roll = []
        for row in recent_trading_days_df["æ—¥æœŸ"]:
            TARGET_DATE = row.replace("/", "")
            day_roll.append(TARGET_DATE)

        BASE_DIR = pathlib.Path(os.path.dirname(os.path.abspath(__file__)))

        if recent_trading_days_df is not None:
            print(f"\n--{TARGET_STOCK_NAME}æœ€è¿‘5å€‹äº¤æ˜“æ—¥--")

        CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll[0:1][0]}_BWIBBU_d_IndexReturn.csv"
        get_price_before = lookup_stock_price(
                file_path=CSV_PATH,
                stock_name=TARGET_STOCK_NAME,
                name_col=CSV_NAME_COLUMN,
                price_col=CSV_PRICE_COLUMN
            )
        print("å‰5äº¤æ˜“æ—¥æ”¶ç›¤åƒ¹:", get_price_before)
        
        total_price_percent = 0
        for day_roll1 in day_roll[1:]:
            CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll1}_BWIBBU_d_IndexReturn.csv"

            # --- è®€å–è²·è³£è¶…è³‡æ–™ä¸¦ç™¼é€é€šçŸ¥ ---

            file_path = BASE_DIR / "datas" / "raw" / "11_T86" / f"{day_roll1}_T86_InstitutionalTrades.csv"
            stock_name = TARGET_STOCK_NAME # ç›®æ¨™è­‰åˆ¸åç¨±

            # å‘¼å«å‡½å¼
            net_volume_data = get_stock_net_volume(file_path, stock_name)

            if net_volume_data is not None and not net_volume_data.empty:
                try:
                    # 1. è½‰æ›ç‚ºæ•¸å€¼ (float)ï¼Œä¸¦é™¤ä»¥ 1000 æ›ç®—æˆã€Œå¼µã€
                    net_volume_in_lots = net_volume_data.astype(float) / 1000
                    
                    # 2. (å¯é¸) å°çµæœé€²è¡Œå››æ¨äº”å…¥æˆ–å–æ•´æ•¸
                    # é€™è£¡ä½¿ç”¨ round() ä¿æŒä¸€å®šç²¾ç¢ºåº¦ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€æ±‚æ”¹ç‚º .astype(int)
                    rounded_lots = net_volume_in_lots.round(0).astype(int) 
                    
                    # 3. å°‡ Series è½‰æ›ç‚ºå­—ä¸² (ä¸å«ç´¢å¼•ï¼Œä¸”ä¸å«æ¨™é¡Œ)
                    # ä½¿ç”¨ to_string(index=False, header=False) å–å¾—ç´”æ•¸æ“šå­—ä¸²
                    output_string = rounded_lots.to_string(index=False, header=False).strip()

                except ValueError as e:
                    print(f"âŒ éŒ¯èª¤ï¼šæ•¸æ“šä¸­åŒ…å«ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼çš„è³‡æ–™ï¼Œç„¡æ³•æ›ç®—æˆã€Œå¼µã€ã€‚")
                    # print(f"  è©³ç´°éŒ¯èª¤ï¼š{e}") # æ–¹ä¾¿é™¤éŒ¯
                  
                     
            else:
                print(f"æ‰¾ä¸åˆ° {stock_name} çš„è²·è³£è¶…è‚¡æ•¸è³‡æ–™æˆ–è³‡æ–™ç‚ºç©ºã€‚")
                net_volume_data = "0"
            net_volume_data = net_volume_data.tolist()[0][:-4] + "å¼µ"
            
            get_price = lookup_stock_price(
                file_path=CSV_PATH,
                stock_name=TARGET_STOCK_NAME,
                name_col=CSV_NAME_COLUMN,
                price_col=CSV_PRICE_COLUMN
            )
            day_mmdd = f"{day_roll1[4:6]}/{day_roll1[-2:]}"
            price_percent = (float(get_price) - float(get_price_before)) / float(get_price_before) * 100
            price_percent = round(float(price_percent), 1)
            
            total_price_percent += int(price_percent)
            
            if price_percent > 0:
                price_percent = f"ğŸ”´{abs(price_percent)}"
            else:
                price_percent = f"ğŸŸ¢{abs(price_percent)}"
            
            Send_message += f"{day_mmdd}:{get_price}{price_percent}%({net_volume_data})\n"
            get_price_before = get_price
            
            # å‘¼å«å‡½å¼
            stock_indicators_df = get_stock_indicators(CSV_PATH, stock_name)

            pa_ratio = stock_indicators_df.iloc[0]['æ®–åˆ©ç‡(%)']
            pe_ratio = stock_indicators_df.iloc[0]['æœ¬ç›Šæ¯”']
            pb_ratio = stock_indicators_df.iloc[0]['è‚¡åƒ¹æ·¨å€¼æ¯”']
            
           # message_add = f"\n--ğŸ¯ã€{stock_name}ã€‘å€‹è‚¡è³‡è¨Š ğŸ¯--" + f"\n         æœ¬ç›Šæ¯”  : {pe_ratio}%" + f"\n     è‚¡åƒ¹æ·¨å€¼æ¯”: {pb_ratio}" + f"\n         æ®–åˆ©ç‡  : {pa_ratio}\n\n"
            message_add = f"\n--ğŸ¯ã€{stock_name}ã€‘å€‹è‚¡è³‡è¨Š ğŸ¯--\n  æœ¬ç›Šæ¯”  : {pe_ratio}%\nè‚¡åƒ¹æ·¨å€¼æ¯”: {pb_ratio}\n  æ®–åˆ©ç‡  : {pa_ratio}\n\n"
            
        if total_price_percent > 0:
            total_price_percent = f"ğŸ”´ {abs(total_price_percent)}%"
        else:
            total_price_percent = f"ğŸŸ¢ {abs(total_price_percent)}%"
            
     # å‘¼å«å‡½å¼
        top_20_positive_df = get_top_20_institutional_trades_filtered(file_path)
        
        #print(top_20_positive_df)
    
        #sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ
        # Send_message_ALL += f"\n-{TARGET_STOCK_NAME} æœ€è¿‘5æ—¥æ”¶ç›¤åƒ¹-\n{Send_message}\n--ä¸‰å¤§æ³•äººè²·è¶…å‰20å--\n{top_20_positive_df}"
        Send_message_ALL += f"ç™¼é€æ™‚é–“: {Now_day_time}\n"
        Send_message_ALL += f"***************************\n"
        Send_message_ALL += f"ğŸ“¦ {TARGET_DATE} (åº«å­˜è‚¡)é€šçŸ¥ğŸ“¦\n"
        Send_message_ALL += f"***************************\n"
        Send_message_ALL += f"\n=ğŸ¥‡{TARGET_STOCK_NAME} æœ€è¿‘5æ—¥æ”¶ç›¤åƒ¹ğŸ¥‡ =\n{Send_message}"
        Send_message_ALL += f"== è¿‘5æ—¥ç¸¾æ•ˆ:{total_price_percent} ==\n"
        Send_message_ALL += message_add  # åŠ å…¥å€‹è‚¡è³‡è¨Š
                
    # é‡å°é—œæ³¨çš„è‚¡ç¥¨ï¼Œå–å¾—è¿‘5æ—¥æ”¶ç›¤åƒ¹
    #Send_focused_message_all = ""
    Send_message_ALL += f"*****************************\n"
    Send_message_ALL += f"ğŸ’¡ğŸ’¡ {TARGET_DATE} é—œæ³¨è‚¡è³‡è¨ŠğŸ’¡ğŸ’¡\n"
    Send_message_ALL += f"*****************************\n"
    for focused_stock_name in focused_stock_names:
    #    print(f"\n--- {focused_stock_names} æœ€è¿‘ 5 å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ ---")
        Send_focused_message = ""
        #-- å–å¾—äº”å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ä¸¦åˆä½µ ---
        CSV_NAME_COLUMN = "è­‰åˆ¸åç¨±" # å‡è¨­ CSV ä¸­ç”¨æ–¼åç¨±æ¯”å°çš„æ¬„ä½
        CSV_PRICE_COLUMN = "æ”¶ç›¤åƒ¹"  # å‡è¨­ CSV ä¸­æ”¶ç›¤åƒ¹çš„æ¬„ä½

        day_roll = []
        for row in recent_trading_days_df["æ—¥æœŸ"]:
            TARGET_DATE = row.replace("/", "")
            day_roll.append(TARGET_DATE)

        BASE_DIR = pathlib.Path(os.path.dirname(os.path.abspath(__file__)))

        if recent_trading_days_df is not None:
            print(f"\n--{TARGET_STOCK_NAME}æœ€è¿‘5å€‹äº¤æ˜“æ—¥--")

        CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll[0:1][0]}_BWIBBU_d_IndexReturn.csv"
        get_price_before = lookup_stock_price(
                file_path=CSV_PATH,
                stock_name=focused_stock_name,
                name_col=CSV_NAME_COLUMN,
                price_col=CSV_PRICE_COLUMN
            )
        print("å‰5äº¤æ˜“æ—¥æ”¶ç›¤åƒ¹:", get_price_before)
        
        total_price_percent = 0
        for day_roll1 in day_roll[1:]:
            CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll1}_BWIBBU_d_IndexReturn.csv"

            # --- è®€å–è²·è³£è¶…è³‡æ–™ä¸¦ç™¼é€é€šçŸ¥ ---

            file_path = BASE_DIR / "datas" / "raw" / "11_T86" / f"{day_roll1}_T86_InstitutionalTrades.csv"
            stock_name = focused_stock_name # ç›®æ¨™è­‰åˆ¸åç¨±

            # å‘¼å«å‡½å¼
            net_volume_data = get_stock_net_volume(file_path, stock_name)

            if net_volume_data is not None and not net_volume_data.empty:
                try:
                    # 1. è½‰æ›ç‚ºæ•¸å€¼ (float)ï¼Œä¸¦é™¤ä»¥ 1000 æ›ç®—æˆã€Œå¼µã€
                    net_volume_in_lots = net_volume_data.astype(float) / 1000
                    
                    # 2. (å¯é¸) å°çµæœé€²è¡Œå››æ¨äº”å…¥æˆ–å–æ•´æ•¸
                    # é€™è£¡ä½¿ç”¨ round() ä¿æŒä¸€å®šç²¾ç¢ºåº¦ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€æ±‚æ”¹ç‚º .astype(int)
                    rounded_lots = net_volume_in_lots.round(0).astype(int) 
                    
                    # 3. å°‡ Series è½‰æ›ç‚ºå­—ä¸² (ä¸å«ç´¢å¼•ï¼Œä¸”ä¸å«æ¨™é¡Œ)
                    # ä½¿ç”¨ to_string(index=False, header=False) å–å¾—ç´”æ•¸æ“šå­—ä¸²
                    output_string = rounded_lots.to_string(index=False, header=False).strip()

                except ValueError as e:
                    print(f"âŒ éŒ¯èª¤ï¼šæ•¸æ“šä¸­åŒ…å«ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼çš„è³‡æ–™ï¼Œç„¡æ³•æ›ç®—æˆã€Œå¼µã€ã€‚")
                    # print(f"  è©³ç´°éŒ¯èª¤ï¼š{e}") # æ–¹ä¾¿é™¤éŒ¯
                  
                     
            else:
                print(f"æ‰¾ä¸åˆ° {stock_name} çš„è²·è³£è¶…è‚¡æ•¸è³‡æ–™æˆ–è³‡æ–™ç‚ºç©ºã€‚")
                net_volume_data = "0"
            net_volume_data = net_volume_data.tolist()[0][:-4] + "å¼µ"
            
            get_price = lookup_stock_price(
                file_path=CSV_PATH,
                stock_name=focused_stock_name,
                name_col=CSV_NAME_COLUMN,
                price_col=CSV_PRICE_COLUMN
            )
            day_mmdd = f"{day_roll1[4:6]}/{day_roll1[-2:]}"
            price_percent = (float(get_price) - float(get_price_before)) / float(get_price_before) * 100
            price_percent = round(float(price_percent), 1)
            
            total_price_percent += int(price_percent)
            
            if price_percent > 0:
                price_percent = f"ğŸ”´{abs(price_percent)}"
            else:
                price_percent = f"ğŸŸ¢{abs(price_percent)}"
            
            Send_focused_message += f"{day_mmdd}:{get_price}{price_percent}%({net_volume_data})\n"
            get_price_before = get_price
            
        if total_price_percent > 0:
            total_price_percent = f"ğŸ”´ {abs(total_price_percent)}%"
        else:
            total_price_percent = f"ğŸŸ¢ {abs(total_price_percent)}%"

        Send_message_ALL += f"\n=âš ï¸  {focused_stock_name} æœ€è¿‘5æ—¥æ”¶ç›¤åƒ¹âš ï¸  =\n{Send_focused_message}"
        Send_message_ALL += f"== è¿‘5æ—¥ç¸¾æ•ˆ:{total_price_percent} ==\n"
        
    # å°‡ä¸‰å¤§æ³•äººè²·è¶…è³‡è¨ŠåŠ å…¥
    
    Send_message_ALL += f"\n\n*******************************\n"
    Send_message_ALL += f"ğŸš€{TARGET_DATE}ä¸‰å¤§æ³•äººè²·è¶…å‰20åğŸš€\n"
    Send_message_ALL += f"*******************************\n"
    Send_message_ALL += top_20_positive_df    
    print(Send_message_ALL)
    
    # ---- line notify ç™¼é€è¨Šæ¯ ----1
    # â‹ è¼‰å…¥ line_API.env æª”æ¡ˆä¸­çš„è®Šæ•¸
    # æ³¨æ„ï¼šå¦‚æœæ‚¨ä½¿ç”¨ .env ä»¥å¤–çš„æª”å (å¦‚ line_token.env)ï¼Œéœ€è¦æŒ‡å®šæª”å

    LINE_API_ENV_PATH = BASE_DIR / "line_API.env"
    load_dotenv(LINE_API_ENV_PATH)

    # âŒ å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– Token å’Œ User ID
    LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
    LINE_USER_ID = os.getenv("LINE_USER_ID")


    # ä¿®æ­£ LineBotApiError çš„åŒ¯å…¥è·¯å¾‘ï¼ˆæ ¹æ“šæ‚¨ä¸Šä¸€å€‹å•é¡Œçš„è§£ç­”ï¼‰
    from linebot.v3.messaging import Configuration, ApiClient, MessagingApi
    from linebot.v3.messaging import TextMessage, PushMessageRequest


    # ----------------- æª¢æŸ¥ Token æ˜¯å¦å­˜åœ¨ -----------------
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("éŒ¯èª¤ï¼šLINE_CHANNEL_ACCESS_TOKEN æœªåœ¨ line_API.env ä¸­è¨­ç½®æˆ–è®€å–å¤±æ•—ã€‚ç¨‹å¼ä¸­æ­¢ã€‚")
        exit()
    # ----------------------------------------------------

    try:
        # åˆå§‹åŒ– Configuration å’Œ MessagingApi
        configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
        api_client = ApiClient(configuration)
        messaging_api = MessagingApi(api_client)
        print("Line Bot API åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"Line Bot API åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Tokenï¼š{e}")

    # ... æ¥ä¸‹ä¾†çš„ç¨‹å¼ç¢¼ä¿æŒä¸è®Š ...
    # é€™æ˜¯æ¥æ”¶è¨Šæ¯çš„ç”¨æˆ¶ ID æˆ–ç¾¤çµ„ ID
    # LINE_USER_ID ç¾åœ¨å·²ç¶“å¾ .env æª”æ¡ˆä¸­è®€å–

    def send_stock_notification(user_id, message_text):
        try:
            push_message_request = PushMessageRequest(
                to=user_id,
                messages=[TextMessage(text=message_text)]
            )
            # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨å…¨åŸŸè®Šæ•¸ messaging_apiï¼Œå¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œé€™è£¡æœƒå ±éŒ¯
            messaging_api.push_message(push_message_request) 
            print(f"è¨Šæ¯å·²æˆåŠŸç™¼é€çµ¦ {user_id}")
        except Exception as e:
            print(f"å…¶ä»–éŒ¯èª¤: {e}")


    # ç™¼é€å…¨éƒ¨è³‡è¨Š(åº«å­˜è‚¡é€šçŸ¥ã€é—œæ³¨è‚¡é€šçŸ¥ã€ä¸‰å¤§æ³•äººè²·è¶…å‰20)
    analysis_report = Send_message_ALL
    #send_stock_notification(LINE_USER_ID, analysis_report)
# ===========================================================

# 1. åˆå§‹åŒ–é‹è¡Œç‹€æ…‹
running = True

# å…ˆé‹è¡Œ schedule.clear() å°‡æ’ç¨‹æ¸…é™¤ï¼Œé¿å…ç¿’æ…£ä½¿ç”¨ jupyter notebook æ•´åˆé–‹ç™¼ç’°å¢ƒçš„è®€è€…ï¼Œ
# æœ‰æ®˜å­˜çš„æ’ç¨‹ï¼Œé€ æˆé‹è¡Œçµæœä¸å¦‚é æœŸ
schedule.clear()

# æŒ‡å®šæ¯ 15 ç§’é‹è¡Œä¸€æ¬¡ say_hi å‡½æ•¸
# schedule.every(200).seconds.do(main_run)
# print("âœ… å·²è¨­å®šå®šæ™‚ä»»å‹™ï¼šæ¯1ç§’åŸ·è¡Œ main_runã€‚")
#æ¯å°æ™‚é‹è¡Œä¸€æ¬¡
# schedule.every(1).hour.do(main_run)
# print("âœ… å·²è¨­å®šå®šæ™‚ä»»å‹™ï¼šæ¯å°æ™‚åŸ·è¡Œ main_runã€‚")

# æ¯å¤© 15:30 é‹è¡Œä¸€æ¬¡ get_price å‡½æ•¸
schedule.every().day.at('21:00').do(main_run)
print("âœ… å·²è¨­å®šå®šæ™‚ä»»å‹™ï¼š21:00 åŸ·è¡Œ main_runã€‚")

# 3. è¨­å®šéµç›¤ç†±éµ (éé˜»å¡å¼ç›£è½)
keyboard.add_hotkey('1', main_run)
keyboard.add_hotkey('q', stop_program)
print("âœ… å·²è¨­å®šéµç›¤ç†±éµï¼š[1] åŸ·è¡Œmain_run, [Q] åœæ­¢ç¨‹å¼ã€‚")

print("\n--- ç¨‹å¼é–‹å§‹é‹è¡Œ ---")
print("ä¸»ç¨‹å¼å’Œæ’ç¨‹ç›£è½ä¸­...")

# --- ä¸»å¾ªç’° (Main Loop) ---
try:
    while running:
        # 1. æª¢æŸ¥æ˜¯å¦æœ‰æ’ç¨‹ä»»å‹™éœ€è¦é‹è¡Œ
        schedule.run_pending()
        
        # 2. è®“ä¸»å¾ªç’°çŸ­æš«ä¼‘çœ ï¼ŒåŒæ™‚è®“ CPU è³‡æºé‡‹æ”¾çµ¦å…¶ä»–è¡Œç¨‹ (åŒ…æ‹¬éµç›¤ç›£è½)
        # é€™è£¡è¨­å®šä¸€å€‹è¼ƒçŸ­çš„ä¼‘çœ æ™‚é–“ï¼Œç¢ºä¿å°æ’ç¨‹å’Œéµç›¤è¼¸å…¥çš„éŸ¿æ‡‰æ›´å³æ™‚ã€‚
        time_module.sleep(1)
        
except KeyboardInterrupt:
    # å…è¨±ä½¿ç”¨ Ctrl+C é€€å‡º
    print("\nç¨‹å¼è¢« Ctrl+C ä¸­æ–·é€€å‡ºã€‚")

finally:
# 3. ç§»é™¤æ‰€æœ‰è¨»å†Šçš„ç†±éµ (æ¸…ç†ç’°å¢ƒ)
    keyboard.unhook_all()
    print("æ‰€æœ‰éµç›¤ç›£è½å·²é—œé–‰ã€‚")
    print("ç¨‹å¼å®‰å…¨é€€å‡ºã€‚")