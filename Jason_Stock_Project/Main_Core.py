#æ¨™æº–å‡½å¼åº«
import os
import sys
import re
import shutil
from io import StringIO
import pathlib     # as pathlib
from datetime import date, datetime, timedelta, time as time_TimeClass

#ç¬¬ä¸‰æ–¹å‡½å¼åº«
import numpy as np # ç”¨æ–¼æ•¸å€¼æ“ä½œ
import pandas as pd # ç”¨æ–¼è³‡æ–™è™•ç†èˆ‡åˆ†æ
import requests
import schedule
import keyboard  # ç”¨æ–¼ç›£è½éµç›¤äº‹ä»¶
from dotenv import load_dotenv # âŠ åŒ¯å…¥å‡½å¼åº«
from typing import Optional, Tuple, List, Union, Dict, Any
import time as time_module # ç”¨æ–¼ sleep() æˆ– time()

#æœ¬åœ°æ¨¡çµ„
import get_stocks_company_all 
from utils import jason_utils as jutils

# æŠ‘åˆ¶ç•¶ verify=False æ™‚å½ˆå‡ºçš„ InsecureRequestWarning è­¦å‘Š
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# ==========================================================
# åƒæ•¸è¨­å®š  --- é…ç½® (Configuration) ---
# ==========================================================
SUMMARY_LOG_FILENAME_PREFIX = "fetch_summary" # å®šç¾©æ‘˜è¦æ—¥èªŒæª”æ¡ˆå‰ç¶´

# è¨­å®šéµç›¤ç›£æ§ -- 1. åˆå§‹åŒ–é‹è¡Œç‹€æ…‹ (ç¢ºä¿æ˜¯å…¨å±€è®Šæ•¸)
running = True

# """æŒ‰éµ [Q] åœæ­¢ç¨‹å¼"""
def stop_program():
    print("\n\nğŸ‘‹ åµæ¸¬åˆ° 'Q' éµï¼Œç¨‹å¼å³å°‡å®‰å…¨é€€å‡º...")
    global running
    running = False
    
   
# è®€å–é—œæ³¨çš„è‚¡ç¥¨
def get_stock_names_from_excel(file_path: str, sheet_name: str, column_name: str) -> Optional[pd.Series]:
    """
    è®€å– Excel æª”æ¡ˆä¸­æŒ‡å®šå·¥ä½œè¡¨çš„æŒ‡å®šæ¬„ä½æ•¸æ“šã€‚
    Args:
        file_path (str): Excel æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        sheet_name (str): å·¥ä½œè¡¨çš„æ¨™ç±¤åç¨± (e.g., 'ã€é—œæ³¨çš„è‚¡ç¥¨ã€‘')ã€‚
        column_name (str): è¦æŠ“å–çš„æ¬„ä½åç¨± (e.g., 'è­‰åˆ¸åç¨±')ã€‚

    Returns:
        pd.Series or None: åŒ…å«è­‰åˆ¸åç¨±çš„ Seriesï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨å˜—è©¦è®€å– Excel æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ é–å®šå·¥ä½œè¡¨ï¼šã€{sheet_name}ã€‘")

    try:
        # è®€å– Excel æª”æ¡ˆä¸­æŒ‡å®šçš„å·¥ä½œè¡¨
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        
        # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
        if column_name in df.columns:
            # æŠ“å–ä¸¦è¿”å› 'è­‰åˆ¸åç¨±' æ¬„ä½çš„è³‡æ–™
            stock_names = df[column_name]
            
            print(f"âœ… æˆåŠŸæŠ“å–å·¥ä½œè¡¨ '{sheet_name}' ä¸­ '{column_name}' æ¬„ä½çš„æ•¸æ“šã€‚")
            
            # è¼¸å‡ºåˆ—è¡¨å…§å®¹
            print("-" * 50)
            print("ã€è­‰åˆ¸åç¨±ã€‘åˆ—è¡¨ï¼š")
            print(stock_names.to_string(index=False)) # è¼¸å‡ºä¹¾æ·¨çš„åˆ—è¡¨
            print("-" * 50)
            
            return stock_names
        else:
            print(f"âŒ éŒ¯èª¤ï¼šå·¥ä½œè¡¨ '{sheet_name}' ä¸­æ‰¾ä¸åˆ°æ¬„ä½ '{column_name}'ã€‚")
            print(f"å¯¦éš›æ¬„ä½åç¨±ï¼š{list(df.columns)}")
            return None

    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„ Excel æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except ValueError as e:
        if "Worksheet named" in str(e):
            print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åç‚º '{sheet_name}' çš„å·¥ä½œè¡¨ã€‚è«‹æª¢æŸ¥æ¨™ç±¤åç¨±æ˜¯å¦æ­£ç¢ºã€‚")
        else:
            print(f"âŒ è®€å– Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")
        return None
    
# è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„æ•¸æ“šï¼Œä¸¦è¿”å›å…¶æŒ‡æ¨™è³‡æ–™ã€‚
def get_stock_indicators(file_path: str, target_name: str) -> Optional[pd.DataFrame]:
    """
    è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„æ•¸æ“šï¼Œä¸¦è¿”å›å…¶æŒ‡æ¨™è³‡æ–™ã€‚
    
    Args:
        file_path (str): CSV æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        target_name (str): è¦ç¯©é¸çš„è­‰åˆ¸åç¨± (e.g., 'å°ç»')ã€‚
        
    Returns:
        pd.DataFrame or None: åŒ…å«ç›®æ¨™è­‰åˆ¸æŒ‡æ¨™æ•¸æ“šçš„ DataFrameï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ æœå°‹ç›®æ¨™è­‰åˆ¸ï¼šã€{target_name}ã€‘çš„æŒ‡æ¨™æ•¸æ“š")

    # 1. è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except:
                df = pd.read_csv(file_path, encoding='big5')
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤æˆ–ç·¨ç¢¼å•é¡Œï¼š{e}")
        return None

    # 2. æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
    # æ ¹æ“š TWSE/BWIBBU æª”æ¡ˆå¸¸è¦‹çµæ§‹ï¼Œæ¬„ä½åç¨±å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œé€™è£¡æ²¿ç”¨ä¸Šä¸€æ¬¡çš„æ¬„ä½åç¨±ï¼Œ
    # ä½†è«‹æ ¹æ“šå¯¦éš›æª”æ¡ˆæƒ…æ³èª¿æ•´ï¼Œä¾‹å¦‚ï¼š'æœ¬ç›Šæ¯”' å¯èƒ½ç‚º 'PE Ratio'
    required_cols = ['è­‰åˆ¸åç¨±', 'æ®–åˆ©ç‡(%)', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']
    
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{missing_cols}ã€‚")
        print(f"æª”æ¡ˆå¯¦éš›æ¬„ä½åç¨±ï¼š{list(df.columns)}")
        # ç”±æ–¼ BWIBBU æª”æ¡ˆæ ¼å¼å¯èƒ½è¼ƒç‚ºè¤‡é›œï¼Œé€™è£¡å…ˆå‡è¨­æ¬„ä½åç¨±æ˜¯æ­£ç¢ºçš„ã€‚
        # å¦‚æœåŸ·è¡Œæ™‚å ±éŒ¯ï¼Œè«‹æª¢æŸ¥å¯¦éš› CSV æª”æ¡ˆä¸­çš„æ¬„ä½åç¨±ã€‚
        return None

    # 3. æ•¸æ“šæ¸…ç†èˆ‡ç¯©é¸
    # æ¸…ç† 'è­‰åˆ¸åç¨±' å…©å´ç©ºç™½ï¼Œç¢ºä¿ç²¾ç¢ºåŒ¹é…
    df['è­‰åˆ¸åç¨±'] = df['è­‰åˆ¸åç¨±'].astype(str).str.strip()

    # ç¯©é¸å‡ºç›®æ¨™è­‰åˆ¸åç¨±çš„æ•¸æ“š
    target_data = df[df['è­‰åˆ¸åç¨±'] == target_name]

    if target_data.empty:
        print(f"\nâ„¹ï¸ æç¤ºï¼šåœ¨æª”æ¡ˆä¸­æ‰¾ä¸åˆ°è­‰åˆ¸åç¨±ç‚º ã€{target_name}ã€‘ çš„æ•¸æ“šã€‚")
        return pd.DataFrame()
    
    # 4. æå–ç›®æ¨™æ¬„ä½æ•¸æ“š
    indicator_cols = ['è­‰åˆ¸åç¨±', 'æ®–åˆ©ç‡(%)', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']
    result_df = target_data[indicator_cols].copy() 

    # 5. è¼¸å‡ºçµæœ
    print(f"\nâœ… æˆåŠŸæ‰¾åˆ° ã€{target_name}ã€‘ çš„æŒ‡æ¨™æ•¸æ“šï¼š")
    print("=" * 40)
    
    # ä½¿ç”¨ to_string é€²è¡Œæ ¼å¼åŒ–è¼¸å‡º
    print(
        result_df.to_string(
            index=False,
            justify='left' # è®“æ–‡å­—é å·¦å°é½Š
        )
    )
    print("=" * 40)
    
    return result_df

# ä¸‰å¤§æ³•äººè²·è¶…å‰20
def get_top_20_institutional_trades_filtered(
    file_path: str, 
    volume_column: str = "ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸", 
    code_column: str = "è­‰åˆ¸ä»£è™Ÿ"
) -> Optional[pd.DataFrame]:
    """
    è®€å– CSV æª”æ¡ˆï¼Œé€²è¡Œä»¥ä¸‹ç¯©é¸ï¼š
    1. è­‰åˆ¸ä»£è™Ÿå¿…é ˆç‚º 4 ä½æ•¸å­—ã€‚
    2. ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸å¿…é ˆç‚ºæ­£æ•¸ (è²·è¶…)ã€‚
    3. è¿”å›è²·è³£è¶…è‚¡æ•¸æœ€å¤§çš„å‰ 10 åæ•¸æ“šï¼Œä¸¦è¼¸å‡ºç‚ºæ ¼å¼åŒ–è¡¨æ ¼ã€‚
    """
    print(f"\nğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ ç¯©é¸æ¢ä»¶ï¼š1. ä»£è™Ÿç‚º 4 ä½æ•¸å­— | 2. è²·è³£è¶…è‚¡æ•¸ > 0")

    # 1. è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except:
                df = pd.read_csv(file_path, encoding='big5')
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤æˆ–ç·¨ç¢¼å•é¡Œï¼š{e}")
        return None
    
    # 2. æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
    required_cols = [volume_column, code_column, 'è­‰åˆ¸åç¨±']
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{missing_cols}ã€‚")
        return None

    # 3. æ•¸æ“šæ¸…ç†èˆ‡æ•¸å€¼è½‰æ›
    try:
        # æ¸…ç†è²·è³£è¶…è‚¡æ•¸æ¬„ä½ï¼šç§»é™¤å¼•è™Ÿå’Œé€—è™Ÿ
        df[volume_column] = (
            df[volume_column].astype(str).str.replace('"', '', regex=False)
            .str.replace(',', '', regex=False).str.strip()
        )
        # è½‰æ›ç‚ºæ•¸å€¼é¡å‹ï¼Œç„¡æ³•è½‰æ›çš„å€¼è¨­ç‚º NaN
        df[volume_column] = pd.to_numeric(df[volume_column], errors='coerce')
        
        # æ¸…ç†è­‰åˆ¸ä»£è™Ÿæ¬„ä½
        df[code_column] = df[code_column].astype(str).str.strip()
        
        # ç§»é™¤ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼çš„è¡Œ
        df.dropna(subset=[volume_column], inplace=True)
        
    except Exception as e:
        print(f"âŒ æ•¸æ“šæ¸…ç†æˆ–æ•¸å€¼è½‰æ›å¤±æ•—ï¼š{e}")
        return None

    # 4. åŸ·è¡Œç¯©é¸æ¢ä»¶ 1ï¼šè­‰åˆ¸ä»£è™Ÿç‚º 4 ä½æ•¸å­—
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ç¯©é¸å‡ºå®Œå…¨ç¬¦åˆå››ä½æ•¸å­—çš„ä»£è™Ÿ
    df_filtered_code = df[df[code_column].str.match(r'^\d{4}$')]
    
    if df_filtered_code.empty:
        print("â„¹ï¸ æç¤ºï¼šç¯©é¸å¾Œï¼Œæ²’æœ‰æ‰¾åˆ°è­‰åˆ¸ä»£è™Ÿç‚º 4 ä½æ•¸å­—çš„æ•¸æ“šã€‚")
        return pd.DataFrame()

    # 5. åŸ·è¡Œç¯©é¸æ¢ä»¶ 2ï¼šè²·è³£è¶…è‚¡æ•¸ç‚ºæ­£æ•¸ (è²·è¶…)
    df_filtered_positive = df_filtered_code[df_filtered_code[volume_column] > 0]

    if df_filtered_positive.empty:
        print("â„¹ï¸ æç¤ºï¼šç¯©é¸å¾Œï¼Œæ²’æœ‰æ‰¾åˆ°ä¸‰å¤§æ³•äººè²·è¶… (æ­£æ•¸) çš„æ•¸æ“šã€‚")
        return pd.DataFrame()

    # 6. æ’åºä¸¦å–å‡ºå‰ 20 å
    df_sorted = df_filtered_positive.sort_values(
        by=volume_column, 
        ascending=False # è²·è¶…æ•¸æœ€å¤§çš„æ’åœ¨æœ€å‰é¢
    )
    
    # å–å‡ºå‰ 20 ç­†æ•¸æ“š
    top_20_trades = df_sorted.head(20)

    # 7. è¼¸å‡ºçµæœ (å›ºå®šæ¬„ä½å¯¬åº¦èˆ‡ç½®ä¸­å°é½Š)
    
    print(f"\nâœ… ç¯©é¸å¾Œçš„ä¸‰å¤§æ³•äººè²·è¶…å‰ {len(top_20_trades)} åï¼š")
    print("=" * 40)
    
    # æ ¼å¼åŒ–è¼¸å‡ºï¼šå°‡è‚¡æ•¸è½‰æ›ç‚ºæ•´æ•¸æ ¼å¼ï¼Œä¸¦åŠ ä¸Šåƒåˆ†ä½é€—è™Ÿ
    top_20_trades_display = top_20_trades.copy()
    
    # é‡æ–°å‘½åæ¬„ä½ä»¥ç°¡åŒ–æ¨™é¡Œ
    volume_col_display_name = 'è²·è¶…å¼µæ•¸'
    top_20_trades_display = top_20_trades_display.rename(
        columns={'è­‰åˆ¸ä»£è™Ÿ': 'ä»£è™Ÿ', volume_column: volume_col_display_name}
    )

    # æ ¼å¼åŒ–æ•¸å­— (åŠ ä¸Šåƒåˆ†ä½é€—è™Ÿ)
    top_20_trades_display[volume_col_display_name] = top_20_trades_display[volume_col_display_name].apply(lambda x: f"{int(x/1000):,}")

    # å®šç¾©è¼¸å‡ºçš„æ¬„ä½é †åº
    #actual_display_cols = ['ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', volume_col_display_name]
    actual_display_cols = ['è­‰åˆ¸åç¨±', volume_col_display_name]

    # è¨­å®šæ¯å€‹æ¬„ä½çš„æœ€å°å¯¬åº¦ï¼Œä»¥åˆ©ç½®ä¸­ (ä¸­æ–‡å­—ä½” 2 å¯¬åº¦)
    col_space_width = 10 

    # ä½¿ç”¨ to_string é…åˆ col_space å’Œ justify='center'
    print(
        top_20_trades_display[actual_display_cols].to_string(
            index=False,
            col_space=col_space_width, 
            justify='left' # å˜—è©¦ç½®ä¸­å°é½Š
        )
    )
    print("=" * 40)
    
    top_20_trades = ""
    index_no = 1
    target_length = 6
    for index, rol in top_20_trades_display.iterrows():
        index_no_str = str(index_no).zfill(2)
        
        current_length = len(rol['è­‰åˆ¸åç¨±'].strip())
        
        current_volume_column = rol['è­‰åˆ¸åç¨±'].strip()
        if current_length <= target_length:
        # è¨ˆç®—éœ€è¦å¡«å……çš„é•·åº¦
            padding_needed = target_length - current_length 
            new_current_volume_column = current_volume_column.ljust(padding_needed, ' ')
        
        top_20_trades += f"{index_no_str}." + f"{new_current_volume_column}" + f" ({rol[volume_col_display_name]}å¼µ)âšªï¸ğŸ”´\n"
        index_no += 1
        
    return top_20_trades

# è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„è³‡æ–™ï¼Œä¸¦åªè¿”å›ã€Œè²·è³£è¶…è‚¡æ•¸ã€æ•¸æ“šã€‚
def get_stock_net_volume(file_path, target_name, target_column="ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸"):
    """
    è®€å– CSV æª”æ¡ˆï¼Œç¯©é¸å‡ºæŒ‡å®šè­‰åˆ¸åç¨±çš„è³‡æ–™ï¼Œä¸¦åªè¿”å›ã€Œè²·è³£è¶…è‚¡æ•¸ã€æ•¸æ“šã€‚
    Args:
        file_path (str): CSVæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        target_name (str): è¦ç¯©é¸çš„è­‰åˆ¸åç¨±ã€‚
        target_column (str): è¦å–å‡ºçš„æ¬„ä½åç¨± (é è¨­ç‚º 'è²·è³£è¶…è‚¡æ•¸')ã€‚
    Returns:
        pd.Series or None: åŒ…å«ç›®æ¨™è²·è³£è¶…è‚¡æ•¸çš„ Seriesï¼Œå¦‚æœè®€å–æˆ–ç¯©é¸å¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{file_path}")
    print(f"ğŸ¯ æœå°‹ç›®æ¨™ï¼šã€{target_name}ã€‘ï¼Œä¸¦å–å‡ºã€{target_column}ã€‘æ•¸æ“š")

    # 1. è®€å–CSVæª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦ï¼Œç¢ºä¿è¼¸å…¥æ­£ç¢º)
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'utf-8-sig' ç·¨ç¢¼è®€å–ã€‚")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                print("â„¹ï¸ ä½¿ç”¨ 'utf-8' ç·¨ç¢¼è®€å–ã€‚")
            except:
                df = pd.read_csv(file_path, encoding='big5')
                print("â„¹ï¸ ä½¿ç”¨ 'big5' ç·¨ç¢¼è®€å–ã€‚")
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤æˆ–ç·¨ç¢¼å•é¡Œï¼š{e}")
        return None
    
    # 2. æª¢æŸ¥é—œéµæ¬„ä½æ˜¯å¦å­˜åœ¨
    required_cols = ['è­‰åˆ¸åç¨±', target_column]
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{missing_cols}ã€‚")
        print(f"æª”æ¡ˆå¯¦éš›æ¬„ä½åç¨±ï¼š{list(df.columns)}")
        return None

    # 3. æ•¸æ“šæ¸…ç†èˆ‡ç¯©é¸
    # æ¸…ç† 'è­‰åˆ¸åç¨±' å…©å´ç©ºç™½ï¼Œç¢ºä¿ç²¾ç¢ºåŒ¹é…
    df['è­‰åˆ¸åç¨±'] = df['è­‰åˆ¸åç¨±'].astype(str).str.strip()

    # â­ æ ¸å¿ƒä¿®æ”¹é» A: æ¸…ç† 'è²·è³£è¶…è‚¡æ•¸' æ¬„ä½ï¼Œç§»é™¤å¼•è™Ÿä¸¦æ¸…ç†ç©ºç™½ï¼Œç‚ºæ•¸å€¼è½‰æ›åšæº–å‚™
    try:
        df[target_column] = df[target_column].astype(str).str.replace('"', '', regex=False).str.strip()
        # print(f"âœ… æˆåŠŸç§»é™¤ {target_column} æ¬„ä½ä¸­çš„é›™å¼•è™Ÿã€‚")
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Šï¼šå˜—è©¦æ¸…ç† {target_column} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    target_data = df[df['è­‰åˆ¸åç¨±'] == target_name]

    # 4. å–å‡ºç›®æ¨™æ¬„ä½æ•¸æ“š
    if target_data.empty:
        print(f"\nâ„¹ï¸ æç¤ºï¼šåœ¨æª”æ¡ˆä¸­æ‰¾ä¸åˆ°è­‰åˆ¸åç¨±ç‚º ã€{target_name}ã€‘ çš„æ•¸æ“šã€‚")
        # çµ±ä¸€è¿”å› Noneï¼Œè®“å¤–éƒ¨ç¨‹å¼ç¢¼åªéœ€æª¢æŸ¥ None
        return None
    else:
        # å–å‡º 'è²·è³£è¶…è‚¡æ•¸' æ¬„ä½ï¼Œé€™æ˜¯ä¸€å€‹ pandas.Series å°è±¡
        net_volume_series = target_data[target_column]
        
        print(f"\nâœ… æˆåŠŸæ‰¾åˆ° ã€{target_name}ã€‘ çš„ {len(net_volume_series)} ç­†ã€{target_column}ã€‘æ•¸æ“šã€‚")
        print("-" * 60)
        # é€™è£¡ä¸é¡¯ç¤º Series åŸå§‹å…§å®¹ï¼Œè®“æœ€çµ‚è¼¸å‡ºæ›´èšç„¦
        
    #print("æ¸¬è©¦1",net_volume_series)
    #sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ
        
    return net_volume_series

# è®€å–æŒ‡å®šçš„CSVæª”æ¡ˆï¼Œé¸å–ç¬¬2æ¬„åˆ°ç¬¬6æ¬„çš„æ•¸æ“š
def select_and_save_columns_fix_encoding(input_file_path, output_directory, output_file_name="selected_data.csv"):
    """
    è®€å–æŒ‡å®šçš„CSVæª”æ¡ˆï¼Œé¸å–ç¬¬2æ¬„åˆ°ç¬¬6æ¬„çš„æ•¸æ“šï¼Œ
    ä¸¦å°‡çµæœå¦å­˜ç‚ºæ–°çš„CSVæª”æ¡ˆï¼Œä½¿ç”¨ 'utf-8-sig' ç·¨ç¢¼è§£æ±ºè¼¸å‡ºä¸­æ–‡äº‚ç¢¼å•é¡Œã€‚

    Args:
        input_file_path (str): ä¾†æº CSV æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        output_directory (str): å„²å­˜æ–° CSV æª”æ¡ˆçš„ç›®éŒ„è·¯å¾‘ã€‚
        output_file_name (str): å„²å­˜æ–° CSV æª”æ¡ˆçš„åç¨±ã€‚

    Returns:
        bool: æˆåŠŸå„²å­˜å‰‡è¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
    """
    # çµ„åˆå®Œæ•´çš„è¼¸å‡ºè·¯å¾‘
    output_file_path = os.path.join(output_directory, output_file_name)
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(output_directory, exist_ok=True)
    
    print(f"ğŸ”„ æ­£åœ¨è®€å–æª”æ¡ˆï¼š{input_file_path}")

    try:
        # 1. è®€å–CSVæª”æ¡ˆ (ä¿ç•™å¤šç·¨ç¢¼å˜—è©¦ï¼Œç¢ºä¿è¼¸å…¥æ­£ç¢º)
        try:
            df = pd.read_csv(input_file_path, encoding='big5')
            print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'big5' ç·¨ç¢¼è®€å–æª”æ¡ˆã€‚")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(input_file_path, encoding='utf-8')
                print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'utf-8' ç·¨ç¢¼è®€å–æª”æ¡ˆã€‚")
            except Exception:
                df = pd.read_csv(input_file_path, encoding='cp950')
                print("â„¹ï¸ æˆåŠŸä½¿ç”¨ 'cp950' ç·¨ç¢¼è®€å–æª”æ¡ˆã€‚")
                
        # 2. åˆ¤æ–·æ¬„ä½æ•¸é‡æ˜¯å¦è¶³å¤ ï¼Œä¸¦é¸å–ç¬¬ 2 æ¬„åˆ°ç¬¬ 6 æ¬„ (ç´¢å¼• 1 åˆ° 5)
        start_index = 1 
        end_index = 6
        num_columns = len(df.columns)
        
        if num_columns < end_index:
            print(f"âš ï¸ éŒ¯èª¤ï¼šæª”æ¡ˆæ¬„ä½ç¸½æ•¸ä¸è¶³ {end_index} æ¬„ (åªæœ‰ {num_columns} æ¬„)ã€‚ç„¡æ³•é¸å–ç¬¬ 2 åˆ° 6 æ¬„ã€‚")
            return False

        # é¸å–æ•¸æ“š
        df_selected = df.iloc[:, start_index:end_index]
        
        print(f"âœ… æˆåŠŸé¸å–æ¬„ä½ï¼š{list(df_selected.columns)}")

        # --- æ ¸å¿ƒä¿®æ”¹é» ---
        # 3. å¦å­˜ç‚ºæ–°çš„ CSV æª”æ¡ˆï¼Œä½¿ç”¨ 'utf-8-sig' ç·¨ç¢¼
        # 'utf-8-sig' åŒ…å« BOM (Byte Order Mark)ï¼Œæœ‰åŠ©æ–¼ Excel ç­‰è»Ÿé«”æ­£ç¢ºè­˜åˆ¥ä¸­æ–‡æª”é ­ã€‚
        df_selected.to_csv(output_file_path, index=False, encoding='utf-8-sig') 
        # -------------------

        print("-" * 40)
        print(f"âœ¨ æ•¸æ“šå·²æˆåŠŸå„²å­˜åˆ°ï¼š{output_file_path}")
        print("-" * 40)
        return True

    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„è¼¸å…¥æª”æ¡ˆè·¯å¾‘ -> {input_file_path}")
        return False
    except pd.errors.EmptyDataError:
        print(f"âŒ éŒ¯èª¤ï¼šæª”æ¡ˆæ˜¯ç©ºçš„æˆ–ç„¡æ•ˆçš„æ•¸æ“šæ ¼å¼ -> {input_file_path}")
        return False
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤ï¼š{e}")
        return False

# å°‡æŒ‡å®šæª”æ¡ˆè¤‡è£½åˆ°ç›®æ¨™ç›®éŒ„ã€‚
def copy_file_to_directory(source: str, destination: str):
    """
    å°‡æŒ‡å®šæª”æ¡ˆè¤‡è£½åˆ°ç›®æ¨™ç›®éŒ„ã€‚
    """
    source_path = pathlib.Path(source)
    destination_dir_path = pathlib.Path(destination)
    
    print(f"âœ… æ­£åœ¨æº–å‚™è¤‡è£½æª”æ¡ˆ...")
    print(f"ä¾†æº: {source_path}")
    print(f"ç›®æ¨™ç›®éŒ„: {destination_dir_path}")

    # 1. æª¢æŸ¥ä¾†æºæª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not source_path.exists() or not source_path.is_file():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆæˆ–ä¾†æºä¸æ˜¯ä¸€å€‹æª”æ¡ˆ: {source}")
        return

    # 2. æª¢æŸ¥ç›®æ¨™ç›®éŒ„æ˜¯å¦å­˜åœ¨ (å¦‚æœä¸å­˜åœ¨ï¼Œshutil.copy æœƒè‡ªå‹•å‰µå»ºï¼Œä½†æˆ‘å€‘æœ€å¥½å…ˆæª¢æŸ¥ä¸¦åˆ—å°è¨Šæ¯)
    if not destination_dir_path.exists():
        print(f"âš ï¸ è­¦å‘Šï¼šç›®æ¨™ç›®éŒ„ '{destination}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å˜—è©¦å»ºç«‹...")
        try:
            destination_dir_path.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•å»ºç«‹ç›®æ¨™ç›®éŒ„: {e}")
            return
    
    try:
        # 3. åŸ·è¡Œè¤‡è£½æ“ä½œ
        # shutil.copy æœƒå°‡æª”æ¡ˆè¤‡è£½åˆ°ç›®éŒ„ä¸­ï¼Œä¸¦ä¿ç•™åŸå§‹æª”æ¡ˆå
        shutil.copy(source, destination)
        
        # å»ºç«‹å®Œæ•´çš„ç›®æ¨™è·¯å¾‘ç”¨æ–¼è¼¸å‡ºè¨Šæ¯
        destination_file_path = destination_dir_path / source_path.name
        
        print("\n" + "="*50)
        print("ğŸ‰ æª”æ¡ˆè¤‡è£½æˆåŠŸï¼")
        print(f"æ–°æª”æ¡ˆä½ç½®: {destination_file_path}")
        print("="*50)

    except Exception as e:
        print(f"âŒ è¤‡è£½æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def _read_local_csv(file_path: pathlib.Path) -> Optional[pd.DataFrame]:
    """
    è®€å–æœ¬åœ° CSV æª”æ¡ˆï¼Œä¸¦è™•ç†ä¸å­˜åœ¨çš„æƒ…æ³ã€‚
    """
    if not file_path.exists():
        # print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}")
        return None
    try:
        # å‡è¨­æœ¬åœ°å„²å­˜çš„ CSV å·²ç¶“æ˜¯ UTF-8-SIG ç·¨ç¢¼
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        # æ¸…é™¤æ¬„ä½åç¨±å‰å¾Œçš„ç©ºç™½
        df.columns = df.columns.str.strip()
        return df
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼šè®€å–æœ¬åœ° CSV æª”æ¡ˆ {file_path} æ™‚å¤±æ•—: {e}")
        return None
# å¾æŒ‡å®šçš„ CSV æª”æ¡ˆä¸­æŸ¥è©¢ç‰¹å®šè­‰åˆ¸çš„æ”¶ç›¤åƒ¹ã€‚
def lookup_stock_price(file_path: pathlib.Path, stock_name: str, name_col: str, price_col: str) -> Optional[str]:
    """
    å¾æŒ‡å®šçš„ BWIBBU CSV æª”æ¡ˆä¸­æŸ¥æ‰¾ç‰¹å®šè‚¡ç¥¨çš„æ”¶ç›¤åƒ¹ã€‚
    
    ä¿®æ­£: ç¢ºä¿åœ¨è¿”å›åƒ¹æ ¼å‰ï¼Œç§»é™¤åƒä½åˆ†éš”ç¬¦è™Ÿ (,) ä»¥é¿å… ValueErrorã€‚
    """
    df = _read_local_csv(file_path)
    if df is None:
        print(f"  > è­¦å‘Š: åƒ¹æ ¼æª”æ¡ˆ {file_path.name} ç¼ºå¤±æˆ–è®€å–å¤±æ•—ã€‚")
        #sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ
        return None
        
    try:
        # å°‹æ‰¾åŒ¹é…çš„è‚¡ç¥¨åç¨±
        result = df[df[name_col] == stock_name]
        if not result.empty and price_col in result.columns:
            price_raw = result.iloc[0][price_col]

            # --- åƒ¹æ ¼æ¸…ç†èˆ‡è½‰æ› ---
            price_float = None
            if price_raw is not None:
                # 1. å¦‚æœæ˜¯å­—ä¸²ï¼Œç§»é™¤é€—è™Ÿå’Œå‰å¾Œç©ºç™½
                if isinstance(price_raw, str):
                    price_clean_str = price_raw.replace(',', '').strip()
                else:
                    price_clean_str = str(price_raw)
                
                # 2. å˜—è©¦è½‰æ›ç‚º float
                try:
                    price_float = float(price_clean_str)
                except (ValueError, TypeError):
                    print(f"  > è­¦å‘Š: {stock_name} çš„åƒ¹æ ¼ '{price_raw}' ç„¡æ³•è½‰æ›ç‚ºæ•¸å­—ã€‚")
                    return None
            
            if price_float is not None:
                # è¿”å›æ ¼å¼åŒ–ç‚ºå…©ä½å°æ•¸çš„åƒ¹æ ¼å­—ä¸²
                return f"{price_float:.2f}"
            else:
                return None
        else:
            # print(f"  > è­¦å‘Š: æ‰¾ä¸åˆ° {stock_name} çš„åƒ¹æ ¼è³‡æ–™ã€‚")
            return None
    except Exception as e:
        print(f"  > åƒ¹æ ¼æŸ¥è©¢å¤±æ•—: {e}")
        return None
    

# å¾äº¤æ˜“æ—¥æª”æ¡ˆä¸­ï¼Œæ‰¾å‡ºä»Šå¤©å¾€å‰æ•¸ N å€‹äº¤æ˜“æ—¥ï¼Œä¸¦æ ¹æ“šç•¶å‰æ™‚é–“ (15:00) åˆ¤æ–·æ˜¯å¦ç´å…¥ä»Šå¤©ã€‚
def find_last_n_trading_days_with_time_check(file_path, n=6):
    """
    å¾äº¤æ˜“æ—¥æª”æ¡ˆä¸­ï¼Œæ‰¾å‡ºä»Šå¤©å¾€å‰æ•¸ N å€‹äº¤æ˜“æ—¥ï¼Œä¸¦æ ¹æ“šç•¶å‰æ™‚é–“ (15:00) åˆ¤æ–·æ˜¯å¦ç´å…¥ä»Šå¤©ã€‚

    :param file_path: è‚¡ç¥¨äº¤æ˜“æ—¥ CSV æª”æ¡ˆè·¯å¾‘
    :param n: å¾€å‰æ‰¾çš„äº¤æ˜“æ—¥æ•¸é‡ (é è¨­ç‚º 6)
    --å–6å€‹ä½†æœ€å¾Œä¸€å€‹ä¸é¡¯ç¤ºï¼Œä½œç‚ºæ•¸æ“šè¨ˆç®—ç”¨
    :return: åŒ…å«æœ€è¿‘ N å€‹äº¤æ˜“æ—¥çš„ DataFrame (æˆ– None if failed)
    """
    
    # 1. å®šç¾©ç•¶å‰æ™‚é–“å’Œåˆ¤æ–·æ¨™æº–
    now = datetime.now()
    today_date = now.date()
    cutoff_time = time_TimeClass(15, 0, 0) # ä¸‹åˆ 15:00:00
    is_after_cutoff = now.time() >= cutoff_time

    print(f"ç•¶å‰æ—¥æœŸ: {today_date.strftime('%Y/%m/%d')}, ç•¶å‰æ™‚é–“æ˜¯å¦åœ¨ 15:00 ä¹‹å¾Œ: {is_after_cutoff}")
    
    # 2. è®€å–äº¤æ˜“æ—¥æª”æ¡ˆ
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}")
        return None
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘å’Œç·¨ç¢¼: {e}")
        return None

    # å‡è¨­æ—¥æœŸæ¬„ä½ç‚º 'æ—¥æœŸ'
    date_column = 'æ—¥æœŸ' 
    if date_column not in df.columns:
        # å˜—è©¦ä½¿ç”¨å¸¸è¦‹çš„è‹±æ–‡æ¬„ä½å
        if 'Date' in df.columns:
            date_column = 'Date'
        else:
            print(f"éŒ¯èª¤ï¼šç„¡æ³•è­˜åˆ¥äº¤æ˜“æ—¥æœŸçš„æ¬„ä½åç¨±ã€‚è«‹æª¢æŸ¥æ‚¨çš„ CSV æª”æ¡ˆã€‚")
            return None
        
    # 3. æ¸…ç†å’Œè½‰æ›æ—¥æœŸæ ¼å¼
    df[date_column] = pd.to_datetime(df[date_column], errors='coerce').dt.normalize()
    df.dropna(subset=[date_column], inplace=True)
    
    # å»ºç«‹æ‰€æœ‰äº¤æ˜“æ—¥çš„é›†åˆï¼Œç”¨æ–¼å¿«é€Ÿåˆ¤æ–·ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥
    all_trading_dates = set(df[date_column].dt.date)
    is_today_trading_day = today_date in all_trading_dates
    
    print(f"ä»Šå¤© ({today_date.strftime('%Y/%m/%d')}) æ˜¯å¦ç‚ºäº¤æ˜“æ—¥: {is_today_trading_day}")

    # 4. æ ¹æ“šæ™‚é–“åˆ¤æ–·æ±ºå®šè³‡æ–™ç¯©é¸çš„æˆªæ­¢æ—¥æœŸ
    
    # é è¨­ï¼šå¦‚æœä¸æ»¿è¶³ç´å…¥ä»Šå¤©çš„æ¢ä»¶ï¼Œå‰‡æˆªæ­¢æ—¥æœŸç‚ºæ˜¨å¤©
    inclusion_date = today_date - timedelta(days=1)
    
    # åˆ¤æ–·æ˜¯å¦æ‡‰è©²ç´å…¥ä»Šå¤©
    if is_today_trading_day and is_after_cutoff:
        # æ¢ä»¶ 1: ä»Šå¤©æ˜¯äº¤æ˜“æ—¥
        # æ¢ä»¶ 2: ä¸”æ™‚é–“åœ¨ 15:00 ä¹‹å¾Œ (è¦–ç‚ºä»Šå¤©äº¤æ˜“å·²å®Œæˆ)
        # -> ç´å…¥ä»Šå¤©
        inclusion_date = today_date
        print("-> åˆ¤æ–·ï¼šç´å…¥ä»Šå¤©çš„äº¤æ˜“æ—¥ã€‚")
    else:
        # å…¶ä»–æƒ…æ³ (éäº¤æ˜“æ—¥ã€æˆ–äº¤æ˜“æ—¥ä½†æœªæ»¿ 15:00)
        # -> æ’é™¤ä»Šå¤©ï¼Œåªå–æ˜¨å¤©åŠæ›´æ—©çš„äº¤æ˜“æ—¥
        inclusion_date = today_date - timedelta(days=1)
        print("-> åˆ¤æ–·ï¼šæ’é™¤ä»Šå¤©çš„äº¤æ˜“æ—¥ï¼Œåªå–æ˜¨å¤©åŠæ›´æ—©çš„æ—¥æœŸã€‚")

    # 5. ç¯©é¸ã€æ’åºä¸¦é¸å–æœ€è¿‘ N å€‹äº¤æ˜“æ—¥
    
    # ç¯©é¸å‡ºæ—¥æœŸå°æ–¼æˆ–ç­‰æ–¼æ±ºå®šæˆªæ­¢æ—¥æœŸçš„äº¤æ˜“æ—¥
    df_past = df[df[date_column].dt.date <= inclusion_date]
    
    # ç¢ºä¿æ—¥æœŸç”±è¿‘åˆ°é æ’åº
    df_past = df_past.sort_values(by=date_column, ascending=False)

    # é¸å–æœ€è¿‘çš„ N å€‹äº¤æ˜“æ—¥
    last_n_days = df_past.head(n)

    if last_n_days.empty:
        print(f"è­¦å‘Šï¼šäº¤æ˜“æ—¥è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•æ‰¾åˆ°å¾€å‰ {n} å€‹äº¤æ˜“æ—¥ã€‚")
        return None

    # å°‡çµæœç”±èˆŠåˆ°æ–°æ’åºä¸¦æ ¼å¼åŒ–è¼¸å‡º
    last_n_days = last_n_days.sort_values(by=date_column, ascending=True)
    last_n_days[date_column] = last_n_days[date_column].dt.strftime('%Y/%m/%d')
    
    print(f"\nâœ… æˆåŠŸæ‰¾åˆ°ä»Šå¤©å¾€å‰ {n} å€‹äº¤æ˜“æ—¥ã€‚")
    return last_n_days

# å¾ Excel æª”æ¡ˆä¸­è®€å–è‚¡ç¥¨åº«å­˜ï¼Œå°‡å…¶å¦å­˜ç‚º CSV æª”æ¡ˆã€‚
def extract_excel_sheet_filter_and_save(excel_file_path: str, sheet_name: str, filter_column: str, filter_value: any, output_dir: str = None) -> pathlib.Path:
    """
    å¾æŒ‡å®šçš„ Excel æª”æ¡ˆä¸­è®€å–ç‰¹å®šå·¥ä½œè¡¨ï¼Œè·³éç¬¬äºŒè¡Œï¼Œç¯©é¸è³‡æ–™å¾Œï¼Œå°‡å…¶å¦å­˜ç‚º CSV æª”æ¡ˆã€‚

    Args:
        excel_file_path (str): åŸå§‹ Excel æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
        sheet_name (str): è¦è®€å–çš„å·¥ä½œè¡¨åç¨± (ä¾‹å¦‚: 'è‚¡ç¥¨åº«å­˜çµ±è¨ˆ')ã€‚
        filter_column (str): è¦é€²è¡Œç¯©é¸çš„æ¬„ä½åç¨± (ä¾‹å¦‚: 'ç›®å‰è‚¡æ•¸åº«å­˜çµ±è¨ˆ')ã€‚
        filter_value (any): è¦ç¯©é™¤çš„å€¼ã€‚
        output_dir (str, optional): CSV æª”æ¡ˆçš„å„²å­˜ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡å„²å­˜åœ¨åŸå§‹æª”æ¡ˆçš„ç›®éŒ„ã€‚

    Returns:
        Path: å„²å­˜æˆåŠŸçš„ CSV æª”æ¡ˆè·¯å¾‘ã€‚
    """
    
    original_path = pathlib.Path(excel_file_path)
    
    if not original_path.exists():
        raise FileNotFoundError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° Excel æª”æ¡ˆåœ¨è·¯å¾‘ï¼š{excel_file_path}")

    print(f"âœ… æ­£åœ¨è®€å– Excel æª”æ¡ˆï¼š{original_path.name}")
    print(f"ğŸ¯ ç›®æ¨™å·¥ä½œè¡¨åç¨±ï¼š{sheet_name}")

    try:
        # 1. è®€å– Excel ä¸­æŒ‡å®šå·¥ä½œè¡¨çš„è³‡æ–™
        # header=0: æŒ‡å®š Excel çš„ç¬¬ä¸€è¡Œï¼ˆç´¢å¼• 0ï¼‰ä½œç‚ºæ¬„ä½åç¨±
        # skiprows=[1]: è·³éç´¢å¼•ç‚º 1 çš„è¡Œï¼Œå³ Excel ä¸­çš„ç¬¬äºŒè¡Œ
        df = pd.read_excel(
            original_path, 
            sheet_name=sheet_name, 
            header=0,
            skiprows=[1]  # <--- â— é€™è£¡åŠ å…¥è·³é Excel ç¬¬äºŒè¡Œï¼ˆç´¢å¼• 1ï¼‰çš„è¨­å®š
        )
        
        if df.empty:
            print(f"è­¦å‘Šï¼šå·¥ä½œè¡¨ '{sheet_name}' è®€å–åˆ°çš„æ•¸æ“šç‚ºç©ºã€‚")
            return None

    except ValueError as e:
        raise ValueError(f"éŒ¯èª¤ï¼šåœ¨ Excel æª”æ¡ˆä¸­æ‰¾ä¸åˆ°åç‚º '{sheet_name}' çš„å·¥ä½œè¡¨ã€‚è«‹æª¢æŸ¥åç¨±æ˜¯å¦æ­£ç¢ºã€‚è©³ç´°éŒ¯èª¤: {e}")
    except Exception as e:
        raise Exception(f"è®€å– Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
    # 2. **ã€é—œéµç¯©é¸æ­¥é©Ÿã€‘**
    if filter_column not in df.columns:
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°ç¯©é¸æ¬„ä½ '{filter_column}'ã€‚è·³éç¯©é¸æ­¥é©Ÿã€‚")
    else:
        initial_rows = len(df)
        print(f"\nğŸ” é–‹å§‹ç¯©é¸ï¼šç§»é™¤ '{filter_column}' å€¼ç‚º '{filter_value}' çš„è³‡æ–™...")
        
        # å˜—è©¦å°‡ç¯©é¸æ¬„ä½è½‰æ›ç‚ºæ•¸å€¼é¡å‹ï¼Œcoerce æœƒå°‡éæ•¸å€¼è½‰æ›ç‚º NaN
        df[filter_column] = pd.to_numeric(df[filter_column], errors='coerce')
        
        # ç¯©é¸é‚è¼¯ï¼šä¿ç•™ 'ç›®å‰è‚¡æ•¸åº«å­˜çµ±è¨ˆ' ä¸ç­‰æ–¼ 0 çš„è¡Œ
        df_filtered = df[df[filter_column] != float(filter_value)]
        
        removed_rows = initial_rows - len(df_filtered)
        print(f"  -> åŸå§‹ç­†æ•¸ (å·²è·³éç¬¬äºŒè¡Œ): {initial_rows} ç­†")
        print(f"  -> ç§»é™¤ç­†æ•¸: {removed_rows} ç­†")
        print(f"  -> å‰©é¤˜ç­†æ•¸: {len(df_filtered)} ç­†")
        
        df = df_filtered
        
        if df.empty:
            print("è­¦å‘Šï¼šç¯©é¸å¾Œæ•¸æ“šç‚ºç©ºã€‚")
            return None


    # 3. æº–å‚™è¼¸å‡º CSV æª”æ¡ˆçš„è·¯å¾‘
    
    if output_dir is None:
        output_dir = original_path.parent
    else:
        output_dir = pathlib.Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("_%Y%m%d_%H%M%S")
    csv_file_name = f"{sheet_name}_filtered{timestamp}.csv"
    output_csv_path = output_dir / csv_file_name
    
    # 4. å„²å­˜ç‚º CSV æª”æ¡ˆ
    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')

    return output_csv_path

# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”ç¢ºå¯¦æ˜¯ä¸€å€‹æª”æ¡ˆ (éè³‡æ–™å¤¾)
def check_folder_and_create(folder_path: str):
    """
    åƒæ•¸:
        file_path (str): è¦æª¢æŸ¥çš„æª”æ¡ˆè·¯å¾‘ã€‚
    å›å‚³:
        bool: æª”æ¡ˆå­˜åœ¨æ™‚å›å‚³ Trueï¼›å¦å‰‡å›å‚³ Falseã€‚
    """
    OUTPUT_DIR, filename_new = jutils.get_path_to_folder_file(folder_path)
    jutils.check_and_create_folder(OUTPUT_DIR)
    jutils.check_file_exists(filename_new)
    return True


# å…±ç”¨çš„è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼è™•ç† TWSE çš„ Big5 ç·¨ç¢¼å’Œ Pandas è®€å–é‚è¼¯ã€‚
def _read_twse_csv(response_text: str, header_row: int) -> Optional[pd.DataFrame]:
    """
    Args:
        response_text: HTTP è«‹æ±‚å›å‚³çš„æ–‡å­—å…§å®¹ (Big5 ç·¨ç¢¼)ã€‚
        header_row: CSV æª”æ¡ˆä¸­è³‡æ–™è¡¨é ­æ‰€åœ¨çš„è¡Œæ•¸ (0-indexed)ã€‚
    Returns:
        Optional[pd.DataFrame]: è™•ç†å¾Œçš„ DataFrameã€‚
    """
    try:
        csv_data = StringIO(response_text)
        # å˜—è©¦è®€å– CSV
        df = pd.read_csv(
            csv_data, 
            header=header_row,          # è³‡æ–™è¡¨é ­æ‰€åœ¨çš„è¡Œæ•¸
            skipinitialspace=True,      # è·³éåˆ†éš”ç¬¦å¾Œçš„ç©ºæ ¼
            on_bad_lines='skip',        # è·³éæ ¼å¼ä¸æ­£ç¢ºçš„è¡Œ
            encoding='Big5'             # ä½¿ç”¨ Big5 ç·¨ç¢¼è®€å–
        )
        # TWSE çš„ CSV æ¬„ä½åç¨±å¸¸æœ‰éš±è—ç©ºæ ¼ï¼Œå°è‡´ df.columns ç„¡æ³•æ­£ç¢ºåŒ¹é…ã€‚
        if not df.empty:
            df.columns = df.columns.str.strip()
        # ç§»é™¤æ‰€æœ‰æ¬„ä½çš†ç‚ºç©ºçš„è¡Œ
        df = df.dropna(how='all')
        # ç§»é™¤è³‡æ–™å°¾éƒ¨å¯èƒ½å‡ºç¾çš„å½™ç¸½æˆ–å‚™è¨»è¡Œ
        if not df.empty and df.iloc[-1].astype(str).str.contains('åˆè¨ˆ|ç¸½è¨ˆ|å‚™è¨»', na=False).any():
            df = df.iloc[:-1]
        return df
    except Exception as e:
        print(f"åœ¨è®€å–æˆ–æ¸…ç† CSV æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return None

# å…±ç”¨çš„è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼ç™¼é€ HTTP è«‹æ±‚ä¸¦æª¢æŸ¥ç‹€æ…‹ã€‚
def _fetch_twse_data(url: str) -> Optional[str]:
    """
    Args:
        url: å®Œæ•´çš„ TWSE è³‡æ–™ URLã€‚
    Returns:
        Optional[str]: æˆåŠŸç²å–å¾Œï¼Œä»¥ Big5 è§£ç¢¼çš„æ–‡å­—å…§å®¹ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status() 
        response.encoding = 'Big5'
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f"âŒ HTTP éŒ¯èª¤ï¼š{errh} (è©²æ—¥å¯èƒ½ç„¡äº¤æ˜“è³‡æ–™)")
    except requests.exceptions.RequestException as err:
        print(f"âŒ é€£ç·šæˆ– Requests éŒ¯èª¤: {err}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")

    return None

# --- 10 å¤§ TWSE å ±å‘ŠæŠ“å–å‡½å¼ (åˆ†é èˆ‡å€‹è‚¡) ---
def fetch_twse_stock_day(target_date: str, stock_no: str) -> Optional[pd.DataFrame]:
    """
    (1/10) æŠ“å–æŒ‡å®šæ—¥æœŸå’Œè‚¡ç¥¨ä»£è™Ÿçš„ STOCK_DAY å ±å‘Š (æ¯æ—¥æˆäº¤è³‡è¨Š)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY"
    url = f"{base_url}?date={target_date}&stockNo={stock_no}&response=csv"

    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "1_STOCK_DAY")
    filename = OUTPUT_DIR + f"\\{target_date}_{stock_no}_STOCK_DAY.csv"

    check_folder_and_create(filename)

    print(f"å˜—è©¦æŠ“å– (1/10) {stock_no} STOCK_DAY è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    
    if response_text is None: return None
    
    df = _read_twse_csv(response_text, header_row=1)
    if df is not None and 'æ—¥æœŸ' in df.columns:
        df = df[df['æ—¥æœŸ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (1/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_mi_index(target_date: str) -> Optional[pd.DataFrame]:
    """
    (2/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ MI_INDEX å ±å‘Š (æ‰€æœ‰é¡è‚¡æˆäº¤çµ±è¨ˆ)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX"
    url = f"{base_url}?date={target_date}&type=ALLBUT0999&response=csv"
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "2_MI_INDEX")
    filename = OUTPUT_DIR + f"\\{target_date}_MI_INDEX_Sector.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (2/10) MI_INDEX (é¡è‚¡çµ±è¨ˆ) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # MI_INDEX å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'æŒ‡æ•¸' in df.columns:
        df = df[df['æŒ‡æ•¸'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (2/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_bwibbu_d(target_date: str) -> Optional[pd.DataFrame]:
    """
    (3/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ BWIBBU_d å ±å‘Š (ç™¼è¡Œé‡åŠ æ¬Šè‚¡åƒ¹æŒ‡æ•¸é¡è‚¡æ—¥æˆäº¤é‡å€¼åŠå ±é…¬ç‡)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d"
    url = f"{base_url}?date={target_date}&selectType=ALL&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "3_BWIBBU_d")
    filename = OUTPUT_DIR + f"\\{target_date}_BWIBBU_d_IndexReturn.csv"

    check_folder_and_create(filename)
        
    print(f"å˜—è©¦æŠ“å– (3/10) BWIBBU_d (é¡è‚¡å ±é…¬ç‡) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # BWIBBU_d å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (3/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_mi_index20(target_date: str) -> Optional[pd.DataFrame]:
    """
    (4/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ MI_INDEX20 å ±å‘Š (æ”¶ç›¤æŒ‡æ•¸åŠæˆäº¤é‡å€¼è³‡è¨Š)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX20
    
    **ä¿®æ­£:** è¡¨é ­ç´¢å¼•æ”¹ç‚º 2 (åŸç‚º 1)ã€‚
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX20"
    url = f"{base_url}?date={target_date}&response=csv"
        
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "4_MI_INDEX20")
    filename = OUTPUT_DIR + f"\\{target_date}_MI_INDEX20_Market.csv"
    
    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (4/10) MI_INDEX20 è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None
    
    # MI_INDEX20 å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 2
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (4/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_twtasu(target_date: str) -> Optional[pd.DataFrame]:
    """
    (5/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ TWTASU å ±å‘Š (æ¯æ—¥ç¸½æˆäº¤é‡å€¼èˆ‡å¹³å‡è‚¡åƒ¹)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/TWTASU
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/TWTASU"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "5_TWTASU")
    filename = OUTPUT_DIR + f"\\{target_date}_TWTASU_VolumePrice.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (5/10) TWTASU (ç¸½é‡å€¼) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # TWTASU å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=2)
   
    if df is not None and 'è­‰åˆ¸åç¨±' in df.columns: 
        df = df[df['è­‰åˆ¸åç¨±'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (5/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_bfiamu(target_date: str) -> Optional[pd.DataFrame]:
    """
    (6/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ BFIAMU å ±å‘Š (è‡ªç‡Ÿå•†è²·è³£è¶…å½™ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/BFIAMU
    
    **ä¿®æ­£:** è¡¨é ­ç´¢å¼•æ”¹ç‚º 3 (åŸç‚º 2)ã€‚
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/BFIAMU"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "6_BFIAMU")
    filename = OUTPUT_DIR + f"\\{target_date}_BFIAMU_DealerTrade.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (6/10) BFIAMU (è‡ªç‡Ÿå•†è²·è³£è¶…) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # BFIAMU å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'åˆ†é¡æŒ‡æ•¸åç¨±' in df.columns:
        df = df.dropna(how='all') 
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (6/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_fmtqik(target_date: str) -> Optional[pd.DataFrame]:
    """
    (7/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ FMTQIK å ±å‘Š (æ¯æ—¥åˆ¸å•†æˆäº¤é‡å€¼ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/afterTrading/FMTQIK
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/afterTrading/FMTQIK"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "7_FMTQIK")
    filename = OUTPUT_DIR + f"\\{target_date}_FMTQIK_BrokerVolume.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (7/10) FMTQIK (åˆ¸å•†æˆäº¤ç¸½è¡¨) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # FMTQIK å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 2
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'æ—¥æœŸ' in df.columns:
        df = df[df['æ—¥æœŸ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (7/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_bfi82u(target_date: str) -> Optional[pd.DataFrame]:
    """
    (8/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ BFI82U å ±å‘Š (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨ - æ—¥)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/BFI82U
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/fund/BFI82U"
    # åªä½¿ç”¨ dayDate é€²è¡Œæ—¥æœŸåƒæ•¸æ¨¡çµ„åŒ–
    url = f"{base_url}?type=day&dayDate={target_date}&response=csv"
        
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "8_BFI82U")
    filename = OUTPUT_DIR + f"\\{target_date}_BFI82U_3IParty_Day.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (8/10) BFI82U (ä¸‰å¤§æ³•äººæ—¥å ±) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # BFI82U å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'å–®ä½åç¨±' in df.columns:
        df = df[df['å–®ä½åç¨±'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (8/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_twt43u(target_date: str) -> Optional[pd.DataFrame]:
    """
    (9/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ TWT43U å ±å‘Š (å¤–è³‡åŠé™¸è³‡è²·è³£è¶…å½™ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/TWT43U
    
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/fund/TWT43U"
    url = f"{base_url}?date={target_date}&response=csv"

    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "9_TWT43U")
    filename = OUTPUT_DIR + f"\\{target_date}_TWT43U_ForeignTrade.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (9/10) TWT43U (å¤–è³‡è²·è³£è¶…) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # TWT43U å ±è¡¨çš„è¡¨é ­åœ¨ç´¢å¼• 3
    df = _read_twse_csv(response_text, header_row=2)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (9/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

def fetch_twse_twt44u(target_date: str) -> Optional[pd.DataFrame]:
    """
    (10/10) æŠ“å–æŒ‡å®šæ—¥æœŸçš„ TWT44U å ±å‘Š (æŠ•ä¿¡è²·è³£è¶…å½™ç¸½è¡¨)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/TWT44U
    """
    if not re.fullmatch(r'\d{8}', target_date): return None
    base_url = "https://www.twse.com.tw/rwd/zh/fund/TWT44U"
    url = f"{base_url}?date={target_date}&response=csv"
    
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "10_TWT44U")
    filename = OUTPUT_DIR + f"\\{target_date}_TWT44U_InvestmentTrust.csv"

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– (10/10) TWT44U (æŠ•ä¿¡è²·è³£è¶…) è³‡æ–™...")
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    df = _read_twse_csv(response_text, header_row=1)

    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… (10/10) {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    return None

# æŠ“å–æŒ‡å®šæ—¥æœŸçš„ T86 å ±å‘Š (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨ - ä¾é¡åˆ¥)ã€‚
def fetch_twse_t86(target_date: str) -> Optional[pd.DataFrame]:
    """
    æŠ“å–æŒ‡å®šæ—¥æœŸçš„ T86 å ±å‘Š (ä¸‰å¤§æ³•äººè²·è³£è¶…å½™ç¸½è¡¨ - ä¾é¡åˆ¥)ã€‚
    URL: https://www.twse.com.tw/rwd/zh/fund/T86
    
    :param target_date: æŸ¥è©¢æ—¥æœŸï¼Œæ ¼å¼ç‚º YYYYMMDD (ä¾‹å¦‚: 20251031)
    :return: åŒ…å«ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™çš„ DataFrameï¼Œå¦‚æœå¤±æ•—å‰‡ç‚º None
    """
    
    if not re.fullmatch(r'\d{8}', target_date): 
        print("æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYYMMDD æ ¼å¼ã€‚")
        return None
        
    # å®šç¾© URL çµæ§‹
    base_url = "https://www.twse.com.tw/rwd/zh/fund/T86"
    url = f"{base_url}?date={target_date}&selectType=ALL&response=csv"
    
    # å®šç¾©æª”æ¡ˆå„²å­˜è·¯å¾‘
    # å‡è¨­ "datas/raw/10_T86" æ˜¯ç›¸å°æ–¼æ­¤è…³æœ¬çš„ä½ç½®
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "11_T86")
    filename = os.path.join(OUTPUT_DIR, f"{target_date}_T86_InstitutionalTrades.csv")

    check_folder_and_create(filename)
    
    print(f"å˜—è©¦æŠ“å– T86 (ä¸‰å¤§æ³•äººè²·è³£è¶… - ä¾é¡åˆ¥) è³‡æ–™ï¼Œæ—¥æœŸ: {target_date}...")
    
    # 1. æŠ“å–è³‡æ–™
    response_text = _fetch_twse_data(url)
    if response_text is None: return None

    # 2. è§£æ CSV (header_row=1 è¡¨ç¤ºæ¬„ä½åç¨±åœ¨ç¬¬äºŒè¡Œ)
    # T86 è¡¨æ ¼çš„æ¬„ä½åç¨±é€šå¸¸åœ¨å›å‚³çš„ CSV å…§å®¹çš„ç¬¬äºŒè¡Œ
    df = _read_twse_csv(response_text, header_row=1)

    # 3. æ•¸æ“šæ¸…ç†èˆ‡å„²å­˜
    if df is not None and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
        # æ¸…é™¤æ²’æœ‰è­‰åˆ¸ä»£è™Ÿçš„ç©ºè¡Œ
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip() != '']
        
        # æ¸…ç†å¤šé¤˜çš„æè¿°è¡Œï¼ˆä¾‹å¦‚åº•éƒ¨çš„åˆè¨ˆè¡Œï¼Œå…¶è­‰åˆ¸ä»£è™Ÿæ¬„ä½å¯èƒ½ç‚ºç©ºï¼‰
        if 'æŠ•ä¿¡è²·è³£è¶…' in df.columns:
            # ç¢ºä¿æ•¸å­—æ¬„ä½å¯ä»¥è¢«è½‰æ›
            df['æŠ•ä¿¡è²·è³£è¶…'] = pd.to_numeric(df['æŠ•ä¿¡è²·è³£è¶…'], errors='coerce')
        
        # åˆªé™¤æ‰€æœ‰æ•¸å­—æ¬„ä½çš†ç‚º NaN çš„è¡Œ (å¯èƒ½æ˜¯åˆè¨ˆæˆ–ç„¡ç”¨è¨Šæ¯)
        df.dropna(subset=df.columns[2:], how='all', inplace=True)
        
        # å„²å­˜ CSV
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… {filename} å„²å­˜æˆåŠŸã€‚")
        return df
    
    print(f"âŒ æ•¸æ“šè™•ç†å¤±æ•—ï¼Œå¯èƒ½è©²æ—¥æœŸ ({target_date}) ç‚ºéäº¤æ˜“æ—¥æˆ–ç¶²ç«™è³‡æ–™çµæ§‹æ”¹è®Šã€‚")
    return None

# æ ¹æ“šæŒ‡å®šæ—¥æœŸèˆ‡æ™‚é–“ï¼ˆ21:00æˆªæ­¢ï¼‰æä¾›å¾€å‰6å€‹äº¤æ˜“æ—¥ï¼Œå‰ä¸€å€‹äº¤æ˜“æ—¥å‰‡ç‚ºdf[-1]
def get_previous_n_trading_days(
    file_path: str,
    datetime_to_check: str,
    n_days: int = 6,        # å¾€å‰æå‡º6å€‹äº¤æ˜“æ—¥
    CUTOFF_HOUR: int = 21,  # è¨­å®šæˆªæ­¢æ™‚é–“ç‚º 21:00
    date_column_name: str = 'æ—¥æœŸ') -> Union[List[str], None]:
    """
    æ ¹æ“šæŒ‡å®šæ—¥æœŸèˆ‡æ™‚é–“ï¼ˆ21:00æˆªæ­¢ï¼‰ç¢ºå®šä¸€å€‹æœ‰æ•ˆæŸ¥è©¢æ—¥æœŸï¼Œ
    ä¸¦å¾è©²æ—¥æœŸï¼ˆå«ï¼‰é–‹å§‹å‘å‰è¿½æº¯ N å€‹æœ€è¿‘çš„äº¤æ˜“æ—¥ã€‚
    Args:
        file_path (str): åŒ…å«äº¤æ˜“æ—¥æ¸…å–®çš„ CSV æª”æ¡ˆå®Œæ•´è·¯å¾‘ (å‡è¨­æª”æ¡ˆä¸­åˆ—å‡ºçš„æ˜¯äº¤æ˜“æ—¥)ã€‚
        datetime_to_check (str): è¦æª¢æŸ¥çš„æ—¥æœŸå’Œæ™‚é–“å­—ä¸²ï¼Œä¾‹å¦‚ '2025/10/10 15:30:00'ã€‚
        n_days (int): è¦ç²å–çš„ä¸Šä¸€å€‹äº¤æ˜“æ—¥çš„æ•¸é‡ (é è¨­ç‚º 6)ã€‚
        date_column_name (str): æª”æ¡ˆä¸­åŒ…å«æ—¥æœŸçš„æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æ—¥æœŸ'ã€‚
    Returns:
        Union[List[str], None]: åŒ…å« N å€‹äº¤æ˜“æ—¥å­—ä¸²ï¼ˆ'YYYY/MM/DD' æ ¼å¼ï¼‰çš„åˆ—è¡¨ï¼Œ
                                 æˆ–ç™¼ç”ŸéŒ¯èª¤æ™‚å›å‚³ Noneã€‚
    """
    
    # æª¢æŸ¥æª”æ¡ˆè·¯å¾‘
    if not os.path.exists(file_path):
        print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢º: {file_path}")
        return None
        
    try:
        # è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        if date_column_name not in df.columns:
            print(f"ã€éŒ¯èª¤ã€‘æª”æ¡ˆä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ—¥æœŸæ¬„ä½: '{date_column_name}'ã€‚æ¬„ä½æœ‰: {df.columns.tolist()}")
            return None
            
        # ç¢ºä¿æ—¥æœŸæ¬„ä½æ˜¯å­—ä¸²ï¼Œä»¥é¿å…æ ¼å¼ä¸ä¸€è‡´çš„å•é¡Œ
        df[date_column_name] = df[date_column_name].astype(str)

        # è¨­å®šæ—¥æœŸæ ¼å¼
        input_dt_format = '%Y/%m/%d %H:%M:%S'
        input_date_format = '%Y/%m/%d'
        
        # 1. è§£æè¼¸å…¥çš„æ—¥æœŸæ™‚é–“
        try:
            input_dt = datetime.strptime(datetime_to_check, input_dt_format)
        except ValueError:
            print(f"ã€éŒ¯èª¤ã€‘è¼¸å…¥æ—¥æœŸæ™‚é–“æ ¼å¼ä¸æ­£ç¢ºã€‚æ‡‰ç‚º '{input_dt_format}'ã€‚æ‚¨è¼¸å…¥çš„æ˜¯: {datetime_to_check}")
            return None
        
        input_date = input_dt.date()
        input_time = input_dt.time()
        
        # 2. æ ¹æ“šæ™‚é–“åˆ¤æ–·ã€Œæœ‰æ•ˆæŸ¥è©¢æ—¥æœŸã€
        # å¦‚æœæ™‚é–“åœ¨ 21:00 (å«) ä¹‹å¾Œï¼Œæœ‰æ•ˆæ—¥æœŸç‚ºä»Šå¤©ï¼›å¦å‰‡ç‚ºå‰ä¸€å¤©ã€‚
        cutoff_time = input_dt.replace(hour=CUTOFF_HOUR, minute=0, second=0, microsecond=0).time()
        
        effective_check_date = input_date
        
        if input_time < cutoff_time:
            # å¦‚æœåœ¨ 21:00 ä¹‹å‰ï¼Œè¦–ç‚ºå‰ä¸€å¤©çš„äº¤æ˜“
            effective_check_date = input_date - timedelta(days=1)
        
        # 3. è¿´åœˆå‘å‰å°‹æ‰¾æœ€è¿‘çš„ N å€‹äº¤æ˜“æ—¥
        current_check_date = effective_check_date
        trading_days_found: List[str] = []
        
        print(f"è¼¸å…¥æ—¥æœŸæ™‚é–“: {datetime_to_check}")
        print(f"èµ·å§‹æŸ¥è©¢æ—¥æœŸ (æ ¹æ“š {CUTOFF_HOUR}:00 æˆªæ­¢ç·šåˆ¤æ–·): {current_check_date.strftime(input_date_format)}")
        print(f"ç›®æ¨™ï¼šå‘å‰è¿½æº¯ {n_days} å€‹äº¤æ˜“æ—¥...")

        max_lookback_days = n_days * 3  # è¨­å®šæœ€å¤§è¿½æº¯å¤©æ•¸ï¼Œé¿å…ç„¡é™è¿´åœˆ
        days_passed = 0

        while len(trading_days_found) < n_days:
            
            # å®‰å…¨æ©Ÿåˆ¶æª¢æŸ¥
            if days_passed > max_lookback_days:
                print(f"ã€è­¦å‘Šã€‘å·²å‘å‰è¿½æº¯è¶…é {max_lookback_days} å¤© ({current_check_date.strftime(input_date_format)})ï¼Œå¯èƒ½è³‡æ–™æ¸…å–®ä¸å®Œæ•´ã€‚åœæ­¢å°‹æ‰¾ã€‚")
                break

            date_str = current_check_date.strftime(input_date_format)
            
            # ä½¿ç”¨ isin æª¢æŸ¥æ—¥æœŸæ˜¯å¦å­˜åœ¨æ–¼äº¤æ˜“æ—¥æ¸…å–®ä¸­
            is_trading_day = df[date_column_name].isin([date_str]).any()
            
            if is_trading_day:
                # æ‰¾åˆ°äº¤æ˜“æ—¥ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                trading_days_found.append(date_str)
                print(f"âœ… æ‰¾åˆ°ç¬¬ {len(trading_days_found)} å€‹äº¤æ˜“æ—¥: {date_str}")
            
            # ç„¡è«–æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼Œéƒ½å¾€å‰æ¨ä¸€å¤©ï¼Œç›´åˆ°æ‰¾åˆ°è¶³å¤ çš„æ•¸é‡
            current_check_date -= timedelta(days=1)
            days_passed += 1

        # å°‡åˆ—è¡¨åè½‰ï¼Œä½¿å…¶æŒ‰æ™‚é–“é †åºæ’åˆ— (å¦‚æœéœ€è¦çš„è©±ï¼Œé€šå¸¸æ˜¯å¾æœ€æ—©åˆ°æœ€è¿‘)
        # å¦‚æœå¸Œæœ›å¾æœ€è¿‘åˆ°æœ€èˆŠï¼Œå‰‡ä¸éœ€è¦åè½‰
        #trading_days_found.reverse() 

        # 4. åˆ¤æ–·ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ä¸¦å›å‚³çµæœ
        current_day_is_trading = df[date_column_name].isin([input_date.strftime(input_date_format)]).any()
        
        if current_day_is_trading:
             print(f"\nä»Šå¤©æ—¥æœŸ ({input_date.strftime(input_date_format)}) ç‚ºäº¤æ˜“æ—¥ã€‚")
        else:
             print(f"\nä»Šå¤©æ—¥æœŸ ({input_date.strftime(input_date_format)}) ç‚ºä¼‘å¸‚æ—¥ã€‚")
        
        
        # ç¢ºä¿åˆ—è¡¨æ˜¯å¾æœ€èˆŠåˆ°æœ€æ–°æ’åˆ—
        trading_days_found.reverse()

        if len(trading_days_found) == n_days:
            # å®Œæ•´æ”¶é›†åˆ° N å¤©
            print(f"âœ… æˆåŠŸæ”¶é›†åˆ° {n_days} å€‹äº¤æ˜“æ—¥ã€‚")
            return trading_days_found
        else:
            # æœªæ”¶é›†åˆ° N å¤© (é€šå¸¸æ˜¯æ•¸æ“šä¸è¶³)
            print(f"âš ï¸ åƒ…æ‰¾åˆ° {len(trading_days_found)} å€‹äº¤æ˜“æ—¥ï¼Œæ•¸é‡ä¸è¶³ {n_days} å€‹ã€‚")
            return trading_days_found # å³ä½¿ä¸è¶³ä¹Ÿå›å‚³æ‰¾åˆ°çš„çµæœ

    except pd.errors.EmptyDataError:
        print("ã€éŒ¯èª¤ã€‘æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚")
        return None
    except Exception as e:
        print(f"ã€éŒ¯èª¤ã€‘è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# è®€å–ã€æ¸…ç†å–®ä¸€ CSV æª”æ¡ˆï¼Œä¸¦ç¯©é¸å‡ºä¸‰å¤§æ³•äººè²·è¶…è‚¡æ•¸æœ€å¤§çš„ Top N è‚¡ç¥¨ã€‚
def _load_and_filter_single_day(
    file_path: str, 
    top_n: int,
    volume_column: str, 
    code_column: str
) -> Optional[pd.DataFrame]:
    """
    è®€å–ã€æ¸…ç†å–®ä¸€ CSV æª”æ¡ˆï¼Œä¸¦ç¯©é¸å‡ºä¸‰å¤§æ³•äººè²·è¶…è‚¡æ•¸æœ€å¤§çš„ Top N è‚¡ç¥¨ã€‚

    Args:
        file_path (str): å–®æ—¥ä¸‰å¤§æ³•äººè²·è³£è¶…æ•¸æ“šæª”æ¡ˆè·¯å¾‘ã€‚
        top_n (int): è¦ç¯©é¸å‡ºçš„å‰ N åæ•¸é‡ã€‚
        volume_column (str): è²·è³£è¶…è‚¡æ•¸æ¬„ä½åç¨±ã€‚
        code_column (str): è­‰åˆ¸ä»£è™Ÿæ¬„ä½åç¨±ã€‚

    Returns:
        Optional[pd.DataFrame]: åŒ…å« 'ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸' çš„ Top N DataFrameï¼Œè‹¥å¤±æ•—å‰‡ç‚º Noneã€‚
    """
    try:
        # è®€å– CSV æª”æ¡ˆ (å¤šç·¨ç¢¼å˜—è©¦)
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, encoding='big5')
            
        required_cols = [volume_column, code_column, 'è­‰åˆ¸åç¨±']
        if not all(col in df.columns for col in required_cols):
            return None

        # æ•¸æ“šæ¸…ç†èˆ‡è½‰æ›
        df[volume_column] = (
            df[volume_column].astype(str).str.replace(r'[",\s]', '', regex=True)
        )
        df[volume_column] = pd.to_numeric(df[volume_column], errors='coerce')
        df[code_column] = df[code_column].astype(str).str.strip()
        
        df.dropna(subset=[volume_column], inplace=True)
        
        # ç¯©é¸æ¢ä»¶ï¼š1. ä»£è™Ÿç‚º 4 ä½æ•¸å­— | 2. è²·è³£è¶…è‚¡æ•¸ > 1000 è‚¡ (è²·è¶…)
        df_filtered = df[
            (df[code_column].str.match(r'^\d{4}$')) & 
            (df[volume_column] > 1000) 
        ].copy()

        # æ’åºä¸¦å–å‡º Top N
        df_sorted = df_filtered.sort_values(
            by=volume_column, 
            ascending=False
        )
        
        top_n_data = df_sorted.head(top_n).rename(
            columns={code_column: 'ä»£è™Ÿ', 'è­‰åˆ¸åç¨±': 'åç¨±', volume_column: 'è‚¡æ•¸'}
        )
        
        return top_n_data[['ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸']]
        
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æª”æ¡ˆ -> {os.path.basename(file_path)}")
        return None
    except Exception as e:
        print(f"âŒ æ•¸æ“šè™•ç†å¤±æ•— ({os.path.basename(file_path)})ï¼š{e}")
        return None

# åˆ†ææœ€æ–°ä¸€æ—¥ä¸‰å¤§æ³•äººè²·è¶… Top N è‚¡ç¥¨åœ¨éå» N å¤©çš„å›æº¯è¶¨å‹¢
def analyze_top_stocks_trend(
    file_paths: List[str],
    top_n: int = 30, 
    n_days_lookback: int = 5, 
    volume_column: str = "ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸", 
    code_column: str = "è­‰åˆ¸ä»£è™Ÿ"
) -> Optional[str]:
    """
    åˆ†ææœ€æ–°ä¸€æ—¥ä¸‰å¤§æ³•äººè²·è¶… Top N è‚¡ç¥¨åœ¨éå» N å¤©çš„å›æº¯è¶¨å‹¢ã€‚
    è¼¸å‡ºçµæœä¸åŒ…å«åæ¬¡ï¼Œä¸¦ä¾ ä»£è™Ÿ, è­‰åˆ¸åç¨±, å›æº¯è¶¨å‹¢, è²·è¶…å¼µæ•¸ æ’åºã€‚
    å›æº¯è¶¨å‹¢çš„æ—¥æœŸæ¨™ç±¤å’Œæ¨™è¨˜çš†ä»¥ã€Œæœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥ã€çš„é †åºæ’åˆ—ã€‚

    Args:
        file_paths (List[str]): ä¾åºç‚º [æœ€æ–°æ—¥, å‰ä¸€æ—¥, ..., å‰ç¬¬ N æ—¥] çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ã€‚
        top_n (int): åŸºæº–æ—¥è¦ç¯©é¸å‡ºçš„å‰ N åæ•¸é‡ (é è¨­ 30)ã€‚
        n_days_lookback (int): è¦å›æº¯çš„äº¤æ˜“æ—¥å¤©æ•¸ (é è¨­ 5)ã€‚
        volume_column (str): è²·è³£è¶…è‚¡æ•¸æ¬„ä½åç¨±ã€‚
        code_column (str): è­‰åˆ¸ä»£è™Ÿæ¬„ä½åç¨±ã€‚

    Returns:
        Optional[str]: æ ¼å¼åŒ–è¼¸å‡ºè¶¨å‹¢çµæœï¼Œæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
    """
    
    # ç¢ºä¿æœ‰è¶³å¤ çš„æª”æ¡ˆé€²è¡ŒåŸºæº–æ—¥ + å›æº¯æ—¥åˆ†æ
    required_files = n_days_lookback + 1
    if len(file_paths) < required_files:
        print(f"âš ï¸ éŒ¯èª¤ï¼šè‡³å°‘éœ€è¦ {required_files} å€‹æª”æ¡ˆ (åŸºæº–æ—¥ + {n_days_lookback} å€‹å›æº¯æ—¥)ã€‚ç›®å‰åªæœ‰ {len(file_paths)} å€‹ã€‚")
        return None

    # --- 1. è™•ç†æ‰€æœ‰äº¤æ˜“æ—¥æ•¸æ“š ---
    all_day_data: Dict[int, Set[str]] = {}
    day_labels: List[str] = []
    df_base_day: Optional[pd.DataFrame] = None 

    print(f"ğŸ” é–‹å§‹è™•ç† {required_files} å¤©æ•¸æ“š...")
    
    for i, path in enumerate(file_paths[:required_files]):
        df_day = _load_and_filter_single_day(path, top_n, volume_column, code_column)
        
        # æå–æª”æ¡ˆåç¨±ä¸­çš„æ—¥æœŸä½œç‚ºæ¨™ç±¤
        try:
            # å‡è¨­æª”æ¡ˆåç¨±åŒ…å«æ—¥æœŸï¼Œä¾‹å¦‚ "20251110_institutional.csv"
            date_label = os.path.basename(path).split('_')[0][:8]
        except:
            date_label = f"Day-{i}"
            
        if df_day is None or df_day.empty:
            print(f"âš ï¸ {date_label} æ•¸æ“šè¼‰å…¥æˆ–ç¯©é¸å¤±æ•—ï¼Œå°‡ç•¥éæ­¤æ—¥ã€‚")
            all_day_data[i] = set()
        else:
            # å°‡è©²æ—¥æœŸçš„ Top N è‚¡ç¥¨ä»£è™Ÿå­˜å„²ç‚ºé›†åˆ (Set)
            all_day_data[i] = set(df_day['ä»£è™Ÿ'].tolist())
            print(f"âœ… {date_label} æˆåŠŸç¯©é¸å‡º {len(all_day_data[i])} æª”è‚¡ç¥¨ã€‚")

        day_labels.append(date_label)
        
        # åŸºæº–æ—¥ (æœ€æ–°æ—¥) çš„æ•¸æ“šéœ€è¦å–®ç¨ä¿å­˜
        if i == 0:
            df_base_day = df_day
    
    if df_base_day is None or df_base_day.empty:
        print("âŒ åŸºæº–æ—¥ (æœ€æ–°æ—¥) æ•¸æ“šç„¡æ•ˆæˆ–ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œè¶¨å‹¢åˆ†æã€‚")
        return None
        
    # --- 2. ç²å–åŸºæº–æ—¥ Top N è‚¡ç¥¨ä»£è™Ÿ (æ­¤ç‚ºåˆ†æçš„ä¸»é«”) ---
    base_stocks = df_base_day[['ä»£è™Ÿ', 'åç¨±', 'è‚¡æ•¸']].head(top_n).copy()
    
    # --- 3. åŸ·è¡Œå›æº¯è¶¨å‹¢åˆ†æ ---
    
    # å»ºç«‹åˆ—è¡¨ç”¨æ–¼å­˜æ”¾æ¯å¤©çš„å›æº¯æ¨™è¨˜ (Series) å’Œæ—¥æœŸæ¨™ç±¤
    # åˆå§‹é †åºç‚º i=1 åˆ° n_days_lookback (æœ€æ–°å›æº¯æ—¥åˆ°æœ€èˆŠå›æº¯æ—¥)
    trend_header_parts: List[str] = []
    trend_marker_series: List[pd.Series] = []
    
    for i in range(1, n_days_lookback + 1):
        # å›æº¯æ—¥æœŸçš„æ¨™ç±¤
        day_tag = day_labels[i].replace("/", "")[-4:] # å–æ—¥æœŸå¾Œå››ç¢¼ (å¦‚ 1110)
        trend_header_parts.append(day_tag)
        
        # å–å¾—è©²å›æº¯æ—¥æœŸçš„ Top N è‚¡ç¥¨ä»£è™Ÿé›†åˆ
        past_top_n_codes = all_day_data.get(i, set())
        
        # å°åŸºæº–æ—¥çš„æ¯ä¸€æª”è‚¡ç¥¨é€²è¡Œæª¢æŸ¥
        presence_markers = []
        for code in base_stocks['ä»£è™Ÿ']:
            # æª¢æŸ¥ä»£è™Ÿæ˜¯å¦å‡ºç¾åœ¨éå» Top N åˆ—è¡¨ä¸­
            if code in past_top_n_codes:
                presence_markers.append("ğŸ”´") # å­˜åœ¨ (å¤šåŠ ä¸€å€‹ç©ºæ ¼ä»¥ç¢ºä¿é–“éš”ä¸€è‡´)
            else:
                presence_markers.append("âšªï¸") # ä¸å­˜åœ¨
        
        # å°‡æ¨™è¨˜ Series å­˜å…¥åˆ—è¡¨
        trend_marker_series.append(pd.Series(presence_markers, index=base_stocks.index))
        
    # *** é—œéµä¿®æ”¹ï¼šå°‡åˆ—è¡¨åè½‰ï¼Œä½¿é †åºè®Šç‚ºã€Œæœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥ã€ ***
    trend_header_parts.reverse()
    trend_marker_series.reverse()
    
    # å»ºç«‹æœ€çµ‚çš„å›æº¯è¶¨å‹¢æ¨™é ­ (æœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥)
    trend_header = " ".join(trend_header_parts)
    
    # ä¸²æ¥æ‰€æœ‰çš„è¶¨å‹¢æ¨™è¨˜ Series (æœ€èˆŠæ—¥åˆ°æœ€æ–°æ—¥)
    if trend_marker_series:
        # ä½¿ç”¨ copy() ç¢ºä¿ç¨ç«‹æ€§
        base_stocks['è¶¨å‹¢'] = trend_marker_series[0].copy() 
        for series in trend_marker_series[1:]:
            base_stocks['è¶¨å‹¢'] += series
    else:
        base_stocks['è¶¨å‹¢'] = pd.Series("", index=base_stocks.index) # é¿å…ç©ºåˆ—è¡¨éŒ¯èª¤

    # ç§»é™¤è¶¨å‹¢æ¬„ä½å°¾éƒ¨å¤šé¤˜çš„ç©ºæ ¼
    base_stocks['è¶¨å‹¢'] = base_stocks['è¶¨å‹¢'].str.strip()
    
    # --- 4. æ ¼å¼åŒ–è¼¸å‡ºçµæœ ---
    
    # å°‡è‚¡æ•¸è½‰æ›ç‚ºå¼µæ•¸ä¸¦æ ¼å¼åŒ–
    volume_col_display_name = 'è²·è¶…å¼µæ•¸'
    base_stocks[volume_col_display_name] = base_stocks['è‚¡æ•¸'].apply(lambda x: f"{int(x / 1000):,}")
    
    # ç¸½å¯¬åº¦èª¿æ•´ (é…åˆæ–°é †åºèˆ‡æ¬„ä½)
    TOTAL_WIDTH = 28
    
    # å»ºç«‹è¡¨æ ¼æ¨™é ­ - ç§»é™¤åæ¬¡ï¼Œèª¿æ•´é †åº: ä»£è™Ÿ | è­‰åˆ¸åç¨± | å›æº¯è¶¨å‹¢ | è²·è¶…å¼µæ•¸
    
    output_lines = [
        f"\n*******************************"
        f"\n   ğŸ“ˆ ä¸‰å¤§æ³•äººè²·è¶…Top{top_n}\nåŸºæº–æ—¥:{day_labels[0]}-éå»{n_days_lookback}æ—¥è¶¨å‹¢"
        f"\n*******************************",
    #    f"{'ä»£è™Ÿ'.center(6)} | {'è­‰åˆ¸åç¨±'.center(6)} | å›æº¯è¶¨å‹¢ > > > > {day_labels[0][-4:]}  | {'è²·è¶…å¼µæ•¸'.center(8)}", 
    #    "-" * TOTAL_WIDTH
    ]
    
    # å»ºç«‹è¡¨æ ¼å…§å®¹
    for index, row in base_stocks.iterrows():
        #code_str = str(row['ä»£è™Ÿ']).center(6)
        name_str = row['åç¨±'].ljust(4, 'ã€€') # ä¸­æ–‡ä½”ç”¨å¯¬åº¦è™•ç†
        volume_str = row[volume_col_display_name].rjust(6) # å³å°é½Šæ•¸å­—
        trend_str = row['è¶¨å‹¢'] 

        show_width_in_stock_name = 4
        if len(name_str.replace(' ', '')) < show_width_in_stock_name :
            padding_width = show_width_in_stock_name - len(name_str.replace(' ', ''))
            name_str = name_str.replace(' ', '') + '  ' * padding_width
        else:
            name_str = name_str.replace(' ', '')

        # è¼¸å‡ºé †åº: ä»£è™Ÿ | è­‰åˆ¸åç¨± | å›æº¯è¶¨å‹¢ | è²·è¶…å¼µæ•¸
        #output_lines.append(f"{code_str} | {name_str} | {trend_str} | {volume_str}")
        output_lines.append(f"{name_str}{trend_str}({volume_str}å¼µ)")
        
        #print(f"âœ… {name_str.replace('  ', '')}")
        #sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ
    output_lines.append("=" * TOTAL_WIDTH)
    output_lines.append(f"ğŸ”´: è©²æ—¥å‡ºç¾åœ¨ Top {top_n} åå–®ä¸­ \nâšªï¸: è©²æ—¥æœªå‡ºç¾åœ¨ Top {top_n} åå–®ä¸­")
    
    return "\n".join(output_lines)

# æŠ“å–ç¶²è·¯è³‡æ–™ "get_1-11-ä¸Šå¸‚è‚¡ç¥¨.py"
# ==========================================================
CODE_DIR = os.path.dirname(os.path.abspath(__file__))
STOCKS_ALL_CSV = os.path.join(CODE_DIR, "datas", "raw", "stocks_all.csv")
# äº¤æ˜“æ—¥æ¸…å–®æª”æ¡ˆè·¯å¾‘ (åƒ…ç”¨æ–¼è¼”åŠ©åˆ¤æ–·ä»Šæ—¥æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼Œä¸å†ç”¨æ–¼ç”Ÿæˆæ­·å²ç¯„åœ)
CSV_FILE_PATH = os.path.join(CODE_DIR, "datas", "processed", "get_holidays", "trading_day_2021-2025.csv")


# --- è¼”åŠ©å‡½æ•¸ ---
# å¾äº¤æ˜“æ—¥æ¸…å–®ä¸­æ‰¾åˆ°ç•¶å‰æ—¥æœŸçš„ã€å‰ä¸€å€‹äº¤æ˜“æ—¥ã€‘ã€‚
def _get_previous_trading_day(file_path: str, current_date: datetime.date) -> Optional[datetime.date]:
    
    try:
        # è®€å–äº¤æ˜“æ—¥æ¸…å–®
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        date_column = df.columns[0]
        
        # 1. çµ±ä¸€è½‰æ›æ—¥æœŸæ ¼å¼ç‚º YYYYMMDD å­—ä¸²
        trading_days_ymd = []
        for date_str in df[date_column].astype(str).str.strip().tolist():
            try:
                # å˜—è©¦ä½¿ç”¨ YYYY/MM/DD æ ¼å¼è§£æ (æ ¹æ“šæ‚¨çš„éŒ¯èª¤è¨Šæ¯)
                dt_obj = datetime.strptime(date_str, "%Y/%m/%d").date()
            except ValueError:
                try:
                    # å˜—è©¦ä½¿ç”¨ YYYYMMDD æ ¼å¼è§£æ (ä½œç‚ºå‚™ç”¨æˆ–æ¨™æº–æ ¼å¼)
                    dt_obj = datetime.strptime(date_str, "%Y%m%d").date()
                except ValueError:
                    # å¿½ç•¥ç„¡æ³•è­˜åˆ¥çš„æ ¼å¼
                    continue 
            
            # å°‡æ‰€æœ‰æ—¥æœŸçµ±ä¸€è½‰æ›ç‚º YYYYMMDD å­—ä¸²æ ¼å¼é€²è¡Œæ¯”è¼ƒ
            trading_days_ymd.append(dt_obj.strftime("%Y%m%d"))
            
        all_trading_days = sorted(list(set(trading_days_ymd)))
        
        # 2. é€²è¡Œæ¯”è¼ƒ
        current_date_str = current_date.strftime("%Y%m%d")
        
        # æ‰¾åˆ°æ‰€æœ‰æ¯”ä»Šå¤©æ—¥æœŸå°çš„äº¤æ˜“æ—¥
        previous_trading_days = [
            d for d in all_trading_days if d < current_date_str
        ]
        
        if previous_trading_days:
            # è¿”å›å…¶ä¸­æœ€å¤§çš„ä¸€å€‹ (å³æœ€è¿‘çš„ä¸€å€‹äº¤æ˜“æ—¥)
            # å› ç‚º previous_trading_days å·²ç¶“æ˜¯æ’åºå¥½çš„ YYYYMMDD å­—ä¸²åˆ—è¡¨
            return datetime.strptime(previous_trading_days[-1], "%Y%m%d").date()
        else:
            print("âš ï¸ éŒ¯èª¤ï¼šäº¤æ˜“æ—¥æ¸…å–®ä¸­æ‰¾ä¸åˆ°å‰ä¸€å€‹äº¤æ˜“æ—¥ã€‚")
            return None

    except FileNotFoundError:
        print(f"è‡´å‘½éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°äº¤æ˜“æ—¥æ¸…å–®æª”æ¡ˆ {file_path}ã€‚")
        return None
    except Exception as e:
        # æ•æ‰å…¶ä»–å¯èƒ½çš„éŒ¯èª¤ï¼Œä¸¦å°å‡ºï¼Œä½†å¸Œæœ›åœ¨å…§éƒ¨è™•ç†æ‰ ValueError
        print(f"è‡´å‘½éŒ¯èª¤ï¼šè™•ç†äº¤æ˜“æ—¥æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    
# æª¢æŸ¥ä¸¦å»ºç«‹æ‰€éœ€çš„ã€è³‡æ–™å¤¾ã€‘
def _check_folder_and_create(filepath: str):
    
    pathlib.Path(filepath).parent.mkdir(parents=True, exist_ok=True)

# æ ¹æ“š 21:00 åŸºæº–åˆ¤æ–·ç›®æ¨™æ—¥æœŸ
def _get_target_date_and_month() -> Dict[str, Optional[str]]:
    """
    æ ¹æ“š 21:00 åŸºæº–åˆ¤æ–·ç›®æ¨™æ—¥æœŸï¼š
    1. 21:00 ä¹‹å¾Œï¼šæŠ“å–ä»Šå¤© (å¯¦éš›æ—¥æœŸ) çš„è³‡æ–™ã€‚
    2. 21:00 ä¹‹å‰ï¼šæŠ“å–ã€å‰ä¸€å€‹äº¤æ˜“æ—¥ã€‘çš„è³‡æ–™ã€‚
    """
    now = datetime.now()
    cutoff_time = datetime.strptime("21:00:00", "%H:%M:%S").time()
    
    if now.time() >= cutoff_time:
        # æƒ…æ³ 1: 21:00 ä¹‹å¾Œ -> æŠ“å–ä»Šå¤© (å¯¦éš›æ—¥æœŸ)
        target_date_dt = now.date()
        print(f"ã€æ—¥æœŸåˆ¤æ–·ã€‘{now.strftime('%H:%M:%S')} æ™šæ–¼ 21:00ï¼Œç›®æ¨™æ—¥ç‚ºä»Šå¤© ({target_date_dt.strftime('%Y/%m/%d')})ã€‚")

    else:
        # æƒ…æ³ 2: 21:00 ä¹‹å‰ -> æŠ“å–å‰ä¸€å€‹äº¤æ˜“æ—¥
        
        # å…ˆæ‰¾åˆ°ã€æ—¥æ›†ä¸Šçš„æ˜¨å¤©ã€‘
        yesterday_dt = (now - timedelta(days=1)).date()
        
        # ç„¶å¾Œå¾äº¤æ˜“æ—¥æ¸…å–®ä¸­æ‰¾åˆ°æœ€è¿‘çš„äº¤æ˜“æ—¥
        previous_trading_day_dt = _get_previous_trading_day(CSV_FILE_PATH, now.date())

        if previous_trading_day_dt:
            target_date_dt = previous_trading_day_dt
            print(f"ã€æ—¥æœŸåˆ¤æ–·ã€‘{now.strftime('%H:%M:%S')} æ—©æ–¼ 21:00ï¼Œç›®æ¨™æ—¥ç‚ºå‰ä¸€å€‹äº¤æ˜“æ—¥ ({target_date_dt.strftime('%Y/%m/%d')})ã€‚")
        else:
            print("âŒ éŒ¯èª¤ï¼šç„¡æ³•ç¢ºå®šå‰ä¸€å€‹äº¤æ˜“æ—¥ï¼Œæ‰€æœ‰æ—¥å ±ä»»å‹™å°‡è·³éã€‚")
            return {
                "daily_date": None,
                "monthly_date": None
            }

    # ç´€éŒ„é–‹å§‹çš„æ™‚é–“          
    start_time = datetime.now().strftime('%H:%M:%S')
 
    # é‡å°æ—¥å ±ï¼š
    final_daily_date = target_date_dt.strftime("%Y%m%d")
    
    # é‡å° STOCK_DAYï¼šåªéœ€è¦ç›®æ¨™æ—¥æœŸæ‰€åœ¨æœˆä»½çš„ä»£è¡¨æ—¥æœŸ (YYYYMM01)
    current_month_date = target_date_dt.strftime("%Y%m") + "01"

    print(f"ã€æœ€çµ‚ç›®æ¨™ã€‘æ—¥å ±æŠ“å–æ—¥æœŸ: {final_daily_date}")
    print(f"ã€æœ€çµ‚ç›®æ¨™ã€‘STOCK_DAYæœˆä»½: {current_month_date[:6]}")

    return {
        "daily_date": final_daily_date,
        "monthly_date": current_month_date,
        "start_time": start_time
    }

# å¾ stocks_all.csv è®€å–è‚¡ç¥¨æ¸…å–®ï¼Œä¸¦ä¾æ“šã€å¸‚å ´åˆ¥ã€‘æ¬„ä½ç¯©é¸å‡ºã€Œä¸Šå¸‚ã€å…¬å¸ã€‚
def get_stock_list(file_path: str) -> Optional[List[str]]:

    try:
        # è®€å–æ•´å€‹ CSV æª”æ¡ˆ
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        # 1. å°‹æ‰¾ã€å¸‚å ´åˆ¥ã€‘æ¬„ä½
        # å˜—è©¦å¾æ¬„ä½åç¨±ä¸­æ‰¾å‡ºåŒ…å« "å¸‚å ´åˆ¥"ã€"é¡åˆ¥" æˆ– "å¸‚å ´" çš„æ¬„ä½
        market_col = None
        for col in df.columns:
            if "å¸‚å ´åˆ¥" in col or "å¸‚å ´" in col or "é¡åˆ¥" in col:
                market_col = col
                break
        
        if market_col is None:
            # è­¦å‘Šï¼šå¦‚æœæ‰¾ä¸åˆ°å¸‚å ´åˆ¥æ¬„ä½ï¼Œå‰‡é€€å›åˆ°åªæŠ“å– 4 ä½æ•¸å­—çš„ä»£è™Ÿï¼ˆé¿å…æŠ“å–å…¨éƒ¨ï¼‰
            print("è­¦å‘Šï¼šæ‰¾ä¸åˆ°åŒ…å« 'å¸‚å ´åˆ¥' æˆ– 'å¸‚å ´' å­—æ¨£çš„æ¬„ä½ï¼Œå°‡é€€å›åƒ…ç¯©é¸ 4 ä½æ•¸å­—ä»£è™Ÿã€‚")
            
            stock_list = df.iloc[:, 0].astype(str).str.strip().tolist()
            filtered_stocks = [
                s for s in stock_list 
                if re.fullmatch(r'^\d{4}$', s)
            ]
            
        else:
            # 2. æ‰¾åˆ°å¸‚å ´åˆ¥æ¬„ä½ï¼Œé€²è¡Œç¯©é¸
            
            # æ¸…ç†å¸‚å ´åˆ¥æ¬„ä½çš„å­—ä¸²ï¼Œä¸¦ç¯©é¸å‡ºåŒ…å«ã€Œä¸Šå¸‚ã€å­—æ¨£çš„è¡Œ
            df_listed = df[
                df[market_col].astype(str).str.strip().str.contains("ä¸Šå¸‚", na=False)
            ].copy() # ä½¿ç”¨ copy é¿å… SettingWithCopyWarning
            
            # 3. å–å¾—ç¬¬ä¸€æ¬„çš„è‚¡ç¥¨ä»£è™Ÿ
            # å‡è¨­è‚¡ç¥¨ä»£è™Ÿæ˜¯ç¬¬ä¸€æ¬„ (ç´¢å¼• 0)
            stock_list = df_listed.iloc[:, 0].astype(str).str.strip().tolist()
            
            # é¡å¤–æª¢æŸ¥ï¼šç¢ºä¿ä»£è™Ÿæ˜¯æœ‰æ•ˆçš„æ•¸å­—æ ¼å¼ï¼ˆé€šå¸¸æ˜¯ 4 ä½ç´”æ•¸å­—ï¼‰
            filtered_stocks = [s for s in stock_list if re.fullmatch(r'\d{4,6}', s)]
            
        
        if not filtered_stocks:
            print("éŒ¯èª¤: ä¾æ“šã€Œå¸‚å ´åˆ¥ã€ç¯©é¸å¾Œï¼Œæ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆæ¢ä»¶çš„ä¸Šå¸‚å…¬å¸ä»£è™Ÿã€‚")
            return None
            
        print(f"--- æˆåŠŸä¾æ“šã€Œå¸‚å ´åˆ¥ã€ç¯©é¸ï¼Œè®€å– {len(filtered_stocks)} å€‹ä¸Šå¸‚å…¬å¸ä»£è™Ÿ ---")
        print(filtered_stocks)
        return filtered_stocks
    except pd.errors.EmptyDataError:
        print(f"éŒ¯èª¤: è‚¡ç¥¨æ¸…å–®æª”æ¡ˆ {file_path} ç‚ºç©ºã€‚")
        return None
    except Exception as e:
        print(f"éŒ¯èª¤: è®€å–æˆ–è™•ç†è‚¡ç¥¨æ¸…å–®æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# å°‡ TWSE è¿”å›çš„æ–‡æœ¬è§£æç‚º Pandas DataFrameã€‚
def _read_twse_csv(response_text: str, header_row: int = 1, first_col_name: Optional[str] = None) -> Optional[pd.DataFrame]:
    
    try:
        data = StringIO(response_text)
        df = pd.read_csv(data, 
                         header=header_row, 
                         encoding='utf-8-sig', 
                         skipinitialspace=True,
                         engine='python',
                         on_bad_lines='skip' 
        )
        if not df.empty:
            df.columns = df.columns.str.strip()
            df.dropna(axis=1, how='all', inplace=True)
            
            if df.empty: return None
            
            if first_col_name in df.columns:
                 df = df[df[first_col_name].astype(str).str.strip() != '']
                
            return df
        return None
    except Exception as e:
        return None

# --- é€šç”¨æ—¥å ±æŠ“å–ä¸»å‡½æ•¸ (åƒ…æŠ“å–ç•¶æ—¥) ---
def fetch_single_daily_report(
    target_date: str, 
    base_url: str, 
    output_folder_num: str,
    output_filename_suffix: str,
    url_params: str = "",
    first_col_name: Optional[str] = None,
    header_row: int = 1
):
    """
    æŠ“å–å–®æ—¥å ±è¡¨æ•¸æ“šçš„é€šç”¨å‡½æ•¸ï¼Œå…·å‚™æª”æ¡ˆå­˜åœ¨å³è·³éçš„æ©Ÿåˆ¶ã€‚
    """
    
    OUTPUT_DIR = os.path.join(CODE_DIR, "datas", "raw", output_folder_num)
    filename = os.path.join(OUTPUT_DIR, f"{target_date}{output_filename_suffix}.csv")
    _check_folder_and_create(filename)

    print(f"\n--- ğŸš€ è™•ç† {output_folder_num} ({target_date}) ---")

    # 1. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(filename):
        print(f" Â â„¹ï¸ {target_date} è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éã€‚")
        return # è·³éï¼Œä¸åŸ·è¡Œå»¶é²

    # 2. æª”æ¡ˆä¸å­˜åœ¨ï¼Œé–‹å§‹åŸ·è¡ŒæŠ“å–å’Œé‡è©¦
    is_successful = False
    max_attempts = 4
    for attempt in range(1, max_attempts + 1):
        
        url = f"{base_url}?date={target_date}{url_params}&response=csv"
        
        # åŸ·è¡ŒæŠ“å–
        response_text = _fetch_twse_data(url)
        df = None
        if response_text is not None:
            df = _read_twse_csv(response_text, header_row=header_row, first_col_name=first_col_name)

        if df is not None:
            # æˆåŠŸå„²å­˜
            try:
                df.to_csv(filename, index=False, encoding='utf-8-sig')
                print(f" Â âœ… {target_date} è³‡æ–™å„²å­˜æˆåŠŸã€‚")
                is_successful = True
                break
            except Exception as e:
                print(f"âŒ {target_date} è³‡æ–™å„²å­˜å¤±æ•—: {e}")
                break

        # å¤±æ•—è™•ç†
        if attempt < max_attempts:
            delay_seconds = attempt * 5 
            print(f"ğŸš¨ æŠ“å–å¤±æ•— (ç¬¬ {attempt} æ¬¡)ã€‚å°‡åœ¨ {delay_seconds} ç§’å¾Œé‡è©¦...")
            time_module.sleep(delay_seconds)
        else:
            print(f"âŒ {target_date} è³‡æ–™ç¶“é {max_attempts} æ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—ã€‚")
            break
    
    # 3. åªæœ‰åœ¨é€²è¡Œäº†ç¶²è·¯æŠ“å–æˆ–é‡è©¦ä¹‹å¾Œï¼Œæ‰éœ€è¦ç­‰å¾… 2 ç§’
    if is_successful or attempt == max_attempts:
        time_module.sleep(2)

def _fetch_twse_data(url: str) -> Optional[str]:
    """å˜—è©¦å¾ TWSE æŠ“å–è³‡æ–™ï¼Œä¸¦è¿”å›åŸå§‹æ–‡æœ¬ã€‚"""
    try:
        headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status() 
        response.encoding = 'Big5'
        
        if "å¾ˆæŠ±æ­‰" in response.text or "æŸ¥ç„¡ç›¸é—œè³‡æ–™" in response.text:
            return None
        
        return response.text
        
    except requests.exceptions.HTTPError:
        return None 
    except requests.exceptions.RequestException as err:
        print(f"âŒ é€£ç·šæˆ– Requests éŒ¯èª¤: {err}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")
        
    return None

# --- ä»»å‹™ 1: STOCK_DAY ç¨ç«‹è™•ç† (ç•¶æœˆ/è¦†è“‹) ---
# æŠ“å–ç•¶æœˆæ‰€æœ‰è‚¡ç¥¨çš„ STOCK_DAY è³‡æ–™ï¼Œä¸¦ç›´æ¥è¦†è“‹æª”æ¡ˆã€‚
def fetch_twse_stock_day_single_month(month_date: str, stock_list: List[str]):

    print(f"\n--- ğŸš€ é–‹å§‹ STOCK_DAY æŠ“å– ({month_date[:6]}) (å°‡ç›´æ¥è¦†è“‹) ---")
    
    OUTPUT_BASE_DIR = os.path.join(CODE_DIR, "datas", "raw", "1_STOCK_DAY")
    BASE_URL = "https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY"
    
    tasks_successful = 0
    tasks_failed = 0
    
    for stock_no in stock_list:
        output_dir = os.path.join(OUTPUT_BASE_DIR, stock_no)
        month_str = month_date[:6]
        filename = os.path.join(output_dir, f"{month_str}_{stock_no}_STOCK_DAY.csv") 
        _check_folder_and_create(filename)
        
        is_successful = False
        max_attempts = 4
        for attempt in range(1, max_attempts + 1):
            
            url = f"{BASE_URL}?date={month_date}&stockNo={stock_no}&response=csv"
            
            # 1. æŠ“å–æ•¸æ“š
            response_text = _fetch_twse_data(url)
            df = None
            if response_text is not None:
                df = _read_twse_csv(response_text, header_row=1, first_col_name='æ—¥æœŸ') 
            
            if df is not None and not df.empty:
                # 2. å„²å­˜è³‡æ–™ (ç›´æ¥è¦†è“‹)
                try:
                    df.to_csv(filename, index=False, encoding='utf-8-sig')
                    print(f" Â âœ… {stock_no} | {month_str} è³‡æ–™å·²è¦†è“‹å„²å­˜ã€‚")
                    tasks_successful += 1
                    is_successful = True
                    break
                except Exception as e:
                    print(f"âŒ {stock_no} | {month_str} è³‡æ–™å„²å­˜å¤±æ•—: {e}")
                    tasks_failed += 1
                    break
            
            # æŠ“å–å¤±æ•— (df is None)
            if attempt < max_attempts:
                delay_seconds = attempt * 5 
                print(f"ğŸš¨ {stock_no} | {month_str} æŠ“å–å¤±æ•—ã€‚å°‡åœ¨ {delay_seconds} ç§’å¾Œé‡è©¦...")
                time_module.sleep(delay_seconds)
            else:
                print(f"âŒ {stock_no} | {month_str} è³‡æ–™ç¶“é {max_attempts} æ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—ï¼Œè·³éæ­¤è‚¡ç¥¨ã€‚")
                tasks_failed += 1
                break
                
        # 3. æ¯æ¬¡å˜—è©¦ç¶²è·¯è«‹æ±‚å¾Œï¼Œç­‰å¾… 2 ç§’ (ç„¡è«–æˆåŠŸæˆ–å¤±æ•—)
        if is_successful or attempt == max_attempts:
            time_module.sleep(2)
            
    print(f"\n--- ğŸ STOCK_DAY æŠ“å–çµæŸã€‚æˆåŠŸè¦†è“‹: {tasks_successful}, å¤±æ•—: {tasks_failed} ---")
# ==========================================================

def get_day_stock_details(
    day_roll1: str,
    target_stock_name: str,
    base_dir: pathlib.Path,
    get_price_before: Optional[str],
    csv_name_column: str,
    csv_price_column: str
) -> Dict[str, Any]:
    """
    ç²å–å–®ä¸€äº¤æ˜“æ—¥ (day_roll1) ç‰¹å®šè‚¡ç¥¨ (target_stock_name) çš„è©³ç´°è³‡è¨Šï¼Œ
    åŒ…æ‹¬æ”¶ç›¤åƒ¹ã€æ¼²è·Œå¹…ã€ä¸‰å¤§æ³•äººè²·è³£è¶…ã€ä»¥åŠå€‹è‚¡æŒ‡æ¨™ã€‚
    
    Args:
        day_roll1: ç•¶å‰äº¤æ˜“æ—¥ (YYYYMMDD æ ¼å¼)ã€‚
        target_stock_name: è‚¡ç¥¨åç¨±ã€‚
        base_dir: å°ˆæ¡ˆåŸºç¤è·¯å¾‘ã€‚
        get_price_before: å‰ä¸€äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ (å­—ä¸²æˆ– None)ã€‚
        csv_name_column: CSV ä¸­ç”¨æ–¼æ¯”å°çš„è‚¡ç¥¨åç¨±æ¬„ä½åç¨±ã€‚
        csv_price_column: CSV ä¸­æ”¶ç›¤åƒ¹çš„æ¬„ä½åç¨±ã€‚

    Returns:
        åŒ…å«ç•¶æ—¥æ‰€æœ‰è™•ç†çµæœçš„å­—å…¸ã€‚
    """
    
    # ------------------ 1. å®šç¾©æª”æ¡ˆè·¯å¾‘ ------------------
    
    # åƒ¹æ ¼èˆ‡æŒ‡æ¨™ CSV è·¯å¾‘ (3_BWIBBU_d)
    csv_price_path = base_dir / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll1}_BWIBBU_d_IndexReturn.csv"
    # ä¸‰å¤§æ³•äººè²·è³£è¶… CSV è·¯å¾‘ (11_T86)
    csv_volume_path = base_dir / "datas" / "raw" / "11_T86" / f"{day_roll1}_T86_InstitutionalTrades.csv"

    # åˆå§‹åŒ–çµæœå­—å…¸
    result = {
        'day_mmdd': f"{day_roll1[4:6]}/{day_roll1[-2:]}",
        'get_price': None,
        'price_percent': 0.0,
        'price_percent_formatted': "0.0%",
        'net_volume_data': "0", # é è¨­ç‚º "0" (ä¸å¸¶å¼µå­—ï¼Œå¾Œé¢çµ±ä¸€åŠ ä¸Š)
        'pa_ratio': "-",
        'pe_ratio': "-",
        'pb_ratio': "-"
    }

    # ------------------ 2. ç²å–æ”¶ç›¤åƒ¹ ------------------

    # å‡è¨­ lookup_stock_price è¿”å›å­—ä¸²æˆ– None
    get_price = lookup_stock_price(
        file_path=csv_price_path,
        stock_name=target_stock_name,
        name_col=csv_name_column,
        price_col=csv_price_column
    )
    result['get_price'] = get_price

    # ------------------ 3. è¨ˆç®—åƒ¹æ ¼æ¼²è·Œå¹… ------------------
    
    if get_price is not None and get_price_before is not None:
        try:
            current_price = float(get_price)
            previous_price = float(get_price_before)
            
            if previous_price != 0:
                price_percent = (current_price - previous_price) / previous_price * 100
                result['price_percent'] = round(price_percent, 1)
                
                # æ ¼å¼åŒ–è¼¸å‡º
                formatted_percent = abs(result['price_percent'])
                if price_percent > 0:
                    result['price_percent_formatted'] = f"ğŸ”´{formatted_percent}%"
                else:
                    result['price_percent_formatted'] = f"ğŸŸ¢{formatted_percent}%"
            else:
                print(f"âš ï¸ è­¦å‘Š: {result['day_mmdd']} å‰ä¸€æ—¥åƒ¹æ ¼ç‚º 0ï¼Œç„¡æ³•è¨ˆç®—æ¼²è·Œå¹…ã€‚")
        except (ValueError, TypeError) as e:
            print(f"âŒ éŒ¯èª¤: {result['day_mmdd']} åƒ¹æ ¼è½‰æ›æˆ–è¨ˆç®—å¤±æ•— ({e})ï¼Œè·³éæ¼²è·Œå¹…ã€‚")
    else:
        # ç•¶ get_price æˆ– get_price_before ç‚º None æ™‚
        print(f"âŒ éŒ¯èª¤: {result['day_mmdd']} åƒ¹æ ¼è³‡æ–™ç¼ºå¤± (None)ï¼Œè·³éæ¼²è·Œå¹…è¨ˆç®—ã€‚")

    # ------------------ 4. ç²å–ä¸‰å¤§æ³•äººè²·è³£è¶… (å·²ä¿®æ­£ NoneType éŒ¯èª¤) ------------------
    
    # å‡è¨­ get_stock_net_volume è¿”å› pd.Series æˆ– None
    net_volume_data_series = get_stock_net_volume(csv_volume_path, target_stock_name)
    
    # >>> é—œéµä¿®æ­£å€å¡Šï¼šé˜²æ­¢ net_volume_data_series ç‚º None å°è‡´ astype éŒ¯èª¤
    if net_volume_data_series is not None and not net_volume_data_series.empty:
        try:
            # 1. å­—ä¸²æ¸…ç†ï¼šå°‡ Series è½‰æ›ç‚ºå­—ä¸²ï¼Œç§»é™¤é€—è™Ÿå’Œè² è™Ÿ (ç¢ºä¿åªå‰©æ•¸å­—å’Œå¯èƒ½çš„å°æ•¸é»)
            cleaned_volume_str = net_volume_data_series.astype(str).str.replace(',', '', regex=False).str.replace('-', '', regex=False).str.strip()
            
            # 2. æ•¸å€¼è½‰æ›ï¼šè½‰æ›ç‚º floatï¼Œç„¶å¾Œé™¤ä»¥ 1000 æ›ç®—æˆã€Œå¼µã€
            net_volume_in_lots = pd.to_numeric(cleaned_volume_str, errors='coerce') / 1000
            
            # 3. å››æ¨äº”å…¥ä¸¦è½‰æ›ç‚ºæ•´æ•¸
            rounded_lots = net_volume_in_lots.round(0).astype('Int64') # ä½¿ç”¨ Int64 è™•ç† NaN/None
            
            # 4. æå–ç´”æ•¸å€¼ä¸¦æ ¼å¼åŒ–
            # ç”±æ–¼ Series ä¸­åªæœ‰ä¸€å€‹å€¼ï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥å–ç¬¬ä¸€å€‹å€¼ï¼ˆæˆ–ä½¿ç”¨ to_stringï¼Œä½†å–å€¼æ›´å®‰å…¨ï¼‰
            if not rounded_lots.empty:
                volume_int = rounded_lots.iloc[0]
                
                # æ ¼å¼åŒ–ä¸¦å„²å­˜ï¼Œä½¿ç”¨åƒä½åˆ†éš”ç¬¦
                if pd.notna(volume_int):
                    # ä½¿ç”¨ f-string æ ¼å¼åŒ–ï¼Œè‡ªå‹•åŠ ä¸Šåƒä½åˆ†éš”ç¬¦
                    result['net_volume_data'] = f"{int(volume_int):,d}"
                else:
                    # å¦‚æœè½‰æ›å¾Œæ˜¯ NaN/Noneï¼Œå‰‡è¨­ç‚º 0
                    result['net_volume_data'] = "0"
            else:
                 # é›–ç„¶ Series ä¸ç‚º emptyï¼Œä½†æ•¸å€¼è½‰æ›å¾Œå¯èƒ½ç‚ºç©º
                 result['net_volume_data'] = "0"

        except Exception as e:
            # æ•ç²æ‰€æœ‰è½‰æ›éŒ¯èª¤
            print(f"âŒ éŒ¯èª¤: {result['day_mmdd']} è²·è³£è¶…è³‡æ–™è½‰æ›å¤±æ•— ({e})ï¼Œè¨­å®šç‚º 'è³‡æ–™éŒ¯èª¤'ã€‚")
            result['net_volume_data'] = "è³‡æ–™éŒ¯èª¤"
            
    else:
        # net_volume_data_series is None æˆ– empty (get_stock_net_volume å¤±æ•—æˆ–æ‰¾ä¸åˆ°è‚¡ç¥¨)
        print(f"æ‰¾ä¸åˆ° {target_stock_name} åœ¨ {result['day_mmdd']} çš„è²·è³£è¶…è‚¡æ•¸è³‡æ–™ã€‚")
        result['net_volume_data'] = "0"

    # ------------------ 5. ç²å–å€‹è‚¡æŒ‡æ¨™ ------------------
    
    stock_indicators_df = get_stock_indicators(csv_price_path, target_stock_name)
    
    if stock_indicators_df is not None and not stock_indicators_df.empty:
        try:
            # ç¢ºä¿æ¬„ä½å­˜åœ¨ï¼Œä¸¦æå–æ•¸æ“š
            result['pa_ratio'] = stock_indicators_df.iloc[0]['æ®–åˆ©ç‡(%)']
            result['pe_ratio'] = stock_indicators_df.iloc[0]['æœ¬ç›Šæ¯”']
            result['pb_ratio'] = stock_indicators_df.iloc[0]['è‚¡åƒ¹æ·¨å€¼æ¯”']
        except KeyError:
            print(f"âš ï¸ è­¦å‘Š: {result['day_mmdd']} å€‹è‚¡æŒ‡æ¨™ CSV æ¬„ä½åç¨±ä¸æ­£ç¢ºæˆ–æ•¸æ“šç¼ºå¤±ã€‚")
            # ä¿æŒé è¨­çš„ "-"

    return result
# è¨­å®šæ‚¨æƒ³è¦æŠ“å–çš„ç›®æ¨™æ—¥æœŸ (åªéœ€ä¿®æ”¹æ­¤è™•å³å¯æŠ“å–æ‰€æœ‰å ±å‘Šçš„è³‡æ–™)
def main_run():
    #----------------
    global running # å¼•ç”¨å…¨å±€è®Šæ•¸
    
    if not running:
        # å¦‚æœåœ¨ç­‰å¾…åŸ·è¡Œçš„æ’ç¨‹éšŠåˆ—ä¸­ï¼Œæª¢æŸ¥åˆ°ä¸é‹è¡Œï¼Œå‰‡ç›´æ¥è·³é
        print("\n[å®šæ™‚ä»»å‹™]: åµæ¸¬åˆ°é€€å‡ºä¿¡è™Ÿï¼Œè·³éæœ¬æ¬¡åŸ·è¡Œã€‚")
        return

    # --- å€å¡Š 1: æ—¥æœŸèˆ‡äº¤æ˜“æ—¥åˆ¤æ–· (å„ªåŒ–æ—¥æœŸæ ¼å¼åŒ–) ---
    NOW_DATETIME = datetime.now()
    Now_day_time = NOW_DATETIME.strftime("%Y-%m-%d %H:%M")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„æ—¥æœŸåŠæ™‚é–“ã€Œä¾‹å¦‚ 2025-11-12 11:12ã€
    Now_time_year = NOW_DATETIME.strftime("%Y")  #å–å¾—ç›®å‰ç³»çµ±æ™‚é–“çš„ã€Œå¹´ã€
    Trading_day_file_path = pathlib.Path(__file__).resolve().parent / "datas" / "processed" / "get_holidays" / f"trading_day_2021-{Now_time_year}.csv"
    
    # æ±ºå®šç•¶å‰è¦æŠ“å–çš„ç›®æ¨™æ—¥æœŸ (TARGET_DATE)    
    TARGET_DATE = NOW_DATETIME.strftime("%Y%m%d") 
    DATE_TO_CHECK = NOW_DATETIME.strftime("%Y/%m/%d")  
    DATE_TO_CHECK_NOW = NOW_DATETIME.strftime("%Y/%m/%d %H:%M:%S")
    
    # è™•ç†è¦æŠ“å–å“ªä¸€å¤©çš„è³‡æ–™é‚è¼¯
    result_found_days = get_previous_n_trading_days(Trading_day_file_path, DATE_TO_CHECK_NOW)
    
    if result_found_days == None:
        DATE_TO_CHECK_NOW = DATE_TO_CHECK_NOW - timedelta(days=1)    
        result_found_days = get_previous_n_trading_days(Trading_day_file_path, DATE_TO_CHECK_NOW)
    
    if DATE_TO_CHECK == result_found_days[-1]:  # å¦‚æœä»Šå¤©æ˜¯äº¤æ˜“æ—¥
        TARGET_DATE = DATE_TO_CHECK  # æŠ“å–ä»Šå¤©çš„è³‡æ–™  
        print(f"\n[æ™‚é–“æª¢æŸ¥]: ä»Šå¤©æ—¥æœŸ ({DATE_TO_CHECK}) ç‚ºäº¤æ˜“æ—¥ã€‚")    
        # æŠ“å–ç•¶å¤©çš„è³‡æ–™
        print(f"\n[æ™‚é–“æª¢æŸ¥]: ç¾åœ¨æ™‚é–“ç‚º {DATE_TO_CHECK_NOW}ï¼ŒæŠ“å– ({TARGET_DATE})ç•¶å¤©è³‡æ–™ã€‚")
        print("\n" + "="*50)
        print("--- ç¨‹å¼é–‹å§‹åŸ·è¡Œï¼šTWSE å ±å‘Šè³‡æ–™æŠ“å– ---")
        print("="*50 + "\n")
    else:
        TARGET_DATE = result_found_days[-1]  # æŠ“å–å‰ä¸€å¤©çš„è³‡æ–™  
        print(f"\n[æ™‚é–“æª¢æŸ¥]: ç¾åœ¨æ™‚é–“ç‚º {DATE_TO_CHECK_NOW}ï¼Œç•¶å¤©è³‡æ–™å°šæœªæ›´æ–°ï¼Œå°‡æä¾›å‰ä¸€å€‹äº¤æ˜“æ—¥ ({TARGET_DATE}) çš„è³‡æ–™ã€‚")
        print("\n" + "="*50)
        print("--- ç¨‹å¼é–‹å§‹åŸ·è¡Œï¼šTWSE å ±å‘Šè³‡æ–™æŠ“å– ---")
        print("="*50 + "\n")

    time_module.sleep(2) 
    
    # æŠ“å–ç¶²è·¯è³‡æ–™ "get_1-11-ä¸Šå¸‚è‚¡ç¥¨.py"
    # ==========================================================
    # 1. ç²å–å–®ä¸€ç›®æ¨™æ—¥æœŸå’Œæœˆä»½
    target_info = _get_target_date_and_month()
    daily_date = target_info["daily_date"]
    monthly_date = target_info["monthly_date"]
    start_time = target_info["start_time"]
    
    # 2. ç²å–ã€è‚¡ç¥¨ä»£è™Ÿã€‘æ¸…å–®
    stock_list = get_stock_list(STOCKS_ALL_CSV)
    
    if daily_date is None:
        print("\n--------------------------------------")
        print("âš ï¸ ç”±æ–¼ç„¡æ³•ç¢ºå®šç›®æ¨™äº¤æ˜“æ—¥ï¼Œæ—¥å ±ä»»å‹™å·²è·³éã€‚")
        print("--------------------------------------")
    else:
        # --- A. è™•ç†å–®æ—¥å ±è¡¨ (å…± 10 å€‹ä»»å‹™) ---
        
        # 1. é›†ä¸­äº¤æ˜“å¸‚å ´çµ±è¨ˆè³‡è¨Š (MI_INDEX)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX", "2_MI_INDEX", "_MI_INDEX_Sector", 
                                first_col_name="é …ç›®", header_row=2)
        
        # 2. é›†ä¸­å¸‚å ´å„é¡è‚¡æˆäº¤é‡å€¼ (BWIBBU_d)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d", "3_BWIBBU_d", "_BWIBBU_d_IndexReturn",
                                first_col_name="ç”¢æ¥­åˆ¥", header_row=1)
                                
        # 3. è‚¡ç¥¨/æŒ‡æ•¸æœŸè²¨æˆäº¤é‡å€¼ (TWTASU)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/afterTrading/TWTASU", "5_TWTASU", "_TWTASU_VolumePrice",
                                first_col_name="é …ç›®", header_row=1)
                                
        # 4. è‡ªç‡Ÿå•†è²·è³£é‡‘é¡ (BFIAMU)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/afterTrading/BFIAMU", "6_BFIAMU", "_BFIAMU_DealerTrade",
                                first_col_name="è‡ªç‡Ÿå•†", header_row=1)
                                
        # 5. åˆ¸å•†æˆäº¤é‡å€¼ç¸½è¡¨ (FMTQIK)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/afterTrading/FMTQIK", "7_FMTQIK", "_FMTQIK_BrokerVolume",
                                first_col_name="åˆ¸å•†", header_row=1)
                                
        # 6. ä¸‰å¤§æ³•äººè²·è³£è¶…é‡‘é¡ (BFI82U) - æ³¨æ„ URL åƒæ•¸çµæ§‹ä¸åŒ
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/fund/BFI82U", "8_BFI82U", "_BFI82U_3IParty_Day",
                                url_params="&type=day&dayDate",
                                first_col_name="é …ç›®", header_row=1)
                                
        # 7. å¤–è³‡åŠé™¸è³‡è²·è³£è¶…å½™ç¸½è¡¨ (TWT43U)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/fund/TWT43U", "9_TWT43U", "_TWT43U_ForeignTrade",
                                first_col_name="å¤–è³‡åŠé™¸è³‡", header_row=1)
                                
        # 8. æŠ•ä¿¡è²·è³£è¶…å½™ç¸½è¡¨ (TWT44U)
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/fund/TWT44U", "10_TWT44U", "_TWT44U_InvestmentTrust",
                                first_col_name="æŠ•ä¿¡", header_row=1)
                                
        # 9. ä¸‰å¤§æ³•äººè²·è³£è¶…çµ±è¨ˆ (T86) - ALL
        fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/fund/T86", "11_T86", "_T86_InstitutionalTrades",
                                url_params="&selectType=ALL",
                                first_col_name="è­‰åˆ¸ä»£è™Ÿ", header_row=1)

        # 10. èè³‡èåˆ¸é¤˜é¡ (TWT92U)
        #fetch_single_daily_report(daily_date, "https://www.twse.com.tw/rwd/zh/marginTrading/TWT92U", "4_TWT92U", "_TWT92U_Margin",
        #                        first_col_name="è‚¡ç¥¨ä»£è™Ÿ", header_row=1)
                                
    # --- B. è™•ç† STOCK_DAY (ä»»å‹™ 1 - ç•¶æœˆè¦†è“‹) ---
    if stock_list and monthly_date:
        fetch_twse_stock_day_single_month(monthly_date, stock_list)
        print("æ¸¬è©¦OKã€‚")
    elif not stock_list:
        print("è­¦å‘Šï¼šç„¡æ³•å–å¾—è‚¡ç¥¨æ¸…å–® (stocks_all.csv)ï¼Œè·³é STOCK_DAY æŠ“å–ã€‚")
    elif not monthly_date:
        print("è­¦å‘Šï¼šç„¡æ³•å–å¾—ç›®æ¨™æœˆä»½ï¼Œè·³é STOCK_DAY æŠ“å–ã€‚")
        
    print("\n======================================")
    print("âœ… æ‰€æœ‰ TWSE æ•¸æ“šæŠ“å–ä»»å‹™å·²å®Œæˆã€‚")
    print(f"ã€é–‹å§‹æ™‚é–“ã€‘{start_time} ")
    print(f"ã€å®Œæˆæ™‚é–“ã€‘{datetime.now().strftime('%H:%M:%S')} ")
    print("======================================")
    # ==========================================================

    # å–å¾—åº«å­˜è‚¡ç¥¨æ¸…å–®åŠè¿‘5æ—¥æ”¶ç›¤åƒ¹
    # ==========================================================
    # --- åƒæ•¸è¨­å®š ---
    # ==========================================================

    BASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)))
    EXCEL_PATH = BASE_DIR + "/datas/è‚¡ç¥¨åˆ†æ.xlsx"
    SHEET_NAME = "è‚¡ç¥¨åº«å­˜çµ±è¨ˆ"
    FILTER_COLUMN = "ç›®å‰è‚¡æ•¸åº«å­˜çµ±è¨ˆ"
    FILTER_VALUE = "0"
    OUTPUT_DIRECTORY = None 

    # ä¾†æºæª”æ¡ˆè·¯å¾‘
    SOURCE_FILE = r"Y:\æ”¶æ”¯è¨˜éŒ„\è‚¡ç¥¨åˆ†æ\è‚¡ç¥¨åˆ†æ.xlsx"
    # ç›®æ¨™ç›®éŒ„è·¯å¾‘
    DESTINATION_DIR = EXCEL_PATH # ä¹Ÿå°±æ˜¯ D æ§½çš„æ ¹ç›®éŒ„

    # --- ä¸»è¦åŸ·è¡Œå€å¡Š ---

    # åŸ·è¡Œè¤‡è£½åŠŸèƒ½
    copy_file_to_directory(SOURCE_FILE, DESTINATION_DIR)

    try:
        final_csv_path = extract_excel_sheet_filter_and_save(
            excel_file_path=EXCEL_PATH,
            sheet_name=SHEET_NAME,
            filter_column=FILTER_COLUMN,
            filter_value=FILTER_VALUE,
            output_dir=OUTPUT_DIRECTORY
        )
        
        if final_csv_path:
            print("\n" + "="*50)
            print("ğŸ‰ ä»»å‹™æˆåŠŸå®Œæˆï¼")
            print(f"CSV æª”æ¡ˆå·²å„²å­˜è‡³ï¼š\n {final_csv_path}")
            print("="*50)

    except (FileNotFoundError, ValueError, Exception) as e:
        print("\n" + "="*50)
        print("âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼")
        print(e)
        print("="*50)

    #-- å–å¾—è­‰åˆ¸åç¨±æ¸…å–® ---
    print("\n--- å–å¾—è­‰åˆ¸åç¨±æ¸…å–® ---")    
    df = pd.read_csv(final_csv_path, encoding='utf-8', skipinitialspace=True)
    df.columns = df.columns.str.strip()

    #print(df["è­‰åˆ¸åç¨±"])
    TARGET_STOCK_NAMES = []
    for col in df["è­‰åˆ¸åç¨±"]:
        TARGET_STOCK_NAMES.append(col)

    focused_sheet_name = "é—œæ³¨çš„è‚¡ç¥¨"
    focused_column_name = "è­‰åˆ¸åç¨±"
    focused_stock_names = get_stock_names_from_excel(DESTINATION_DIR, focused_sheet_name, focused_column_name)

    #-- å–å¾—å¾€å‰6å€‹äº¤æ˜“æ—¥ ---
    N_DAYS = 6 # å¾€å‰æ‰¾çš„äº¤æ˜“æ—¥æ•¸é‡

    recent_trading_days_df = find_last_n_trading_days_with_time_check(Trading_day_file_path, n=N_DAYS)
            
    Send_message_ALL = ""
    Send_message_ALL += f"ç™¼é€æ™‚é–“: {Now_day_time}\n"
    Send_message_ALL += f"***************************\n"
    Send_message_ALL += f"ğŸ“¦ {DATE_TO_CHECK} (åº«å­˜è‚¡)é€šçŸ¥ğŸ“¦\n"        
    Send_message_ALL += f"***************************"
    for TARGET_STOCK_NAME in TARGET_STOCK_NAMES:
        Send_message = ""
        #-- å–å¾—äº”å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ä¸¦åˆä½µ ---
        CSV_NAME_COLUMN = "è­‰åˆ¸åç¨±" 
        CSV_PRICE_COLUMN = "æ”¶ç›¤åƒ¹" 

        day_roll = []
        for row in recent_trading_days_df["æ—¥æœŸ"]:
            TARGET_DATE = row.replace("/", "")
            day_roll.append(TARGET_DATE)

        BASE_DIR = pathlib.Path(os.path.dirname(os.path.abspath(__file__)))

        if recent_trading_days_df is not None:
            print(f"\n--{TARGET_STOCK_NAME}æœ€è¿‘5å€‹äº¤æ˜“æ—¥--")

        # ç²å–ç¬¬ 5 å€‹äº¤æ˜“æ—¥ (day_roll[0]) çš„æ”¶ç›¤åƒ¹ä½œç‚ºæ¯”è¼ƒåŸºæº–
        CSV_PATH_BEFORE = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll[0]}_BWIBBU_d_IndexReturn.csv"
        get_price_before = lookup_stock_price(
            file_path=CSV_PATH_BEFORE,
            stock_name=TARGET_STOCK_NAME,
            name_col=CSV_NAME_COLUMN,
            price_col=CSV_PRICE_COLUMN
        )
        print("å‰5äº¤æ˜“æ—¥æ”¶ç›¤åƒ¹:", get_price_before)
        
        total_price_percent = 0
        final_indicators = {} # ç”¨æ–¼å„²å­˜æœ€å¾Œä¸€å¤©çš„å€‹è‚¡æŒ‡æ¨™

        # è¿´åœˆè™•ç†æœ€è¿‘ 4 å€‹äº¤æ˜“æ—¥çš„æ•¸æ“š (day_roll[1] åˆ° day_roll[4])
        for day_roll1 in day_roll[1:]:
            
            # å‘¼å«æ–°çš„å‡½å¼ä¾†ç²å–ç•¶æ—¥æ‰€æœ‰æ•¸æ“š
            day_data = get_day_stock_details(
                day_roll1=day_roll1,
                target_stock_name=TARGET_STOCK_NAME,
                base_dir=BASE_DIR,
                get_price_before=get_price_before,
                csv_name_column=CSV_NAME_COLUMN,
                csv_price_column=CSV_PRICE_COLUMN
            )
            
            # å½™ç¸½ç¸¾æ•ˆ
            total_price_percent += day_data['price_percent'] # price_percent æ˜¯æ•¸å­—
            #day_data_net_volume_data = day_data['net_volume_data']
            # å»ºç«‹å–®æ—¥è¨Šæ¯
            Send_message += (
                f"{day_data['day_mmdd']}:"
                f"{day_data['get_price']}"
                f"{day_data['price_percent_formatted']}"
                #f"({day_data_net_volume_data}å¼µ)\n"
                f"({day_data['net_volume_data']})\n"
            )
            
            # æ›´æ–°å‰ä¸€æ—¥åƒ¹æ ¼ï¼Œç”¨æ–¼ä¸‹ä¸€è¼ªè¿­ä»£
            get_price_before = day_data['get_price']
            
            # å„²å­˜æœ€å¾Œä¸€å¤©çš„æŒ‡æ¨™ï¼Œç”¨æ–¼å ±è¡¨å°¾éƒ¨ (å‡è¨­æ‚¨åªéœ€è¦æœ€å¾Œä¸€å¤©çš„æŒ‡æ¨™)
            final_indicators = {
                'pa_ratio': day_data['pa_ratio'],
                'pe_ratio': day_data['pe_ratio'],
                'pb_ratio': day_data['pb_ratio'],
            }

        # ------------------ å ±è¡¨å°¾éƒ¨è™•ç† (ä½¿ç”¨å„²å­˜çš„ final_indicators) ------------------

        # è™•ç†ç¸½é«”æ¼²è·Œå¹…
        total_price_percent = round(total_price_percent, 1) # ç¢ºä¿ç¸½è¨ˆä¹Ÿæ˜¯å››æ¨äº”å…¥
        if total_price_percent > 0:
            total_price_percent_formatted = f"ğŸ”´ {abs(total_price_percent)}%"
        else:
            total_price_percent_formatted = f"ğŸŸ¢ {abs(total_price_percent)}%"
        
        # å»ºç«‹å€‹è‚¡è³‡è¨Šè¨Šæ¯
        message_add = (
            f"\n--ğŸ¯ã€{TARGET_STOCK_NAME}ã€‘å€‹è‚¡è³‡è¨Š ğŸ¯--\n"
            f" Â æœ¬ç›Šæ¯” Â : {final_indicators['pe_ratio']}\n"
            f"è‚¡åƒ¹æ·¨å€¼æ¯”: {final_indicators['pb_ratio']}\n"
            f" Â æ®–åˆ©ç‡ Â : {final_indicators['pa_ratio']}%\n\n"
        )

        # å½™ç¸½ç¸½é«”è¨Šæ¯
        # å‡è¨­ TARGET_DATE æ˜¯ day_roll[0] (æœ€è¿‘ä¸€å¤©) çš„æ—¥æœŸï¼Œè«‹ç¢ºä¿é€™è£¡çš„ TARGET_DATE æ˜¯æ­£ç¢ºçš„
        TARGET_DATE = f"{day_roll[1][4:6]}/{day_roll[1][-2:]}" # æš«æ™‚ä½¿ç”¨ day_roll[1] çš„æ—¥æœŸä½œç‚ºå ±è¡¨æ—¥æœŸ
        

        Send_message_ALL += f"\n=ğŸ¥‡{TARGET_STOCK_NAME} æœ€è¿‘5æ—¥æ”¶ç›¤åƒ¹ğŸ¥‡ =\n{Send_message}"
        Send_message_ALL += f"== è¿‘5æ—¥ç¸¾æ•ˆ:{total_price_percent_formatted} ==\n"
        Send_message_ALL += message_add # åŠ å…¥å€‹è‚¡è³‡è¨Š

        ##2025-11-20
                
    # é‡å°é—œæ³¨çš„è‚¡ç¥¨ï¼Œå–å¾—è¿‘5æ—¥æ”¶ç›¤åƒ¹
    #Send_focused_message_all = ""
    Send_message_ALL += f"*****************************\n"
    Send_message_ALL += f"ğŸ’¡ {DATE_TO_CHECK} é—œæ³¨è‚¡è³‡è¨ŠğŸ’¡\n"
    Send_message_ALL += f"*****************************"
    for focused_stock_name in focused_stock_names:
    #    print(f"\n--- {focused_stock_names} æœ€è¿‘ 5 å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ ---")
        Send_focused_message = ""
        #-- å–å¾—äº”å€‹äº¤æ˜“æ—¥çš„æ”¶ç›¤åƒ¹ä¸¦åˆä½µ ---
        CSV_NAME_COLUMN = "è­‰åˆ¸åç¨±" # å‡è¨­ CSV ä¸­ç”¨æ–¼åç¨±æ¯”å°çš„æ¬„ä½
        CSV_PRICE_COLUMN = "æ”¶ç›¤åƒ¹"  # å‡è¨­ CSV ä¸­æ”¶ç›¤åƒ¹çš„æ¬„ä½

        day_roll = []
        for row in recent_trading_days_df["æ—¥æœŸ"]:
            TARGET_DATE = row.replace("/", "")
            day_roll.append(TARGET_DATE)

        BASE_DIR = pathlib.Path(os.path.dirname(os.path.abspath(__file__)))

        if recent_trading_days_df is not None:
            print(f"\n--{TARGET_STOCK_NAME}æœ€è¿‘5å€‹äº¤æ˜“æ—¥--")

        CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll[0:1][0]}_BWIBBU_d_IndexReturn.csv"
        get_price_before = lookup_stock_price(
                file_path=CSV_PATH,
                stock_name=focused_stock_name,
                name_col=CSV_NAME_COLUMN,
                price_col=CSV_PRICE_COLUMN
            )
        print("å‰5äº¤æ˜“æ—¥æ”¶ç›¤åƒ¹:", get_price_before)
        
        total_price_percent = 0
        for day_roll1 in day_roll[1:]:
            CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll1}_BWIBBU_d_IndexReturn.csv"

            # --- è®€å–è²·è³£è¶…è³‡æ–™ä¸¦ç™¼é€é€šçŸ¥ ---

            file_path = BASE_DIR / "datas" / "raw" / "11_T86" / f"{day_roll1}_T86_InstitutionalTrades.csv"
            stock_name = focused_stock_name # ç›®æ¨™è­‰åˆ¸åç¨±

            # å‘¼å«å‡½å¼
            net_volume_series = get_stock_net_volume(file_path, stock_name) # è®Šæ•¸åç¨±æ”¹ç‚º net_volume_series ä»¥ç¤ºå€åˆ¥
            net_volume_data = "0å¼µ" # é è¨­å€¼

            if net_volume_series is not None and not net_volume_series.empty:
                try:
                    # 1. å­—ä¸²æ¸…ç†ï¼šè™•ç†æ½›åœ¨çš„é€—è™Ÿæˆ–è² è™Ÿï¼Œä¸¦å¼·åˆ¶è½‰æ›ç‚º float
                    cleaned_str = net_volume_series.astype(str).str.replace(',', '', regex=False).str.replace('-', '', regex=False).str.strip()
                    # ä½¿ç”¨ pd.to_numeric é€²è¡Œç©©å¥è½‰æ›
                    net_volume_in_lots = pd.to_numeric(cleaned_str, errors='coerce').iloc[0] / 1000
                    
                    # 2. å››æ¨äº”å…¥ä¸¦æ ¼å¼åŒ–
                    if pd.notna(net_volume_in_lots):
                        # æ ¼å¼åŒ–ç‚ºå¸¶æœ‰åƒä½åˆ†éš”ç¬¦çš„æ•´æ•¸å­—ä¸²ï¼Œä¸¦åŠ ä¸Š "å¼µ"
                        net_volume_data = f"{int(round(net_volume_in_lots, 0)):,d}å¼µ"
                    else:
                        net_volume_data = "è³‡æ–™éŒ¯èª¤"

                except Exception as e:
                    print(f"âŒ éŒ¯èª¤ï¼š{focused_stock_name} æ•¸æ“šè½‰æ›å¤±æ•—ï¼Œè·³éè²·è³£è¶…æ›ç®— ({e})ã€‚")
                    net_volume_data = "è³‡æ–™éŒ¯èª¤" 
            else:
                print(f"æ‰¾ä¸åˆ° {focused_stock_name} çš„è²·è³£è¶…è‚¡æ•¸è³‡æ–™æˆ–è³‡æ–™ç‚ºç©ºã€‚")
                net_volume_data = "0å¼µ" # ä¿æŒé è¨­å€¼

            # âš ï¸ åŸå§‹ç¨‹å¼ç¢¼çš„å ±éŒ¯è¡Œå·²ç§»é™¤ï¼
            # net_volume_data = net_volume_data.tolist()[0][:-4] + "å¼µ"
            CSV_PATH = BASE_DIR / "datas" / "raw" / "3_BWIBBU_d" / f"{day_roll1}_BWIBBU_d_IndexReturn.csv"
            get_price = lookup_stock_price(
                file_path=CSV_PATH,
                stock_name=focused_stock_name,
                name_col=CSV_NAME_COLUMN,
                price_col=CSV_PRICE_COLUMN
            )
            day_mmdd = f"{day_roll1[4:6]}/{day_roll1[-2:]}"
            
            print("æ¸¬è©¦:",CSV_PATH)
            print("æ¸¬è©¦:",focused_stock_name)
            print("æ¸¬è©¦:",CSV_NAME_COLUMN)
            print("æ¸¬è©¦:",CSV_PRICE_COLUMN)
            print("æ¸¬è©¦:",get_price)
            
            #sys.exit(1)  # æš«åœåŸ·è¡Œï¼Œè«‹ç¢ºèªæ—¥æœŸç„¡èª¤å¾Œå†ç§»é™¤æ­¤è¡Œ
            price_percent = (float(get_price) - float(get_price_before)) / float(get_price_before) * 100
            price_percent = round(float(price_percent), 1)
            
            total_price_percent += int(price_percent)
            
            if price_percent > 0:
                price_percent = f"ğŸ”´{abs(price_percent)}"
            else:
                price_percent = f"ğŸŸ¢{abs(price_percent)}"
            
            Send_focused_message += f"{day_mmdd}:{get_price}{price_percent}%({net_volume_data})\n"
            get_price_before = get_price
            
        if total_price_percent > 0:
            total_price_percent = f"ğŸ”´ {abs(total_price_percent)}%"
        else:
            total_price_percent = f"ğŸŸ¢ {abs(total_price_percent)}%"

        Send_message_ALL += f"\n=âš ï¸  {focused_stock_name} æœ€è¿‘5æ—¥æ”¶ç›¤åƒ¹âš ï¸  =\n{Send_focused_message}"
        Send_message_ALL += f"== è¿‘5æ—¥ç¸¾æ•ˆ:{total_price_percent} ==\n"
        
    # å°‡ä¸‰å¤§æ³•äººè²·è¶…è³‡è¨ŠåŠ å…¥
    
    Send_message_ALL += f"\n\n"
    #Send_message_ALL += analysis_result    
    print(Send_message_ALL)
    
    # ---- line notify ç™¼é€è¨Šæ¯ ----1
    # â‹ è¼‰å…¥ line_API.env æª”æ¡ˆä¸­çš„è®Šæ•¸
    # æ³¨æ„ï¼šå¦‚æœæ‚¨ä½¿ç”¨ .env ä»¥å¤–çš„æª”å (å¦‚ line_token.env)ï¼Œéœ€è¦æŒ‡å®šæª”å

    LINE_API_ENV_PATH = BASE_DIR / "line_API.env"
    load_dotenv(LINE_API_ENV_PATH)

    # âŒ å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– Token å’Œ User ID
    LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
    LINE_USER_ID = os.getenv("LINE_USER_ID")


    # ä¿®æ­£ LineBotApiError çš„åŒ¯å…¥è·¯å¾‘ï¼ˆæ ¹æ“šæ‚¨ä¸Šä¸€å€‹å•é¡Œçš„è§£ç­”ï¼‰
    from linebot.v3.messaging import Configuration, ApiClient, MessagingApi
    from linebot.v3.messaging import TextMessage, PushMessageRequest


    # ----------------- æª¢æŸ¥ Token æ˜¯å¦å­˜åœ¨ -----------------
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("éŒ¯èª¤ï¼šLINE_CHANNEL_ACCESS_TOKEN æœªåœ¨ line_API.env ä¸­è¨­ç½®æˆ–è®€å–å¤±æ•—ã€‚ç¨‹å¼ä¸­æ­¢ã€‚")
        exit()
    # ----------------------------------------------------

    try:
        # åˆå§‹åŒ– Configuration å’Œ MessagingApi
        configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
        api_client = ApiClient(configuration)
        messaging_api = MessagingApi(api_client)
        print("Line Bot API åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"Line Bot API åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Tokenï¼š{e}")

    # ... æ¥ä¸‹ä¾†çš„ç¨‹å¼ç¢¼ä¿æŒä¸è®Š ...
    # é€™æ˜¯æ¥æ”¶è¨Šæ¯çš„ç”¨æˆ¶ ID æˆ–ç¾¤çµ„ ID
    # LINE_USER_ID ç¾åœ¨å·²ç¶“å¾ .env æª”æ¡ˆä¸­è®€å–

    def send_stock_notification(user_id, message_text):
        try:
            push_message_request = PushMessageRequest(
                to=user_id,
                messages=[TextMessage(text=message_text)]
            )
            # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨å…¨åŸŸè®Šæ•¸ messaging_apiï¼Œå¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œé€™è£¡æœƒå ±éŒ¯
            messaging_api.push_message(push_message_request) 
            print(f"è¨Šæ¯å·²æˆåŠŸç™¼é€çµ¦ {user_id}")
        except Exception as e:
            print(f"å…¶ä»–éŒ¯èª¤: {e}")


    # ç™¼é€å…¨éƒ¨è³‡è¨Š(åº«å­˜è‚¡é€šçŸ¥ã€é—œæ³¨è‚¡é€šçŸ¥ã€ä¸‰å¤§æ³•äººè²·è¶…å‰20)
    analysis_report = Send_message_ALL
    send_stock_notification(LINE_USER_ID, analysis_report)
# ===========================================================

# 1. åˆå§‹åŒ–é‹è¡Œç‹€æ…‹
running = True

# å…ˆé‹è¡Œ schedule.clear() å°‡æ’ç¨‹æ¸…é™¤ï¼Œé¿å…ç¿’æ…£ä½¿ç”¨ jupyter notebook æ•´åˆé–‹ç™¼ç’°å¢ƒçš„è®€è€…ï¼Œ
schedule.clear()

# æŒ‡å®šæ¯ 15 ç§’é‹è¡Œä¸€æ¬¡ say_hi å‡½æ•¸
# schedule.every(200).seconds.do(main_run)

#æ¯å°æ™‚é‹è¡Œä¸€æ¬¡
# schedule.every(1).hour.do(main_run)
# print("âœ… å·²è¨­å®šå®šæ™‚ä»»å‹™ï¼šæ¯å°æ™‚åŸ·è¡Œ main_runã€‚")

# æ¯å¤© 21:00 åŸ·è¡Œä¸€æ¬¡
schedule.every().day.at('21:00').do(main_run)
print("âœ… å·²è¨­å®šå®šæ™‚ä»»å‹™ï¼š21:00 åŸ·è¡Œ main_runã€‚")

# 3. è¨­å®šéµç›¤ç†±éµ (éé˜»å¡å¼ç›£è½)
keyboard.add_hotkey('1', main_run)
keyboard.add_hotkey('q', stop_program)
print("âœ… å·²è¨­å®šéµç›¤ç†±éµï¼š[1] ç›´æ¥åŸ·è¡Œmain_run, [Q] åœæ­¢ç¨‹å¼ã€‚")

print("\n--- ç¨‹å¼é–‹å§‹é‹è¡Œ ---")
print("ä¸»ç¨‹å¼å’Œæ’ç¨‹ç›£è½ä¸­...")

# --- ä¸»å¾ªç’° (Main Loop) ---
try:
    while running:
        # 1. æª¢æŸ¥æ˜¯å¦æœ‰æ’ç¨‹ä»»å‹™éœ€è¦é‹è¡Œ
        schedule.run_pending()
        
        # 2. è®“ä¸»å¾ªç’°çŸ­æš«ä¼‘çœ ï¼ŒåŒæ™‚è®“ CPU è³‡æºé‡‹æ”¾çµ¦å…¶ä»–è¡Œç¨‹ (åŒ…æ‹¬éµç›¤ç›£è½)
        # é€™è£¡è¨­å®šä¸€å€‹è¼ƒçŸ­çš„ä¼‘çœ æ™‚é–“ï¼Œç¢ºä¿å°æ’ç¨‹å’Œéµç›¤è¼¸å…¥çš„éŸ¿æ‡‰æ›´å³æ™‚ã€‚
        time_module.sleep(1)
        
except KeyboardInterrupt:
    # å…è¨±ä½¿ç”¨ Ctrl+C é€€å‡º
    print("\nç¨‹å¼è¢« Ctrl+C ä¸­æ–·é€€å‡ºã€‚")

finally:
# 3. ç§»é™¤æ‰€æœ‰è¨»å†Šçš„ç†±éµ (æ¸…ç†ç’°å¢ƒ)
    keyboard.unhook_all()
    print("æ‰€æœ‰éµç›¤ç›£è½å·²é—œé–‰ã€‚")
    print("ç¨‹å¼å®‰å…¨é€€å‡ºã€‚")