import os
import re
import time
import requests
import pandas as pd
from typing import Optional, List
from io import StringIO
from datetime import datetime, timedelta
import pathlib     # as pathlib

# æŠ‘åˆ¶ç•¶ verify=False æ™‚å½ˆå‡ºçš„ InsecureRequestWarning è­¦å‘Š
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# --- è¨­å®šèˆ‡è·¯å¾‘ ---
# âš ï¸ è«‹ç¢ºä¿ 'datas/raw/7_FMTQIK' è·¯å¾‘å­˜åœ¨æˆ–å¯è¢«å»ºç«‹
OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "datas", "raw" , "7_FMTQIK")
BASE_URL = "https://www.twse.com.tw/rwd/zh/afterTrading/FMTQIK"

# å‡è¨­äº¤æ˜“æ—¥æ¸…å–®æª”æ¡ˆç‚º trading_day_2021-2025.csv
CSV_FILE_PATH = pathlib.Path(__file__).resolve().parent / "datas" / "processed" / "get_holidays" / f"trading_day_2021-2025.csv"

def get_date_list_based_on_time(file_path: str) -> Optional[List[str]]:
    """
    1. è®€å– CSV æª”æ¡ˆå…§çš„æ—¥æœŸ (å‡å®šç‚ºäº¤æ˜“æ—¥æ¸…å–®)ã€‚
    2. æ ¹æ“šç•¶å‰æ™‚é–“ (21:00 å‰/å¾Œ) ç¢ºå®šæˆªæ­¢æ—¥æœŸ (æ˜¨å¤©/ä»Šå¤©)ã€‚
    3. è¼¸å‡ºå¾æª”æ¡ˆç¬¬ä¸€å€‹æ—¥æœŸåˆ°æˆªæ­¢æ—¥æœŸçš„æ—¥æœŸæ¸…å–®ã€‚
    """
    
    # 1. è®€å– CSV æª”æ¡ˆ
    try:
        # è®€å– CSVï¼Œå‡è¨­æ—¥æœŸåœ¨ç¬¬ä¸€æ¬„
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        # å˜—è©¦æ‰¾å‡ºåŒ…å«æ—¥æœŸçš„æ¬„ä½ (å‡è¨­æ˜¯ç¬¬ä¸€æ¬„)
        date_column = df.columns[0]
        
        # éæ¿¾ç©ºå€¼ä¸¦è½‰æ›ç‚ºå·²æ’åºçš„å­—ä¸²åˆ—è¡¨ (æ ¼å¼ç‚º YYYYMMDD)
        all_dates_list = df[date_column].astype(str).str.strip().tolist()
        all_dates_list = sorted(list(set(all_dates_list)))

        if not all_dates_list:
            print(f"éŒ¯èª¤: æª”æ¡ˆ {file_path} ä¸­æ‰¾ä¸åˆ°ä»»ä½•æ—¥æœŸæ•¸æ“šã€‚")
            return None

    except FileNotFoundError:
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}ï¼Œè«‹ç¢ºèªè·¯å¾‘æˆ–å…ˆé‹è¡Œæ¨¡æ“¬åˆå§‹åŒ–ã€‚")
        return None
    except Exception as e:
        print(f"éŒ¯èª¤: è®€å–æˆ–è™•ç†æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

    # 2. åˆ¤æ–·ç¾åœ¨çš„æ™‚é–“ä¾†æ±ºå®šæˆªæ­¢æ—¥æœŸ
    now = datetime.now()
    current_time = now.time()
    
    # å®šç¾© 21:00 (æ™šä¸Š 9 é») çš„æˆªæ­¢æ™‚é–“
    cutoff_time = datetime.strptime("21:00:00", "%H:%M:%S").time()
    
    # æª¢æŸ¥ç•¶å‰æ™‚é–“ï¼ˆ2025/11/22 21:03:14 CSTï¼‰
    if current_time >= cutoff_time:
        # 21é»ä»¥å¾Œ (å« 21:00:00): æˆªæ­¢æ—¥ç‚ºä»Šå¤© (11/22)
        end_date = now.date()
        print(f"ã€æ™‚é–“åˆ¤æ–·ã€‘ç•¶å‰æ™‚é–“ ({now.strftime('%H:%M:%S')}) æ™šæ–¼ 21:00ï¼Œæˆªæ­¢æ—¥ç‚ºä»Šå¤© ({end_date.strftime('%Y/%m/%d')})ã€‚")
    else:
        # 21é»ä»¥å‰: æˆªæ­¢æ—¥ç‚ºæ˜¨å¤©
        end_date = (now - timedelta(days=1)).date()
        print(f"ã€æ™‚é–“åˆ¤æ–·ã€‘ç•¶å‰æ™‚é–“ ({now.strftime('%H:%M:%S')}) æ—©æ–¼ 21:00ï¼Œæˆªæ­¢æ—¥ç‚ºæ˜¨å¤© ({end_date.strftime('%Y/%m/%d')})ã€‚")

    # 3. ç¢ºå®šæ—¥æœŸç¯„åœ
    start_date_str = all_dates_list[0]
    end_date_str = end_date.strftime("%Y%m%d")
    
    # 4. ç¯©é¸ CSV å…§çš„æ—¥æœŸæ¸…å–®
    # åªä¿ç•™ä»‹æ–¼ [èµ·å§‹æ—¥æœŸ, æˆªæ­¢æ—¥æœŸ] ä¹‹é–“çš„æ‰€æœ‰æ—¥æœŸ
    filtered_dates = [
        date_str for date_str in all_dates_list 
        if start_date_str <= date_str <= end_date_str
    ]

    if not filtered_dates:
        print(f"è­¦å‘Š: åœ¨ç¯„åœ [{start_date_str} - {end_date_str}] å…§æ‰¾ä¸åˆ°ä»»ä½•æ—¥æœŸã€‚")
        return []
        
    print(f"\n--- æœ€çµ‚æ—¥æœŸæ¸…å–® (å…± {len(filtered_dates)} å¤©) ---")
    print(f"èµ·å§‹æ—¥æœŸ: {filtered_dates[0]}")
    print(f"æˆªæ­¢æ—¥æœŸ: {filtered_dates[-1]}")
    
    return filtered_dates
# --- è¼”åŠ©å‡½æ•¸ ---
# æª¢æŸ¥ä¸¦å»ºç«‹æ‰€éœ€çš„ã€è³‡æ–™å¤¾ã€‘
def _check_folder_and_create(filepath: str):
    # ä½¿ç”¨ pathlib ç¢ºä¿è·¨å¹³å°å…¼å®¹æ€§
    pathlib.Path(filepath).parent.mkdir(parents=True, exist_ok=True)

def _fetch_twse_data(url: str) -> Optional[str]:
    """å˜—è©¦å¾ TWSE æŠ“å–è³‡æ–™ï¼Œä¸¦è¿”å›åŸå§‹æ–‡æœ¬ã€‚"""
    try:
        headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status() 
        response.encoding = 'Big5'
        
        # æª¢æŸ¥ TWSE å›å‚³å…§å®¹æ˜¯å¦ç‚ºéŒ¯èª¤è¨Šæ¯
        if "å¾ˆæŠ±æ­‰" in response.text or "æŸ¥ç„¡ç›¸é—œè³‡æ–™" in response.text:
            print("âš ï¸ TWSE ç¶²ç«™è¿”å›éŒ¯èª¤è¨Šæ¯ï¼Œè©²æ—¥å¯èƒ½ç„¡è³‡æ–™ã€‚")
            return None
        
        return response.text
        
    except requests.exceptions.HTTPError as errh:
        print(f"âŒ HTTP éŒ¯èª¤ï¼š{errh} (è©²æ—¥å¯èƒ½ç„¡äº¤æ˜“è³‡æ–™)")
    except requests.exceptions.RequestException as err:
        print(f"âŒ é€£ç·šæˆ– Requests éŒ¯èª¤: {err}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")
        
    return None

def _read_twse_csv(response_text: str, header_row: int) -> Optional[pd.DataFrame]:
    """å°‡ TWSE è¿”å›çš„æ–‡æœ¬è§£æç‚º Pandas DataFrameã€‚"""
    try:
        data = StringIO(response_text)
        # å ±è¡¨å¯¦éš›çš„è¡¨é ­åœ¨ç´¢å¼• 1 (0-based)
        df = pd.read_csv(data, 
                         header=header_row, 
                         encoding='utf-8-sig', 
                         skipinitialspace=True,
                         engine='python',
                         on_bad_lines='skip' 
        )
        if not df.empty:
            df.columns = df.columns.str.strip() # æ¸…ç†æ¬„ä½åç¨±
            
            # ç§»é™¤æ‰€æœ‰å…§å®¹ç‚ºç©ºçš„æ¬„ä½
            df.dropna(axis=1, how='all', inplace=True)
            
            if df.empty:
                print("âš ï¸ è§£æ CSV å¾Œ DataFrame ç‚ºç©ºï¼Œå¯èƒ½ç„¡æœ‰æ•ˆè³‡æ–™ã€‚")
                return None
            
            # FMTQIK å ±è¡¨é€šå¸¸ä»¥ã€Œåˆ¸å•†ã€ä½œç‚ºç¬¬ä¸€æ¬„ä½ï¼Œç§»é™¤ç©ºç™½è¡Œ
            if 'åˆ¸å•†' in df.columns:
                 df = df[df['åˆ¸å•†'].astype(str).str.strip() != '']
                
            return df
        return None

    except Exception as e:
        print(f"âŒ è§£æ CSV æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


# --- æ ¸å¿ƒå–®æ—¥æŠ“å–å‡½æ•¸ ---

def fetch_twse_fmtqik_single(target_date: str) -> Optional[pd.DataFrame]:
    """
    æŠ“å–æŒ‡å®šæ—¥æœŸçš„ FMTQIK å ±å‘Š (åˆ¸å•†æˆäº¤é‡å€¼ç¸½è¡¨)ã€‚

    Args:
        target_date: æ¬²æŠ“å–çš„æ—¥æœŸï¼Œæ ¼å¼ç‚º YYYYMMDDã€‚

    Returns:
        æˆåŠŸæ™‚è¿”å› DataFrameï¼Œå¤±æ•—æ™‚è¿”å› Noneã€‚
    """
    if not re.fullmatch(r'\d{8}', target_date): 
        print(f"æ—¥æœŸæ ¼å¼éŒ¯èª¤: {target_date}")
        return None
        
    url = f"{BASE_URL}?date={target_date}&response=csv"
    # ä½¿ç”¨ os.path.join ç¢ºä¿è·¨å¹³å°å…¼å®¹æ€§
    filename = os.path.join(OUTPUT_DIR, f"{target_date}_FMTQIK_BrokerVolume.csv") 
    
    _check_folder_and_create(filename) # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    
    print(f" Â -> å˜—è©¦æŠ“å– {target_date}...")
    
    response_text = _fetch_twse_data(url)
    if response_text is None: 
        return None
    
    # å‡è¨­ FMTQIK çš„è¡¨é ­åœ¨ç´¢å¼• 1
    df = _read_twse_csv(response_text, header_row=1) 

    if df is not None:
        
        # å„²å­˜è³‡æ–™
        try:
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f" Â âœ… {target_date} è³‡æ–™å„²å­˜æˆåŠŸ: {filename}")
        except Exception as e:
            print(f"âŒ {target_date} è³‡æ–™å„²å­˜å¤±æ•—: {e}")
            return None # å„²å­˜å¤±æ•—ä¹Ÿè¿”å› None
            
        return df
    else:
        print(f" Â âš ï¸ {target_date} è³‡æ–™æŠ“å–æˆåŠŸä½†è§£æå¾Œç‚ºç©ºï¼Œè·³éå„²å­˜ã€‚")
        return None

# --- æ‰¹æ¬¡è™•ç†èˆ‡é‡è©¦å‡½æ•¸ (æª”æ¡ˆå­˜åœ¨ä¸ç­‰å¾… 2 ç§’) ---

def batch_fetch_twse_fmtqik(date_list: List[str]):
    """
    é‡å°æä¾›çš„æ—¥æœŸæ¸…å–®ï¼Œé€ä¸€æŠ“å– TWSE FMTQIK è³‡æ–™ï¼Œä¸¦åœ¨å¤±æ•—æ™‚å¯¦ä½œé‡è©¦æ©Ÿåˆ¶ã€‚
    """
    print("--- é–‹å§‹æ‰¹æ¬¡æŠ“å– TWSE FMTQIK è³‡æ–™ ---")
    
    for target_date in date_list:
        target_date = target_date.replace("/", "")
        max_attempts = 4
        
        filename = os.path.join(OUTPUT_DIR, f"{target_date}_FMTQIK_BrokerVolume.csv")
        _check_folder_and_create(filename) # ç¢ºä¿ç›®éŒ„å­˜åœ¨

        # é—œéµä¿®æ­£ï¼šæ­¥é©Ÿ 1. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ï¼Œå­˜åœ¨å‰‡è·³éå»¶é²
        if os.path.exists(filename):
            print(f" Â â„¹ï¸ {target_date} è³‡æ–™å·²å­˜åœ¨ ({filename})ï¼Œè·³éæŠ“å–ã€‚")
            continue  # ç«‹å³è·³åˆ°ä¸‹ä¸€å€‹æ—¥æœŸï¼Œä¸åŸ·è¡Œå»¶é²
        
        # æ­¥é©Ÿ 2. æª”æ¡ˆä¸å­˜åœ¨ï¼Œé–‹å§‹åŸ·è¡ŒæŠ“å–å’Œé‡è©¦
        is_successful = False
        for attempt in range(1, max_attempts + 1):
            
            # åŸ·è¡ŒæŠ“å–
            df = fetch_twse_fmtqik_single(target_date)
            
            if df is not None:
                # æˆåŠŸ
                print(f"ğŸŒŸ {target_date} è³‡æ–™å·²å®Œæˆã€‚")
                is_successful = True
                break  # æˆåŠŸï¼Œè·³å‡ºé‡è©¦è¿´åœˆ
            
            # å¤±æ•—è™•ç†
            if attempt < max_attempts:
                delay_hours = attempt 
                
                # æ¸¬è©¦ç’°å¢ƒç”¨:
                delay_seconds = delay_hours * 5 

                print(f"ğŸš¨ {target_date} æŠ“å–å¤±æ•— (ç¬¬ {attempt} æ¬¡å˜—è©¦)ã€‚å°‡åœ¨ {delay_seconds} ç§’å¾Œé‡è©¦ (ä¸‹æ¬¡ç­‰å¾… {delay_hours} å°æ™‚)...")
                time.sleep(delay_seconds)
            else:
                # è¶…éæœ€å¤§å˜—è©¦æ¬¡æ•¸
                print(f"âŒ {target_date} è³‡æ–™ç¶“é {max_attempts} æ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—ï¼Œè·³éæ­¤æ—¥æœŸã€‚")
        
        # æ­¥é©Ÿ 3. åªæœ‰åœ¨åŸ·è¡Œäº†ç¶²è·¯æŠ“å–æˆ–é‡è©¦ä¹‹å¾Œï¼Œæ‰éœ€è¦ç­‰å¾… 2 ç§’
        if is_successful or attempt == max_attempts:
            print("ç­‰å¾… 2 ç§’å¾Œï¼Œæº–å‚™è™•ç†ä¸‹ä¸€å€‹æ—¥æœŸ...")
            time.sleep(2)


# --- åŸ·è¡Œç¯„ä¾‹ ---

if __name__ == "__main__":
    
    # 2. åŸ·è¡Œæ—¥æœŸæ¸…å–®ç”Ÿæˆ
    final_date_list = get_date_list_based_on_time(CSV_FILE_PATH)
    
    if final_date_list:
        print("--- é–‹å§‹æŠ“å–/æª¢æŸ¥æª”æ¡ˆ ---")
        batch_fetch_twse_fmtqik(final_date_list)
    else:
        print("æ²’æœ‰å¯ä¾›è™•ç†çš„æ—¥æœŸæ¸…å–®ï¼Œç¨‹å¼çµæŸã€‚")